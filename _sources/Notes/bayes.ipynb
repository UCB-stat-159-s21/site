{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian and Frequentist Estimation and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores Bayesian methods from a foundational perspective, contrasting the assumptions required for Bayesian inferences with the assumptions required for frequentist inferences, and the differences in interpretation of uncertainties for the two paradigms.\n",
    "\n",
    "It also discusses using frequentist measures of uncertainty for Bayesian estimates.\n",
    "\n",
    "It is adapted from Stark (2015) and Stark and Tenorio (2010), with additional material on election audits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Bayesian?\n",
    "\n",
    "Per I.J. Good, a Bayesian is someone for whom it makes sense to talk about the conditional probability that a hypothesis $H$ is true, given evidence $E$.\n",
    "\n",
    "For a frequentist, such a question generally does not make sense: $H$ is either true or not. Its truth is not random, just unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian estimates\n",
    "\n",
    "+ General approach (viz, don't even need data to make an estimate)\n",
    "\n",
    "+ Guaranteed to have some good frequentist properties (if the prior is proper and the model is finite-dimensional)\n",
    "\n",
    "+ Can be thought of as a kind of \"regularization\"\n",
    "\n",
    "+ Elegant math from perspective of decision theory: convexify strategy space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainties\n",
    "\n",
    "+ Bayesian and frequentist uncertainties have completely different interpretations\n",
    "\n",
    "    - Frequentist: hold parameter constant, characterize behavior under repeated measurement\n",
    "    - Bayesian: hold measurement constant, characterize behavior under repeated selection of the parameter from the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Credible regions versus confidence regions\n",
    "\n",
    "    - credible level: probability that by drawing from prior, nature generates an element of the set, given the data\n",
    "    - confidence level: probability that procedure gives a set that contains the truth\n",
    "\n",
    "+ Can grade Bayesian methods using frequentist criteria\n",
    "    - E.g., what is the coverage probability of a credible region?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy problem: bounded normal mean\n",
    "\n",
    "+ Observe $Y \\sim N(\\theta, 1)$.\n",
    "\n",
    "+ Know _a priori_ that $\\theta \\in [-\\tau, \\tau]$\n",
    "\n",
    "+ Bayes \"uninformative\" prior: $\\theta \\sim U[-\\tau, \\tau]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figs/starkTenorio09-length.png\" style=\"height: 650px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figs/starkTenorio09-coverage.png\" style=\"height: 650px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duality between Bayes and minimax approaches\n",
    "\n",
    "+ Formal Bayesian uncertainty can be made as small as desired by choosing prior appropriately.\n",
    "\n",
    "+ Under suitable conditions, the minimax frequentist risk is equal to the Bayes risk for the \"least-favorable\" prior.\n",
    "\n",
    "+ If Bayes risk is less than minimax risk, prior artificially reduced the (apparent) uncertainty. Regardless, means something different.\n",
    "\n",
    "+ Least-favorable prior can be approximated numerically even for \"black-box\" numerical models, a la Schafer & Stark (2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Posterior uncertainty measures meaningful only if you believe prior\n",
    "\n",
    "+ Changes the subject\n",
    "\n",
    "+ Is the truth unknown? Is it a realization of a known probability distribution?\n",
    "\n",
    "+ Where does prior come from?\n",
    "\n",
    "    - Usually chosen for computational convenience or habit, not \"physics\"\n",
    "    \n",
    "    - Priors get their own literature\n",
    "    \n",
    "    - Eliciting priors deeply problemmatic\n",
    "    \n",
    "    - Why should I care about your posterior, if I don't share your prior?\n",
    "\n",
    "+ How much does prior matter?\n",
    "\n",
    "+ Slogan \"the data swamp the prior.\" Theorem has conditions that aren't always met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncertainty\n",
    "\n",
    "+ Not all uncertainty can be represented by a probability\n",
    "\n",
    "+ \"Aleatory\" randomness\n",
    "\n",
    "    - Canonical examples: coin toss, die roll, lotto, roulette\n",
    "    - under some circumstances, behave \"as if\" random (but not perfectly)\n",
    "\n",
    "+ Epistemic: stuff we don't know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"LeCam 1977\" src=\"../Figs/leCam77Probs.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Bayesian way to combine aleatory variability epistemic uncertainty puts beliefs on a par with an unbiased physical measurement w/ known uncertainty.\n",
    "    \n",
    "+ Claims by introspection, can estimate without bias, with known accuracy,\n",
    "as if one's brain were unbiased instrument with known accuracy\n",
    "\n",
    "+ Bacon put this to rest philosophically. Moreover, empirically:\n",
    "    - people are bad at making even rough quantitative estimates\n",
    "    - quantitative estimates are usually biased\n",
    "    - bias can be manipulated by anchoring, priming, etc.\n",
    "    - people are bad at judging weights _in their hands_:  biased by shape & density\n",
    "    - people are bad at judging when something is random\n",
    "    - people are overconfident in their estimates and predictions\n",
    "    - confidence  unconnected to actual accuracy.\n",
    "    - anchoring affects entire disciplines (e.g., Millikan, c, Fe in spinach)\n",
    "\n",
    "+ what if I don't trust your internal scale, or your assessment of its accuracy?\n",
    "\n",
    "+ same observations that are factored in as \"data\" are also used to form beliefs: the \"measurements\" made by introspection are not independent of the data\n",
    "\n",
    "+ if you try several priors, you are not using Bayesian statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian and Frequentist Uncertainties\n",
    "\n",
    "+ What varies?\n",
    "\n",
    "+ What is constant (conditioned on)?\n",
    "\n",
    "+ How do we know how the variable thing varies?\n",
    "\n",
    "+ Are we talking about our world, or a set of possible worlds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Framework\n",
    "\n",
    "We want to use the measurement $Y$ to learn about $\\theta$, an unknown parameter.\n",
    "\n",
    "The datum $Y$ is an $n$-vector of real numbers, i.e., elements\n",
    "of $\\Re^n$.\n",
    "Mathematics, physics, or previous experiments tell us the possible values\n",
    "of $\\theta$, which are represented by the set $\\Theta$.\n",
    "The set $\\Theta$ can be high-dimensional, even infinite-dimensional.\n",
    "\n",
    "The datum $Y$ is related to $\\theta$ through a _measurement model_\n",
    "that gives the probability distribution $\\Pr_\\eta$ of $Y$ for each \n",
    "$\\eta \\in \\Theta$.\n",
    "It connects the parameter to the probability distribution of the data.\n",
    "\n",
    "If $\\theta = \\eta$, then $Y \\sim \\Pr_\\eta$.\n",
    "\n",
    "For any particular set $A \\subset \\Re^n$, $\\Pr_\\eta \\{ Y \\in A \\}$ generally will\n",
    "not be equal to $\\Pr_{\\eta'} \\{ Y \\in A \\}$ if $\\eta \\ne \\eta'$.\n",
    "If there are parameters $\\eta \\ne \\eta'$ such that\n",
    "$\\Pr_\\eta \\{ Y \\in A \\} = \\Pr_{\\eta'} \\{ Y \\in A \\}$ for all (measurable) subsets $A \\subset \\Re^n$, then $\\theta$ is not _identifiable_.\n",
    "\n",
    "**Technical note.** We assume that there is a known measure $\\mu$ that dominates\n",
    "the set of distributions $\\mathcal{P} \\equiv \\{ \\Pr_\\eta : \\eta \\in \\Theta\\}$, so\n",
    "we can use \"densities\" even if some members of $\\mathcal{P}$ have atoms.\n",
    "This assumption makes it easier to define likelihoods, which are required\n",
    "for the Bayesian framework; it also implies that all the measures $\\mathcal{P}$\n",
    "are defined on a common sigma-algebra $\\cal A$ of subsets of $\\Re^n$,\n",
    "which avoids some potential pathologies.\n",
    "If this language isn't familiar, just assume for now that no matter what $\\eta \\in \\Theta$\n",
    "might be, $Y$ has an ordinary probability density (or assume that $Y$ has a \n",
    "discrete distribution).\n",
    "\n",
    "With respect to $\\mu$, the density of $\\Pr_\\eta$ at $y$ is\n",
    "\\begin{equation*}\n",
    "p_\\eta(y) \\equiv \\left . \\frac{d\\Pr_\\eta}{d\\mu} \\right |_y.\n",
    "\\end{equation*}\n",
    "\n",
    "For any fixed $y$, the _likelihood of $\\eta$ given $Y = y$_ is\n",
    "$p_\\eta(y)$, viewed as a function of $\\eta$ with $y$ held constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often impossible to estimate $\\theta$\n",
    "with any useful level of accuracy; in many problems, $\\theta$ is not even identifiable.\n",
    "\n",
    "But it still may be possible and scientifically interesting to estimate a\n",
    "_parameter_ $\\lambda = \\lambda[\\theta]$, a property of $\\theta$.\n",
    "The parameter might be the average of $\\theta$ over some volume of space or time,\n",
    "a norm or semi norm of $\\theta$, or the number of local maxima $\\theta$ has,\n",
    "for instance.\n",
    "We shall assume that the possible values of $\\lambda[\\theta]$ are also elements of\n",
    "a Hilbert space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian election audits\n",
    "\n",
    "A Bayesian election audit (Rivest and Shen, 2012) uses Bayes rule applied to a prior probability distribiution on voter preferences, together with audit data, to find the posterior probability that the reported outcome is incorrect.\n",
    "The audit stops only if that posterior probability is smaller than a specified limit.\n",
    "\n",
    "Rivest and Shen (2012) frame it as:\n",
    "\n",
    "> What is the probability that each candidate\n",
    "would be determined to be the actual winner, if\n",
    "the auditing process continued to examine all\n",
    "the ballots, and the remaining ballots audited\n",
    "showed results similar to what we’ve seen in\n",
    "the ballots audited so far?\n",
    "\n",
    "If the probability that anybody other than the reported winner actually won is small enough, the audit stops: the _upset probability_ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet priors\n",
    "\n",
    "A Bayesian auditor requires a prior probability distribution for voter preferences--for the votes that each ballot shows.\n",
    "\n",
    "A typical form assumes that the prior probability that a ballot is of type $k$, $1 \\le k \\le K \\ge 2$ has a _Dirichlet distribution_:\n",
    "\n",
    "Fix $ a = (a_j)_{j=1}^k$ with $a_j > 0$.\n",
    "Let $S_K$ denote the _unit simplex_ in $K$-dimensional space, that is,\n",
    "\n",
    "\\begin{equation*}\n",
    "   S_K \\equiv \\{ (x_k)_{k=1}^K \\in \\Re^K : x_k \\in (0, 1) \\mbox{ and } \\sum_k x_k = 1.\n",
    "\\end{equation*}\n",
    "\n",
    "Then for $x \\in S_k$, the joint density is \n",
    "\n",
    "\\begin{equation*}\n",
    "   f_a(x) \\equiv \\frac{1}{B(a)} \\prod_{k=1}^K x_k^{a_k-1},\n",
    "\\end{equation*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation*}\n",
    "   B(a) \\equiv \\frac{\\prod_k \\Gamma(a_k)}{\\Gamma \\left ( \\sum_k a_k \\right )}.\n",
    "\\end{equation*}\n",
    "\n",
    "The Dirichlet distribution is _conjugate_ to the multinomial distribution, in the sense that if $X \\sim \\mbox{Multinomial}(\\eta)$ and $\\eta \\sim \\mbox{Dirichlet}(a)$ then the posterior distribution of $\\eta$ given $X=x = (x_1, \\ldots, x_K)$ is $\\mbox{Dirichlet}(a+x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/matplotlib/font_manager.py:229: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division, print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "from scipy.stats import dirichlet\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code is adapted from \n",
    "# http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/\n",
    "# and updated for compatibility with Python 3\n",
    "\n",
    "corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
    "triangle = tri.Triangulation(corners[:, 0], corners[:, 1])\n",
    "refiner = tri.UniformTriRefiner(triangle)\n",
    "trimesh = refiner.refine_triangulation(subdiv=4)\n",
    "\n",
    "midpoints = [(corners[(i + 1) % 3] + corners[(i + 2) % 3]) / 2.0 \\\n",
    "             for i in range(3)]\n",
    "\n",
    "def xy2bc(xy, tol=1.e-3):\n",
    "    '''Converts 2D Cartesian coordinates to barycentric coordinates.'''\n",
    "    s = [(corners[i] - midpoints[i]).dot(xy - midpoints[i]) / 0.75 \\\n",
    "         for i in range(3)]\n",
    "    return np.clip(s, tol, 1.0 - tol)\n",
    "\n",
    "class Dirichlet(object):\n",
    "    def __init__(self, alpha):\n",
    "        from math import gamma\n",
    "        from operator import mul\n",
    "        self._alpha = np.array(alpha)\n",
    "        self._coef = gamma(np.sum(self._alpha)) / \\\n",
    "                     np.prod(np.array([gamma(a) for a in self._alpha]))\n",
    "    def pdf(self, x):\n",
    "        '''Returns Dirichlet pdf at `x`.'''\n",
    "        from operator import mul\n",
    "        return self._coef * np.prod(np.array([xx ** (aa - 1)\n",
    "                                         for (xx, aa)in zip(x, self._alpha)]))\n",
    "\n",
    "def draw_pdf_contours(dist, nlevels=300, subdiv=8, **kwargs):\n",
    "    refiner = tri.UniformTriRefiner(triangle)\n",
    "    trimesh = refiner.refine_triangulation(subdiv=subdiv)\n",
    "    pvals = [dist.pdf(xy2bc(xy)) for xy in zip(trimesh.x, trimesh.y)]\n",
    "\n",
    "    plt.tricontourf(trimesh, pvals, nlevels, **kwargs)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 0.75**0.5)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl0aLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(a1=1, a2=1, a3=1)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a1=1, a2=1, a3=1):\n",
    "    draw_pdf_contours(Dirichlet([a1, a2, a3]))\n",
    "    plt.show()\n",
    "\n",
    "interact(f, a1=(1,50,1), a2=(1,50,1), a3=(1,50,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, Bayes election audits work as follows:\n",
    "\n",
    "1. Model voter preferences as random.\n",
    "1. Invent a joint prior probability distribution for voter preferences in that model.\n",
    "1. Simulate infinitely many elections from that prior model for voter preferences.\n",
    "1. Audit each of those elections; discard those for which the audit data does not match the actual audit data.\n",
    "\n",
    "+ A Bayes audit then answers the question, \"among the elections that remain after step 4, what fraction have actual winners that match the reported winners for this election?\"\n",
    "\n",
    "+ That question is about an infinite hypothetical ensemble of elections generated from an invented model. It is not about the current election. (It does use the current audit data.)\n",
    "\n",
    "+ For the posterior probability to mean anything about the current election, you also have to assume that the current election is like an election selected at random from those left in the hypothetical population after step 4.  \n",
    "    - In that case, you are asserting that step 1 makes sense for real elections and that at step 2, you accurately included all features of the distribution of voter preferences that might affect your conclusion.\n",
    "    - For instance, if there's dependence among preferences within families, or if there's a piece of \"fake news\" that affects people's preferences just before election day, those are included to the extent that they could change the posterior probability meaningfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concerns about Bayes audits\n",
    "\n",
    "+ A Bayes audit does not necessarily provide strong evidence that _this_ election found the right winners.\n",
    "\n",
    "+ Instead, it assesses whether a large fraction of synthetic elections in some hypothetical population based on some postulated (and simple) model, conditioned on the audit data, have the same winners as those reported in the current election.\n",
    "\n",
    "+ There's no closed form calculation for the risk: finding the posterior generally involves simulations, and somewhat complex software. That makes audits less than transparent. \n",
    "\n",
    "+ Bayesian audits don't necessarily provide a quantifiable limit on the risk, and they depend on a prior probability distribution that the user can choose--it's an ad hoc choice. \n",
    "\n",
    "+ Different priors lead to different posterior probabilities of the outcome being incorrect. \n",
    "\n",
    "+ It might be the case that if the prior is chosen to be \"least favorable,\" the resulting Bayes risk would be equal to the frequentist risk limit in an RLA. This would require some mathematical theory to demonstrate, and would require even more complex software to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Estimation and Inference\n",
    "\n",
    "The Bayesian approach starts with a _prior probability distribution_ $\\pi$ on $\\Theta$,\n",
    "and the likelihood function $p_\\eta(y)$.\n",
    "\n",
    "To have a prior probability distribution involves some technical restrictions\n",
    "that will not be considered here.\n",
    "(For instance, $\\Theta$ must be a measurable subset of a measurable space, and\n",
    "the likelihood function must be jointly measurable with respect to $\\eta$ and $y$.)\n",
    "\n",
    "A prior $\\pi$ is _proper_ if it has total mass 1, that is, if $\\pi(\\Theta) = 1$.\n",
    "(Some people use _improper priors_, for instance, the uniform distribution on\n",
    "the real numbers $\\Re$, which assignes infinite probability to $\\Theta = \\Re$.)\n",
    "\n",
    "Together, $\\pi$ and $p_\\eta$ imply a joint distribution of $\\theta$ and $Y$.\n",
    "The _marginal distribution_ or _predictive distribution_ of $Y$ is\n",
    "\\begin{equation*}\n",
    "\t   m(y) = \\int_\\Theta p_\\eta(y) \\, \\pi(d\\eta).\n",
    "\\end{equation*}\n",
    "\n",
    "If we observe that $Y=y$, we use that information in\n",
    "Bayes' rule to find the _posterior distribution of $\\theta$ given $Y=y$_:\n",
    "\\begin{equation*}\n",
    "\\pi (d\\eta | Y = y)  = \\frac{p_\\eta(y) \\; \\pi(d\\eta) }{m(y)} .\n",
    "\\end{equation*}\n",
    "\n",
    "The marginal density $m(y)$ can be zero, but the probability that happens is zero.\n",
    "\n",
    "In principle, the posterior distribution is a complete solution to the inference problem:\n",
    "All the information from the prior and the data is combined in the posterior\n",
    "distribution.\n",
    "The posterior distribution $\\pi_\\lambda(d \\ell | Y = y)$\n",
    "of $\\lambda[\\theta]$ is the distribution induced by the posterior distribution\n",
    "of $\\theta$:\n",
    "For any (measurable) set $A$ of possible values of $\\lambda$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Pr(\\lambda[\\theta] \\in A | Y = y) = \\int_{\\ell \\in A} \\pi_\\lambda(d \\ell | Y = y)\n",
    "\\equiv \\int_{\\eta: \\lambda[\\eta] \\in A} \\pi(d \\eta | Y = y).\n",
    "\\end{equation*}\n",
    "\n",
    "If we get additional data, we apply Bayes' rule again: the current posterior\n",
    "becomes the new prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments for the Bayesian approach\n",
    "\n",
    "One argument for the Bayesian approach is that people _are_ Bayesian: the approach is _descriptive_.\n",
    "There is considerable evidence that this is false.\n",
    "I do not know of anyone who uses Bayes' theorem to combine and\n",
    "update prior beliefs in ordinary life.\n",
    "Extensive empirical research, starting with the seminal work\n",
    "of Tversky and Kahneman (1974), shows that even people with training in\n",
    "probability fail to incorporate Bayes rule in their day-to-day reasoning.\n",
    "\n",
    "Another argument is that people _should be_ Bayesian: the approach is\n",
    "_normative_.\n",
    "According to the argument, if people are not Bayesians,\n",
    "their probability assignments are \"incoherent\" and others can make \"Dutch book\" against\n",
    "them.\n",
    "(\"Dutch book\" is a combination of bets such that no matter what the outcome, the bettor\n",
    "loses money.)\n",
    "The coherence argument depends in part on the assumption that all beliefs\n",
    "can be expressed as probability distributions--which I thinks is false.\n",
    "The \"Dutch book\" argument depends on the non-Bayesian analyst's willingness to\n",
    "cover an unlimited number of bets, and on the assumption that the Bayesian analyst's\n",
    "prior is _proper_, that is, that the total mass of the prior is 1.\n",
    "In practice, improper priors are common; for instance, it is common to use a uniform\n",
    "prior for parameters on unbounded domains.\n",
    "\n",
    "A third argument is that the choice of the prior does not matter much, because the data\n",
    "eventually \"overwhelm\" the prior: given enough\n",
    "data, you end up with essentially the same posterior distribution for \"every\" prior.\n",
    "This convergence occurs in some circumstances and not in\n",
    "others: theorems have conditions.\n",
    "\n",
    "Bayesian methods\n",
    "generally make the uncertainty appear smaller than a frequentist analysis would show,\n",
    "which makes the results more optimistic, and in part because\n",
    "they give a (numerically computable) recipe that can be applied to essentially \n",
    "any problem--if you have a prior, a fast enough computer, and a good algorithm.\n",
    "(The development of Markov Chain Monte Carlo, MCMC, made it possible to\n",
    "compute posterior distributions in a far larger class of problems, greatly\n",
    "increasing the appeal of Bayesian methods.)\n",
    "Of course, the fact that you can compute something does not automatically make\n",
    "the answer meaningful, relevant, or useful.\n",
    "\n",
    "+ A Bayesian can tell you the probability of an act of nuclear terrorism in the year 2025; a frequentist cannot make sense of what \"probability\" could mean in that context.\n",
    "\n",
    "+ A Bayesian can tell you the probability that there are civilizations of intelligent beings in other galaxies; a frequentist cannot make sense of the question.\n",
    "\n",
    "+ A Bayesian can tell you the probability that a particular coin in your pocket is fair, sight unseen; a frequentist cannot make sense of the question.\n",
    "\n",
    "+ A Bayesian can tell you the probability that a particular hypothesis is true; a frequentist thinks hypotheses are either true or false.\n",
    "\n",
    "When both frequentist and Bayesian methods can be applied,\n",
    "probability and uncertainty mean quite different things to frequentists and to \n",
    "Bayesians, as elaborated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors\n",
    "\n",
    "In the Bayesian approach, all information about $\\theta$ is expressed as\n",
    "a probability distribution, and all probabilities quantify degree of belief.\n",
    "(In the frequentist framework, probability is defined in terms of long-run relative frequency.)\n",
    "\n",
    "+ If the analyst is certain that $A$ is true, $A$ has probability 1.\n",
    "\n",
    "+ If she is certain that $A$ is false, $A$ has probability 0.\n",
    "\n",
    "+ If she believes that $A$ is true (or that $A$ will occur) with the same strength that she believes that $A$ is false (or that $A$ will not occur), $A$ has probability $1/2$.\n",
    "\n",
    "+ If she believes twice as strongly that $A$ is true as she believes $A$ is false, $A$ has probability $2/3$.\n",
    "\n",
    "More generally, if $A$ and $B$ are two statements\n",
    "and the analyst believes $r$ times as strongly\n",
    "that $A$ is true than she believes that $B$ is true, then the probability of $A$ is\n",
    "$r$ times the probability of $B$. \n",
    "\n",
    "To use the Bayesian framework to quantify uncertainty, \n",
    "one _must_ quantify beliefs and\n",
    "constraints by means of probability distributions.\n",
    "(As discussed below, there can be good reasons to use Bayesian estimators \n",
    "without necessarily using the Bayesian framework to quantify uncertainty.)\n",
    "The prior probability distribution \n",
    "quantifies the analyst's beliefs about $\\theta$ before data are collected:\n",
    "The constraint $\\theta \\in \\Theta$ means the prior probability\n",
    "distribution $\\pi$ must assign probability $1$ to $\\Theta$.\n",
    "\n",
    "Even in the simplest non-degenerate case, there are infinitely many probability distributions\n",
    "that assign probability 1 to $\\Theta$.\n",
    "In principle, it is up to the analyst to introspect to find the unique prior that reflects\n",
    "her beliefs about $\\theta$; in turn those beliefs should be constructed from previous\n",
    "experience and previous beliefs through the repeated application of Bayes' theorem.\n",
    "\n",
    "I have never seen a Bayesian analysis of real data in which\n",
    "the data analyst made a serious attempt to quantify her beliefs using a prior.\n",
    "(Nor, to my knowledge, have I met anyone who uses Bayes' theorem in real life to \n",
    "update her beliefs.)\n",
    "Instead, in my experience, priors are generally taken as given, and\n",
    "appear to be selected or justified in five ways: \n",
    "\n",
    "1. to make the calculations simple (e.g., closed form, using a \"conjugate\" prior)\n",
    "1. because the particular prior is conventional\n",
    "1. so that the prior satisfies some invariance principle\n",
    "1. with the assertion that the prior is \"uninformative,\"\n",
    "1. because the prior roughly matches the relative frequencies of values in some population.\n",
    "\n",
    "Some researchers use _Laplace's Principle of Insufficient Reason_\n",
    "to select an \"uninformative\" prior:\n",
    "If there is no reason to  believe that outcomes are not equally likely, assume that\n",
    "they are equally likely.\n",
    "Of course, the outcomes considered may depend on the parametrization,\n",
    "among other things.\n",
    "Generally, however, the principle generally leads to a prior $\\pi$ that is uniform\n",
    "on $\\Theta$.\n",
    "That is, the probability of any subset of $\\Theta$ is assumed to be proportional to its\n",
    "Lebesgue measure.\n",
    "\n",
    "For instance, the \"uninformative\" prior for a real parameter known\n",
    "to be in $\\Theta \\equiv [-1, 1]$\n",
    "is the uniform distribution on $[-1, 1]$,\n",
    "which has density $f(\\eta) = \\{ 1/2, \\eta \\in [-1, 1]; \\;\\; 0, \\mbox{ otherwise}\\}$.\n",
    "\n",
    "This prior captures the constraint $\\theta \\in [-1, 1]$, but it does far more than that:\n",
    "It assigns probabilities to all measurable subsets of $[-1, 1]$.\n",
    "For instance, it says that there is a 50% chance that $\\theta$ is positive,\n",
    "a 50% chance that the absolute value of $\\theta$ is greater than $1/2$,\n",
    "and a 90% chance that the absolute value of $\\theta$ is greater than $1/10$.\n",
    "This is not information that came from the constraint: It is information added by the\n",
    "prior.\n",
    "The constraint $\\theta \\in \\Theta$ requires $\\pi$ to assign probability $1$ to $\\Theta$,\n",
    "but it does not restrict the probabilities $\\pi$ assigns to subsets of $\\Theta$.\n",
    "Any choice of $\\pi$, \"uninformative\" or not, says more about $\\theta$ than the original\n",
    "constraint did.\n",
    "\n",
    "This problem---that turning constraints into priors adds information--grows worse as the\n",
    "dimension of the parameter $\\theta$ grows.\n",
    "For instance, suppose the unknown $\\theta$ is a vector in $n$-dimensional Euclidean space\n",
    "$\\Re^n$, and we know that $\\| \\theta \\| \\le 1$--that is, $\\Theta$ is the unit\n",
    "ball in $\\Re^n$.\n",
    "\n",
    "The volume of a spherical shell from radius $1-\\epsilon$ to $1$ is a larger and larger\n",
    "fraction of the volume of the unit sphere as the dimension $n$ grows.\n",
    "For any $\\alpha \\in (0, 1)$ and $\\epsilon \\in (0, 1)$, there is a dimension\n",
    "$n$ so that the (uniform) probability of\n",
    "$\\{ \\eta : \\| \\eta \\| \\in [1-\\epsilon, 1]\\}$,\n",
    "the spherical shell from radius $1-\\epsilon$ to $1$,\n",
    "is at least $\\alpha$.\n",
    "\n",
    "What does this mean?\n",
    "Starting with the constraint that $\\| \\theta \\| \\le 1$--and without collecting any data--we\n",
    "end up with arbitrarily high certainty that in fact $\\| \\theta \\| \\ge 1-\\epsilon$.\n",
    "It is the prior that gives us this certainty, not the constraint.\n",
    "The prior is not \"uninformative\" about the norm.\n",
    "\n",
    "Conversely, suppose we put a rotationally invariant prior on the unit ball in such a \n",
    "way that the marginal\n",
    "distribution of the norm is uniform.\n",
    "Consider the ball of radius $1-\\epsilon$.\n",
    "It has probability $1-\\epsilon$ regardless of the dimension of the space, even though\n",
    "its volume is a negligible fraction of the volume of the unit ball if the dimension of the\n",
    "space is large.\n",
    "This prior is not \"uninformative\" with respect to volume:  It says that the model is extremely\n",
    "likely to be in a subset of $\\Theta$ that has very small volume.\n",
    "\n",
    "The approach collapses in infinite-dimensional spaces.\n",
    "For instance, suppose $\\theta$ is an element of an infinite-dimensional separable Hilbert space,\n",
    "and that the constraint set $\\Theta$ is rotationally invariant\n",
    "(an example would be $\\Theta \\equiv \\{ \\eta : \\| \\eta \\| \\le 1 \\}$).\n",
    "If the prior respects that rotational invariance, it is a theorem that\n",
    "the prior either assigns probability 1 to the event that $\\theta = 0$\n",
    "or it assigns probability 1 to the event that the norm of $\\theta$ is \n",
    "infinite--contradicting\n",
    "the constraint!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Bayesian priors and estimates\n",
    "Prior probability distributions--and hence posterior distributions--are quantifications\n",
    "of the analyst's degree of belief.\n",
    "As such, they change the subject from the experiment and the external world \n",
    "to the analyst's state of mind.\n",
    "Suppose I claim that my prior probability distribution for the load on a structure as a fraction of\n",
    "its breaking strength is the uniform distribution on $[0, 1]$.\n",
    "I am right if that accurately reflects what I believe.\n",
    "I am wrong if it does not accurately reflect what I believe.\n",
    "The relationship between the prior and the world has no bearing on whether I am right or wrong.\n",
    "Experiments that could show I am wrong involve checking what I actually believe--for instance,\n",
    "psychological testing or determining what bets I would take at what odds--rather than\n",
    "measurements of the structure or similar structures.\n",
    "\n",
    "Two analysts can have very different priors and both be right, because what makes\n",
    "a prior right is that it correctly quantifies the analyst's belief.\n",
    "If I do not share your prior beliefs in detail, then even if we agree on the likelihood function and the data,\n",
    "we will have different posterior distributions for $\\theta$.\n",
    "Why should your posterior distribution matter to me?\n",
    "\n",
    "If a Bayesian analysis results in the statement, \"there is a 99.9% chance that the applied load\n",
    "will be less than 10% the breaking strength,\"\n",
    "it means that the analyst is quite sure that the load will be low,\n",
    "but it is not at all clear what it means about safety.\n",
    "\n",
    "For a different prior, an equally correct analysis might find that\n",
    "there is a 99.9% chance that the applied load will exceed 90% the breaking strength.\n",
    "If so, a Bayesian analysis might appropriately be viewed with skepticism.\n",
    "\n",
    "On the other hand, if one could show that no matter what prior is used, there is at least a 99.9% chance\n",
    "that the applied load will be less than 10% of the breaking strength, the Bayesian position seems much\n",
    "more persuasive.\n",
    "\n",
    "The utility and persuasiveness of Bayesian analyses may hinge on the \n",
    "sensitivity of the conclusions to the choice of prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The role of the posterior\n",
    "\n",
    "One fundamental feature (indeed, an axiom) of Bayesian estimation and inference is that the posterior distribution of the model given the data contains **all** the information about the model, including information from the prior and from the data. \n",
    "No information other than the posterior is needed to completely describe our state of knowledge about the parameter.\n",
    "\n",
    "The following example from LeCam (1977) shows that this is problematic.\n",
    "\n",
    "Suppose we are interested in the probability $\\theta$ that a particular coin lands heads. We start with a prior probability distribution for $\\theta$.\n",
    "Suppose we start with a Dirichlet prior, which, in this case, simplifies to a prior density\n",
    "\n",
    "\\begin{equation*}\n",
    "   f(\\theta) \\propto \\theta^{a_1} (1-\\theta)^{a_2}.\n",
    "\\end{equation*}\n",
    "\n",
    "Since the Dirichlet distribution is conjugate to the multinomial, and the binomial distribution is a special case of the multinomial, after tossing the coin $n$ times independently and updating the prior to find the posterior, the posterior will be of the form\n",
    "\n",
    "\\begin{equation*}\n",
    "   f(\\theta) \\propto \\theta^{a_1+k} (1-\\theta)^{a_2+n-k},\n",
    "\\end{equation*}\n",
    "\n",
    "where $k$ is the number of heads among the $n$ tosses.\n",
    "\n",
    "A Bayesian now reports the posterior distribution. For instance, the posterior might turn out to be\n",
    "\n",
    "\\begin{equation*}\n",
    "  f(\\theta) \\propto \\theta^{100} (1-\\theta)^{100}.\n",
    "\\end{equation*}\n",
    "\n",
    "The problem is this: that posterior might be the result of a \"flat\" prior after 200 tosses, or just as easily be simply the prior after zero tosses.\n",
    "\n",
    "To quote LeCam:\n",
    "\n",
    "> If the neo-Bayesian has to give not only his final measure, but also his\n",
    "initial measure, the description of the experiment and the result obtained\n",
    "there, the simplicity of the Bayes approach is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Frequentist approach\n",
    "\n",
    "The main difference between Bayesian and frequentist approaches to constraints\n",
    "is the difference between\n",
    "believing that $\\theta$ is drawn at random from $\\Theta$ according to the \n",
    "known distribution $\\pi$\n",
    "and believing that $\\theta$ is simply an unknown element of $\\Theta$.\n",
    "(The interpretation of probability also differs substantially between the two points of view.)\n",
    "\n",
    "For Bayesians, probability quantifies degree of belief.\n",
    "\n",
    "For frequentists, probability has to do with long-term regularities in repeated trials.\n",
    "The probability of an event is defined to be the long-run limiting relative frequency with which\n",
    "the event occurs in independent trials under 'essentially identical' conditions.\n",
    "(If the conditions were exactly identical,\n",
    "then--within classical physics, at least--the outcome would be identical.)\n",
    "\n",
    "The canonical random experiment, tossing a fair coin, will give heads every time or tails every time\n",
    "if the coin is tossed with initial conditions that are similar enough.\n",
    "Defining \"essentially identical\" is a hard problem for the frequentist approach.\n",
    "Another issue is the assumption that repeated trials result in relative frequencies that converge to\n",
    "a limit.\n",
    "\n",
    "The frequentist approach restricts the \n",
    "kinds of things one can make probability statements about:\n",
    "Only trials that, in principle, can be repeated indefinitely lead to\n",
    "probabilities.\n",
    "For instance, a conventional frequentist approach cannot make sense of questions like\n",
    "\"what is the chance of an act of nuclear terrorism in the year 2025?\" or\n",
    "\"what is the chance of an earthquake with magnitude 8.0 or above in the San Francisco Bay\n",
    "Area in the next 20 years?,\" much less supply\n",
    "numerical values for those chances.\n",
    "\n",
    "In the frequentist approach, probability generally comes from the measurement process or the\n",
    "experiment, not from the parameter or \"state of the world.\"\n",
    "There is statistical uncertainty because there is sampling variability or measurement error\n",
    "or random assignment of subjects to treatments,\n",
    "not because the underlying parameter is random.\n",
    "\n",
    "There is no need to assume that $\\theta$ is random to use the frequentist approach,\n",
    "so there is no need to invent a prior distribution $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing uncertainty\n",
    "There are many ways to quantify uncertainty.\n",
    "We shall consider two, each of which has a Bayesian and a frequentist variant:\n",
    "mean squared error (a frequentist measure) and posterior mean squared error\n",
    "(the related Bayesian measure); and confidence sets (a frequentist construct)\n",
    "and credible regions (the related Bayesian construct).\n",
    "\n",
    "### Mean Squared Error\n",
    "Recall that we have assumed that $\\lambda[\\theta]$ takes values in a Hilbert space.\n",
    "\n",
    "Suppose we choose to estimate $\\lambda[\\theta]$ by the estimator $\\widehat{\\lambda}(Y)$,\n",
    "a (measurable) map from possible data values $y$ into possible values of $\\lambda[\\eta]$,\n",
    "$\\eta \\in \\Theta$.\n",
    "\n",
    "The mean squared error (MSE) of $\\widehat{\\lambda}$ when $\\theta = \\eta$ is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mbox{MSE}(\\widehat{\\lambda}(Y), \\eta) \\equiv \\mathbb{E}_\\eta \\| \\widehat{\\lambda}(Y) - \\lambda[\\eta] \\|^2.\n",
    "\\end{equation*}\n",
    "\n",
    "+ MSE depends on $\\eta$.\n",
    "\n",
    "+ The expectation is with respect to $\\Pr_\\eta$, the distribution of the data $Y$ on the assumption that $\\theta = \\eta$.\n",
    "\n",
    "+ If we get to select the estimator $\\widehat{\\lambda}$, we might seek an estimator that makes $\\mbox{MSE}(\\widehat{\\lambda}(Y), \\theta)$ small.\n",
    "\n",
    "+ Since the true value of $\\theta$ is unknown, in general we cannot select the estimator $\\widehat{\\lambda}$ to make the actual MSE as small as possible.\n",
    "\n",
    "+ Instead, we might choose $\\widehat{\\lambda}$ to make the largest MSE as $\\eta$ ranges over $\\Theta$ as small as possible: _minimax MSE estimator_.\n",
    "\n",
    "Related Bayesian measure: posterior mean squared error (PMSE),\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mbox{PMSE}(\\widehat{\\lambda}(y), \\pi) \\equiv \\mathbb{E}_\\pi \\| \\widehat{\\lambda}(y) - \\lambda[\\eta] \\|^2.\n",
    "\\end{equation*}\n",
    "\n",
    "+ PMSE depends on $\\pi$ and the observed value of $y$.\n",
    "\n",
    "+ The expectation is with respect to the posterior distribution of $\\theta$ given $Y = y$.\n",
    "\n",
    "+ Since $\\pi$ is known, we can select (for each $y$) the estimator that has the smallest possible $\\mbox{PMSE}$.\n",
    "\n",
    "+ That estimator, the Bayes estimator for PMSE, is the _marginal posterior mean_, the mean of\n",
    "$\\pi_\\lambda(d \\ell | Y=y)$, the marginal posterior distribution of $\\lambda[\\theta]$\n",
    "given $Y$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\widehat{\\lambda}_\\pi(y) \\equiv \\int \\ell \\pi_\\lambda(d \\ell | Y = y).\n",
    "\\end{equation*}\n",
    "\n",
    "MSE and PMSE both involve expectations of the squared norm of the\n",
    "difference between the parameter estimate and the true value of the parameter,\n",
    "but are conceptually quite different:\n",
    "\n",
    "+ The MSE is an expectation with respect to the distribution of the data $Y$, holding the parameter $\\theta = \\eta$ fixed\n",
    "\n",
    "+ PMSE is an expectation with respect to the posterior distribution of $\\theta$, holding the data $Y = y$ fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Sets and Credible Regions\n",
    "\n",
    "Suppose $\\alpha \\in (0, 1)$.\n",
    "A random set $\\mathcal{I}(Y)$ of possible values of $\\lambda$ is\n",
    "a $1-\\alpha$ confidence set for $\\lambda[\\theta]$ if\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Pr_\\eta \\{ \\mathcal{I}(Y) \\ni \\lambda[\\eta] \\} \\ge 1 - \\alpha, \\;\\; \\forall \\eta \\in \\Theta.\n",
    "\\end{equation*}\n",
    "\n",
    "+ The probability on the left is with respect to the distribution of the data $Y$, holding $\\eta$ fixed.\n",
    "\n",
    "+ In the frequentist view, once the data are collected and we know that $Y = y$,\n",
    "there is no longer any probability: $\\mathcal{I}(y)$ is some particular\n",
    "set and the value $\\lambda[\\theta]$ is some particular (but unknown)\n",
    "vector, so either $\\mathcal{I}(y)$ contains $\\lambda[\\theta]$ or it does not.\n",
    "\n",
    "+ \"Coverage probability\" of the rule $\\mathcal{I}$ is the (smallest) chance that\n",
    "$\\mathcal{I}(Y)$ will include $\\lambda[\\eta]$ as $\\eta$ ranges over $\\Theta$,\n",
    "with $Y$ generated from $\\Pr_\\eta(y)$.\n",
    "\n",
    "A related Bayesian construct is a _posterior credible region_.\n",
    "\n",
    "A set $\\mathcal{I}(y)$ of possible values of $\\lambda$ is\n",
    "a $1-\\alpha$ posterior credible region for $\\lambda[\\theta]$ if\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Pr_{\\pi( d\\theta | Y=y)} (\\lambda[\\theta] \\in \\mathcal{I}(y))\n",
    "\\equiv \\int_{\\mathcal{I}(y)} \\pi_\\lambda(d \\ell | Y = y) \\ge 1-\\alpha.\n",
    "\\end{equation*}\n",
    "\n",
    "+ The probability on the left is with respect to the marginal posterior distribution of \n",
    "$\\lambda[\\theta]$, holding the data fixed:\n",
    "It is is the posterior probability that\n",
    "$\\mathcal{I}(y)$ contains $\\lambda[\\theta]$ given that $Y = y$.\n",
    "\n",
    "+ In the Bayesian view, once the data are collected and we know that $Y = y$, there is still probability, because the value of $\\theta$ itself remains random: its value is uncertain, and all uncertainty is represented as probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision theory\n",
    "\n",
    "Decision theory treats estimation as a two-player game: Nature versus analyst.\n",
    "The game frequentists play has slightly different rules from the game Bayesians play.\n",
    "\n",
    "+ According to both sets of rules, Nature and the analyst know $\\Theta$, $\\Pr_\\eta$ for all $\\eta \\in \\Theta$, $\\lambda$, and the payoff rule (loss function) $\\loss(\\ell, \\lambda[\\eta])$, the amount of money the analyst loses if she guesses that $\\lambda[\\eta] = \\ell$ when in fact $\\theta = \\eta$.\n",
    "\n",
    "+ Nature selects an element $\\theta$ of $\\Theta$.\n",
    "\n",
    "+ The analyst selects an estimator $\\widehat{\\lambda}$.\n",
    "\n",
    "+ The analyst does not know the value of $\\theta$ and Nature does not know what estimator the analyst plans to use.\n",
    "\n",
    "+ Data $Y$ are generated using the value of $\\theta$ that Nature selected; the data are plugged into $\\widehat{\\lambda}$, and $\\loss(\\widehat{\\lambda}(Y), \\lambda[\\theta])$ is calculated.\n",
    "\n",
    "+ Holding $\\theta$ constant, a new value of $Y$ is generated, and $\\loss(\\widehat{\\lambda}(Y), \\lambda[\\theta])$ is calculated again. This is repeated many times.\n",
    "\n",
    "+ The analyst has to pay the average value of $\\loss(\\widehat{\\lambda}(Y), \\lambda[\\theta])$ over all those values of $Y$, the _risk of $\\widehat{\\lambda}$ at $\\theta$_, denoted $\\rho_\\theta(\\widehat{\\lambda}, \\lambda[\\theta])$.\n",
    "\n",
    "+ The analyst's goal is to lose as little as possible in repeated play.\n",
    "\n",
    "#### In the Bayesian version of the game, Nature selects $\\theta$ at random according to the prior distribution $\\pi$, and the analyst knows $\\pi$.\n",
    "\n",
    "#### In the frequentist version of the game, the analyst does not know how Nature will select $\\theta$ from $\\Theta$.\n",
    "\n",
    "This is perhaps the most important difference between the \n",
    "frequentist and Bayesian viewpoints:\n",
    "Bayesians claim to know more about how Nature generates the data.\n",
    "\n",
    "+ A cautious frequentist might wish to select $\\widehat{\\lambda}$ to minimize her worst-case risk, on the assumption that Nature might play deliberately to win as much as possible.\n",
    "\n",
    "+ An estimator that minimizes the worst-case risk over $\\eta \\in \\Theta$ (for some specified class of estimators) is called a _minimax estimator_; its maximum risk is the _minimax risk_.\n",
    "\n",
    "+ Minimax estimates are not the only option for frequentists (indeed, in many problems the minimax estimator is not known, and frequentists rely on estimators that have simple recipes and generally good asymptotic properties, e.g., maximum likelihood), but minimaxity is a common principle for optimality, as is _minimax regret_.\n",
    "\n",
    "+ A Bayesian might instead select the estimator that minimizes the _average_ risk on the assumption that Nature selects $\\theta$ at random following the prior probability distribution $\\pi$.\n",
    "\n",
    "+ An estimator that minimizes the average risk when $\\theta$ is selected from $\\pi$ (for some specified class of estimators) is called a _Bayes estimator_; its average risk for prior $\\pi$ is the _Bayes risk_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duality between Bayes Risk and Minimax Risk\n",
    "\n",
    "+ The Bayes risk depends not only on $\\Theta$, the distributions \n",
    "$\\{ \\Pr_\\eta: \\eta \\in \\Theta\\}$, the parameter $\\lambda$, and the loss function $\\loss$: It also depends on $\\pi$.\n",
    "\n",
    "+ Consider allowing $\\pi$ to vary over a (suitably) rich set of possible priors. The prior $\\pi^*$ for which the Bayes risk is largest is the _least favorable_ prior.\n",
    "\n",
    "+ The least favorable prior typically is not the \"uninformative\" or \"flat\" prior.\n",
    "\n",
    "+ Under some technical conditions, the Bayes risk for the least favorable prior is equal to the minimax risk.\n",
    "\n",
    "+ If the Bayes risk is smaller than the minimax risk, it is because the prior added information not present in the constraint itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist properties of Bayes Estimates\n",
    "\n",
    "A frequentist need not believe the prior or even think that the parameter\n",
    "is random to use a Bayesian estimator.\n",
    "\n",
    "The Bayesian approach gives a recipe for calculating\n",
    "an estimate, just like maximum likelihood gives a recipe for calculating an estimate.\n",
    "\n",
    "The performance of that estimate can be measured using \n",
    "frequentist constructs without relying on the posterior distribution or the interpretation\n",
    "of the prior or the posterior.\n",
    "\n",
    "Sometimes, Bayesian estimates have good frequentist properties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory and practice\n",
    "\n",
    "In complex scientific applications, there are very few \"orthodox\" frequentist _or_ \n",
    "Bayesian analyses.\n",
    "\n",
    "Models and priors tend to be chosen for convenience or tractability--or to respond to\n",
    "criticisms by adding layers of complexity.\n",
    "To paraphrase David Freedman, frequentist analyses tend to make up models, and Bayesian analyses tend to make up priors.\n",
    "I would argue that _both_ camps tend to make up models, by which I mean the mapping\n",
    "$\\eta \\rightarrow \\Pr_\\eta$ and the set $\\Theta$.\n",
    "And both groups tend to _invent_ constraints \n",
    "far more stringent than the constraints that actually come from the underlying \n",
    "scientific problem,\n",
    "artificially reducing the apparent uncertainty.\n",
    "\n",
    "If you point out a missing source of uncertainty or variability, \n",
    "a frequentist may model it; a Bayesian may model it and put a prior on any new parameters.\n",
    "The result---in both camps---is a tendency towards rococo \n",
    "recursion in which the embellishments have embellishments, \n",
    "and the weakness of the foundation is obscured by the complexity of the edifice.\n",
    "In the end, one still has a model or a prior, but an incomprehensible one that can't \n",
    "possibly correspond to anyone's true beliefs, or to Nature. \n",
    "\n",
    "The verse by Augustus de Morgan (known for de Morgan's rules, which both frequentists and Bayesians rely on) \n",
    "describes multilevel modeling and hierarchical priors well:\n",
    "\n",
    "> Great fleas have little fleas upon their backs to bite 'em,\n",
    "> And little fleas have lesser fleas, and so _ad infinitum_.\n",
    "> And the great fleas themselves, in turn, have greater fleas to go on,\n",
    "> While these again have greater still, and greater still, and so on.\n",
    "\n",
    "Frequentist analyses of complex problems often have model-selection phases (e.g., deciding which variables to use in a regression model) \n",
    "that are not accounted for properly in the quantification\n",
    "of uncertainty through confidence sets and so on.\n",
    "This problem has been recognized for decades, but only recently have rigorous\n",
    "methods to deal with it been proposed, and only in quite limited contexts.\n",
    "\n",
    "The best frequentist analyses tend to be bespoke: tailored to the scientific \n",
    "details of the problem.\n",
    "That requires substantive and statistical knowledge.\n",
    "Calculating frequentist estimates may require solving\n",
    "difficult--and sometimes numerically intractable--constrained numerical optimization problems.\n",
    "The set of tractable problems will grow over time, as algorithms improve and computational power increases.\n",
    "\n",
    "The advent of fast MCMC codes makes it possible to compute Bayesian estimates in a broad\n",
    "variety of applications, sometimes without much scientific thought: \n",
    "insert a prior, a likelihood, and data, run MCMC to sample the posterior,\n",
    "and out comes an estimate and an uncertainty appraisal.\n",
    "What that appraisal means is generally not examined, much less questioned.\n",
    "The warning \"if all you have is a hammer, everything everything looks like a nail\" is apropos. \n",
    "This is especially true if you love your hammer; it is especially pernicious if true nails are rare.\n",
    "\n",
    "In much the same way, frequentists often rush to apply the latest modeling technique \n",
    "to every set of data with the right \"signature,\" with little attention to how the\n",
    "data were collected or the underlying science.\n",
    "\n",
    "The result in both cases is unlikely to advance scientific knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "+ In many applications, there is prior information about unknown parameters. For instance, \n",
    "    - masses and energies are nonnegative and finite\n",
    "    - velocities do not exceed the speed of light\n",
    "    - a financial overcharge cannot exceed the entire charge\n",
    "    - vote-counting error that favored the winner cannot exceed the number of ballots\n",
    "\n",
    "+ Frequentist methods can use constraints directly.\n",
    "\n",
    "+ Bayesian methods require augmenting the constraints with prior probability distributions.\n",
    "\n",
    "+ The difference between the frequentist and Bayesian viewpoints is that Bayesians\n",
    "_claim to know more about how the data are generated_:\n",
    "\n",
    "    - Frequentists claim to know that the parameter $\\theta \\in \\Theta$, but not how $\\theta$ was selected from $\\Theta$.\n",
    "\n",
    "    - Bayesians claim to know that the parameter $\\theta$ was selected at random from $\\Theta$ according to a prior probability distribution $\\pi$ known to them.\n",
    "\n",
    "    - Both claim to know $\\Pr_\\eta$, the probability distribution that the data would have if the value of $\\theta$ is $\\eta$, for each $\\eta \\in \\Theta$.\n",
    "\n",
    "+ In Bayesian analysis, the prior probability distribution captures the analyst's beliefs about the parameter before the data are collected.\n",
    "\n",
    "    - The prior is updated using the data to construct the posterior distribution via Bayes' rule.\n",
    "\n",
    "    - The posterior combines the analyst's prior beliefs with information from the data.\n",
    "\n",
    "    - An analyst with different prior beliefs will in general arrive at a different posterior distribution.\n",
    "\n",
    "    - To measure a probability in the Bayesian framework is to discover what the analyst thinks, while to measure a probability in the frequentist framework is to discover empirical regularities.\n",
    "\n",
    "+ Because of the difference in interpretations of probability, the Bayesian framework allows probability statements to be made about a much larger range of phenomena. But the probability might be relevant only to the person performing the analysis.\n",
    "\n",
    "+ Bayesian and frequentist measures of uncertainty differ.\n",
    "\n",
    "    - For instance, mean squared error and posterior mean squared error are expectations of the same quantity, but with respect to different distributions:\n",
    "        + MSE is an expectation with respect to the distribution of the data, holding the parameter fixed\n",
    "        + PMSE is an expectation with respect to the posterior distribution of the parameter, holding the data fixed.\n",
    "\n",
    "    - coverage probability and credible level are the chance that a set contains the parameter, but\n",
    "        + coverage probability is computed with respect to the distribution of the data, holding the parameter fixed and allowing the set to vary randomly\n",
    "        + credible level is computed with respect to posterior distribution of the parameter, holding the data and the set fixed and allowing the parameter to vary randomly.\n",
    "\n",
    "+ The interpretation and intended use \n",
    "of the results matter, and these may depend on the application.\n",
    "\n",
    "    - Is the parameter in question actually random?\n",
    "    - If so, is its prior distribution known?\n",
    "    - Which is the more interesting question: what would happen if Nature generated a new value of the parameter and the data happened to remain the same, or what would happen for the same value of the parameter if the measurement were repeated?\n",
    "\n",
    "+ Under some conditions, the largest Bayes risk as the prior is allowed to vary is equal to smallest maximum risk (the minimax risk) of any estimator as the parameter is allowed to vary.\n",
    "\n",
    "    - If the Bayes risk for a given prior is less than the minimax risk, the prior added information not present in the constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "+ Evans, S.N., B. Hansen, and P.B. Stark, 2005. Minimax Expected Measure Confidence Sets for Restricted Location Parameters, Bernoulli, 11, 571–590. Also Tech. Rept. 617, Dept. Statistics Univ. Calif Berkeley (May 2002, revised May 2003). Preprint: \n",
    "https://www.stat.berkeley.edu/~stark/Preprints/617.pdf\n",
    "\n",
    "+ Freedman, D.A., 1995. Some issues in the foundations of statistics, _Foundations of Science_, _1_, 19--39. https://doi.org/10.1007/BF00208723\n",
    "\n",
    "+ LeCam, L., 1977.  Note on metastatistics or 'An essay toward stating a problem in the doctrine of chances,' _Synthese_, _36_, 133-160.\n",
    "\n",
    "+ Rivest, R.L., and E. Shen, 2012. A Bayesian Method for Auditing Elections, 2012 Electronic Voting Technology Workshop / Workshop on Transparent Elections (EVT/WOTE 2012), https://www.usenix.org/system/files/conference/evtwote12/evtwote12-final30.pdf\n",
    "\n",
    "+ Schafer, C.M., and P.B. Stark, 2009. Constructing Confidence Sets of Optimal Expected Size. Journal of the American Statistical Association, 104, 1080–1089. Reprint: \n",
    "https://www.stat.berkeley.edu/~stark/Preprints/schaferStark09.pdf\n",
    "\n",
    "+ Stark, P.B. and D.A. Freedman, 2003. What is the Chance of an Earthquake? in Earthquake Science and Seismic Risk Reduction, F. Mulargia and R.J. Geller, eds., NATO Science Series IV: Earth and Environmental Sciences, v. 32, Kluwer, Dordrecht, The Netherlands, 201–213. Preprint: \n",
    "https://www.stat.berkeley.edu/~stark/Preprints/611.pdf\n",
    "\n",
    "+ Stark, P.B. and L. Tenorio, 2010. A Primer of Frequentist and Bayesian Inference in Inverse Problems. In _Large Scale Inverse Problems and Quantification of Uncertainty_, Biegler, L., G. Biros, O. Ghattas, M. Heinkenschloss, D. Keyes, B. Mallick, L. Tenorio, B. van Bloemen Waanders and K. Willcox, eds. John Wiley and Sons, NY. Preprint:\n",
    "https://www.stat.berkeley.edu/~stark/Preprints/freqBayes09.pdf\n",
    "\n",
    "+ Stark, P.B., 2015. Constraints versus priors. _SIAM/ASA Journal on Uncertainty Quantification, 3_(1), 586–598. doi:10.1137/130920721, Reprint: http://epubs.siam.org/doi/10.1137/130920721, Preprint: https://www.stat.berkeley.edu/~stark/Preprints/constraintsPriors15.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10400 -1400\n",
      "9600 600\n"
     ]
    }
   ],
   "source": [
    "Nw1= 4550\n",
    "Nl1=4950\n",
    "N1=10000\n",
    "Nw2=750\n",
    "Nl2=150\n",
    "N2=1000\n",
    "V = 200\n",
    "print(Nw1-Nl1-N1,V-(Nw2-Nl2+N2))\n",
    "print(Nw1-Nl1+N1,V-(Nw2-Nl2-N2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
