{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving least squares problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, if $Y = X\\beta + \\epsilon$ with $Y, \\epsilon \\in \\Re^n$, $X$ an $n$ by $p$ matrix, $\\beta \\in \\Re^p$, $p \\le n$, and $\\mathrm{rank}(X) = p$, then\n",
    "\n",
    "$$ \\hat{\\beta}_{\\mathrm{OLS}} = \\arg \\min_{\\gamma \\in \\Re^p} \\| Y - X\\gamma \\|^2 =  (X^TX)^{-1} X^T Y.$$\n",
    "\n",
    "We will derive this, then explain why--although the solution is mathematically correct--it is not a good way to find \n",
    "$\\hat{\\beta}_{\\mathrm{OLS}}$ in practice, because it is numerically less stable than other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The normal equations\n",
    "\n",
    "Note that $\\| Y - X\\gamma \\|^2 = (Y - X\\gamma)^T(Y - X\\gamma)$ is a quadratic function of $\\gamma$. \n",
    "Extrema with respect to $\\gamma$ will be at stationary points.\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\gamma} \\| Y - X\\gamma \\|^2 = 2 (Y - X\\gamma)^T X = 2 X^T (Y - X\\gamma) = 2 X^TY - X^TX\\gamma.$$\n",
    "\n",
    "Note that the 2nd derivative with respect to $\\gamma$ is $X^TX$, a square, symmetric matrix.\n",
    "\n",
    "$\\gamma_0$ is a stationary point of $\\| Y - X\\gamma \\|^2$ if $ X^TX \\gamma_0 = X^T Y$. These linear relations are called the _normal equations_.\n",
    "\n",
    "If $X$ has rank $p$, then $X^TX$ is positive definite, and hence invertible, so the normal equations have a unique solution:\n",
    "\n",
    "$$ \\gamma_0 = (X^TX)^{-1}X^TY.$$\n",
    "\n",
    "Since $X^TX$ (the 2nd derivative) is positive definite, this is a minimum, not a maximum. Therefore, the value of $\\beta$ that minimizes $\\|Y - X\\beta\\|^2$ is $\\hat{\\beta} = (X^TX)^{-1}X^TY$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing the optimum\n",
    "\n",
    "### Reminders\n",
    "\n",
    "Two vectors of the same dimension, $x$ and $y$, are _orthogonal_ if $x^Ty = 0$.\n",
    "\n",
    "A very useful result is that the residual vector, $e = Y - X\\hat{\\beta}$, is orthogonal to the subspace spanned by the columns of $X$; that is, $X^Te = 0$.\n",
    "\n",
    "Let $\\mathrm{colspan}(X)$ denote the span of the columns of $X$, that is,\n",
    "\n",
    "$$ \\mathrm{colspan}(X) \\equiv \\{ X \\gamma : \\gamma \\in \\Re^p \\}.$$\n",
    "\n",
    "Suppose not. Any vector $e \\in \\Re^n$ can be decomposed into a component that is contained in the subspace spanned by the columns of $X$ and a component that is orthogonal to that subspace.\n",
    "Write $e = e_\\parallel + e_\\perp$, where $e_\\parallel \\in \\mathrm{\\colspan}(X)$ and $e_\\perp \\perp \\mathrm{\\colspan}(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator norm of a matrix\n",
    "\n",
    "Suppose $X$ is an $n$ by $p$ matrix.\n",
    "\n",
    "$$ \\| X \\| \\equiv \\sup_{\\gamma \\in \\Re^p} \\frac{ \\| X\\gamma \\|}{\\|\\gamma \\|}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
