{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing, Combinations of Tests, and Stratified Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypotheses\n",
    "\n",
    "A _statistical hypothesis_ is an assertion about the probability distribution $\\mathbb{P}$ of the data $X$.\n",
    "\n",
    "A _simple_ hypothesis is an assertion that completely specifies the probability distribution of the data, e.g., $\\mathbb{P} = \\mathbb{P}_0$ for some distribution $\\mathbb{P}_0$. Here are some simple hypotheses:\n",
    "\n",
    "* $X \\sim N(0,1)$ The data have a standard normal distribution.\n",
    "* $X \\sim U[0,1]$ The data have a standard uniform distribution.\n",
    "* $X \\sim N(\\mu, \\Sigma)$ for a given $\\mu \\in \\Re^n$ and positive semi-definite $n \\times n$ matrix $\\Sigma$. The data are jointly normally distributed with mean $\\mu$ and covariance matrix $\\Sigma$.\n",
    "* $\\{X_j\\}\\; \\mbox{IID} \\; U[0, 17]$. The components of the data are independent and identically distributed with a uniform distribution on $[0, 17]$.\n",
    "* $X \\sim \\mbox{Binom}(n, p)$ for given values of $n$ and $p$. The data have a binomial distribution with parameters $n$ and $p$.\n",
    "\n",
    "A _composite_ hypothesis is an assertion that does not completely specify the distribution; it only says the\n",
    "distribution is in some specified set of distributions. \n",
    "E.g., $\\mathbb{P} \\in \\mathcal{P}_0$ for some set\n",
    "$\\mathcal{P}_0$ of distributions. \n",
    "Here are some composite null hypotheses:\n",
    "\n",
    "* $X \\sim N(\\mu,1)$, $\\mu \\in [0, 7]$ The data have a normal distribution with variance 1 and mean between 0 and 7. \n",
    "* $X \\sim \\mbox{Binom}(n, p)$, $p > 1/2$ ($n$ given). The data have a binomial distribution with known parameter $n$ and unknown parameter $p > 1/2$.\n",
    "* $\\mathbb{E}X=0$ The expected value of the data is zero.\n",
    "* $\\mathbb{E}X \\le 1/2$ The expected value of the data is at most 1/2.\n",
    "* The distribution $\\mathbb{P}$ of $X$ has at most 3 modes.\n",
    "* $\\{X_j\\}_{j=1}^n$ are _exchangeable_ (A collection of random variables is _exchangeable_ if their joint distribution is invariant under permuting their labels. If a collection of random variables is IID, it is also exchangeable.)\n",
    "* The distribution of $X$ is spherically symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test a statistical hypothesis, one specifies a set $A$ of possible data values (before examining the actual data). If the data fall inside that set, i.e., if $X \\in A$, the null hypothesis is _not rejected_; otherwise, the null hypothesis is _rejected_.\n",
    "Each (measurable) set $A$ implicitly defines a hypothesis test.\n",
    "The set $A$ is called the _acceptance region_ for the test.\n",
    "\n",
    "Sometimes the set $A$ is defined explicitly, but\n",
    "more often it is defined implicitly in terms of a _test statistic_, a function of the data that does not depend on any unknown parameters.\n",
    "\n",
    "For instance an acceptance region for testing the hypothesis that $\\{X_j\\}_{j=1}^n$ are IID $N(0,1)$ might be\n",
    "\\begin{eqnarray*}\n",
    "A \\equiv \\left \\{x = \\{x_1, \\ldots, x_n\\}: \\frac{1}{n} \\sum_{j=1}^n x_j \\le \\frac{c}{\\sqrt{n}} \\right \\}.\n",
    "\\end{eqnarray*}\n",
    "In this example, the test statistic is the sample mean, and the region $A$ is the set of all data for which the\n",
    "sample mean does not exceed $c/\\sqrt{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside on notation\n",
    "\n",
    "The following all mean the same thing, namely, the probability that the random variable $X$ takes a value in the set $A$ if the distribution of $X$ is $\\mathbb{P}_0$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Pr \\{X \\in A || X \\sim \\mathbb{P}_0 \\}\\\\\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\Pr_{X \\sim \\mathbb{P}_0} \\{X \\in A\\}\\\\\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}_0 \\{X \\in A\\}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance level\n",
    "\n",
    "The _significance level_ of the test $A$ of the simple null hypothesis $X \\sim \\mathbb{P}_0$ is\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\alpha \\equiv \\mathbb{P}_0 \\{X \\notin A \\}.\n",
    "\\end{equation*}\n",
    "That is, it is the probability that the test rejects the null hypothesis when the null hypothesis is true.\n",
    "\n",
    "The _significance level_ of the test $A$ of the composite null hypothesis $H_0: X \\sim \\mathbb{P} \\in \\mathcal{P}_0$ is\n",
    "\\begin{equation*}\n",
    "   \\alpha \\equiv \\sup_{\\mathbb{P_0} \\in \\mathcal{P}_0} \\mathbb{P}_0 \\{X \\notin A \\}.\n",
    "\\end{equation*}\n",
    "That is, it is the largest probability that that the test rejects the null hypothesis when the null hypothesis is true.\n",
    "\n",
    "When $\\mathcal{P}_0$ contains only one distribution, the two definitions coincide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power\n",
    "\n",
    "The _power_ of the test $A$ against the simple hypothesis $H_1: X \\sim \\mathbb{P}_1$ is \n",
    "\n",
    "\\begin{equation*}\n",
    "   1-\\beta \\equiv \\mathbb{P}_1 \\{X \\notin A \\}\n",
    "\\end{equation*}\n",
    "That is, the power is the chance that the test rejects the null hypothesis $H_0$ when the alternative hypothesis $H_1$ is true.\n",
    "\n",
    "The _power_ of the test $A$ against the composite alternative hypothesis $H_1: X \\sim \\mathbb{P} \\in \\mathcal{P}_1$ is\n",
    "\\begin{equation*}\n",
    "   1-\\beta \\equiv \\inf_{\\mathbb{P_1} \\in \\mathcal{P}_1} \\mathbb{P}_1 \\{X \\notin A \\}.\n",
    "\\end{equation*}\n",
    "That is, it is the smallest probability that that the test rejects the null hypothesis when the alternative hypothesis is true.\n",
    "\n",
    "When the alternative $\\mathcal{P}_1$ contains only one distribution, the two definitions coincide.\n",
    "\n",
    "Power and significance level are the (extremal) probability of the same event--namely, rejecting the null hypothesis--but computed under different assumptions.\n",
    "The power is computed under the assumption that the alternative hypothesis is true; the significance level is computed under the assumption that the null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundational work by Jerzy Neyman (the founder of Berkeley's Department of Statistics) and Egon Pearson showed how to find the most powerful test of a simple null hypothesis against a simple alternative hypothesis among all tests with a given significance level. They showed that the most powerful test had an acceptance region characterized by the likelihood ratio; see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type I and Type II errors\n",
    "\n",
    "A type I error occurs when a test rejects the null hypothesis but the null hypothesis is true.\n",
    "The chance of a type I error is the significance level of the test.\n",
    "\n",
    "\n",
    "A type II error occurs when a test does not reject the null hypothesis the null hypothesis is false.\n",
    "The chance of a type II error when a particular alternative is true is 100% minus the power of the test\n",
    "against that alternative, i.e., $\\beta$.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type III errors\n",
    "\n",
    "There are a number of informal definitions of type III errors. \n",
    "\n",
    "One is that a Type III error occurs when a test correctly rejects the null hypothesis \"for the wrong reason.\" For instance, suppose that we test the null hypothesis $X \\sim N(0, 1)$\n",
    "at significance level $\\alpha = 0.05$ by defining $A \\equiv \\{x: |x| \\le 1.96\\}$. \n",
    "Suppose that in reality $X \\sim N(-1, 1)$ and we observe $X = 2$. \n",
    "Then we would _correctly_ reject $H_0$, but because $X$ was \"too big,\" while it was much more likely to be \"too small\" since in reality the distribution of $X$ has mean $-1$ instead of $0$. (If, after rejecting $H_0$, we concluded that $\\mathbb{E}X > 0$, that directional conclusion would be wrong.)\n",
    "\n",
    "Similarly, suppose we want to test whether the mean of some finite population is zero from a simple random sample $X = (X_1, \\ldots, X_n)$.\n",
    "We set $A = \\{x \\in \\Re^n : |\\bar{x}| \\le z_\\alpha/\\sqrt{n} \\}$, where $z_\\alpha$ is the $1-\\alpha$ percentage\n",
    "point of the standard normal distribution.\n",
    "This is a significance level $\\alpha$ test of the null hypothesis that $X \\sim N(0, 1)$ from an IID sample, not \n",
    "of the null hypothesis that the population mean is zero from a random sample without replacement.\n",
    "Nonetheless, this incorrect test might correctly reject the null hypothesis.\n",
    "\n",
    "Another informal definition is that a Type III error occurs when one gets the right answer to the wrong question.\n",
    "One example is testing the hypothesis $\\mathbb{E}X=0$ by testing the hypothesis $X \\sim N(0, \\sigma^2)$ when\n",
    "there is no reason to think that $X$ has a normal distribution. Even if the test is performed correctly,\n",
    "it is testing the wrong null.\n",
    "*Many* hypothesis tests in applications have Type III errors baked in. \n",
    "Indeed, most analyses of clinical trial data I've seen test a null hypothesis that involves selecting subjects at random from a superpopulation--which did not occur--rather than test a hypothesis based on the randomization of subjects into treatment or control--which actually did occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized tests\n",
    "\n",
    "Sometimes it is useful for a hypothesis test to depend not only on the data but also on \"auxilliary\" randomness, typically a $U[0, 1]$ variable $U$ that is independent of the data $X$. Then the acceptance region $A$ is a subset of the Cartesian product of the data space and $[0, 1]$. The null hypothesis is not rejected if $(X, U) \\in A$; otherwise it is rejected.\n",
    "\n",
    "Randomized tests arise in a number of situations, including tests involving discrete distributions.\n",
    "They are also useful mathematical constructs for proving theorems about tests. \n",
    "For instance, _the Neyman-Pearson lemma_ shows that the most powerful test of a simple null hypothesis $X \\sim \\mathbb{P}_0$ against a simple alternative hypothesis $X \\sim \\mathbb{P}_1$ at significance level $\\alpha$ is\n",
    "a randomized test of the form:\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \\{ (x, u): \\mathcal{L}_0(x)/\\mathcal{L}_1(x) > c \\} \\cup \\{ (x, u): (\\mathcal{L}_0(x)/\\mathcal{L}_1(x) = c) \\mbox{ and }\n",
    "  (u \\le d) \\}\n",
    "\\end{equation*}\n",
    "for suitable choices of $c$ and $d$,\n",
    "where $\\mathcal{L}_j(x)$ is the likelihood of hypothesis $j$ for data $x$. \n",
    "(This assumes $\\mathbb{P}_0$ and $\\mathbb{P}_1$ are _absolutely continuous_ with respect to each other.)\n",
    "For a definition of the likelihood function, see [Bayesian and Frequentist Estimation and Inference](./bayes.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $P$-values\n",
    "\n",
    "Here are two approaches to defining $P$-values, in terms of families of hypothesis tests and in terms of a test statistic.\n",
    "\n",
    "Family of tests:\n",
    "+ Suppose you have a set of nested (monotone) hypothesis tests:\n",
    "\n",
    "    - $\\{A_\\alpha : \\alpha \\in (0, 1] \\}$\n",
    "\n",
    "    - $\\mathbb{P}_0 \\{ X \\notin A_\\alpha \\} \\le \\alpha$ (or more generally, $\\mathbb{P} \\{ X \\notin A_\\alpha \\} \\le \\alpha, \\; \\forall \\mathbb{P} \\in \\mathcal{P}_0$)\n",
    "\n",
    "    - $A_\\alpha \\subset A_\\beta$ if $\\beta < \\alpha$ (Can always re-define $A_\\alpha \\leftarrow \\cup_{\\beta \\ge \\alpha } A_\\beta$)\n",
    "    \n",
    "+ If we observe $X = x$, the $P$-value is $\\sup \\{ \\alpha: x \\in A_\\alpha \\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test statistic definition of a $P$-value:\n",
    "\n",
    "If $P = P(X)$ is a random variable whose probability distribution is _dominated_ by the uniform distribution on $[0, 1]$ when\n",
    "the null hypothesis is true, then $P$ is a $P$-value.\n",
    "\n",
    "That is, $P$ is a $P$-value if\n",
    "\\begin{equation*}  \n",
    " \\mathbb{P}_0 \\{ P(X) \\le x \\} \\le x \\;\\; \\forall x \\in [0, 1].\n",
    "\\end{equation*}\n",
    "\n",
    "You may hear someone say that a $P$-value is the chance that a test statistic is \"as extreme or more extreme than\n",
    "observed.\"\n",
    "That is not really a precise mathematical definition; moreover, not every $P$-value can be expressed naturally\n",
    "in that form and not everything of that form is a $P$-value.\n",
    "\n",
    "For randomized tests, $P(X, U)$ is a $P$-value if\n",
    "\\begin{equation*}  \n",
    " \\mathbb{P}_0 \\{ P(X,U) \\le x \\} \\le x \\;\\; \\forall x \\in [0, 1].\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $Z$-test\n",
    "\n",
    "A $Z$-test tests the hypothesis that some function of the data has a standard Normal distribution, that is,\n",
    "the hypothesis that for some given test statistic $f(x)$, $f(X) \\sim N(0,1)$.\n",
    "\n",
    "What kinds of things have a standard normal distribution?\n",
    "\n",
    "+ A random draw from a population that has a standard normal distribution.\n",
    "+ The sample mean of $n$ IID draws from a population that has a standard normal distribution, after multiplying the sample mean by $\\sqrt{n}$.\n",
    "+ The sample mean of $n$ IID draws from a population that has a $N(\\mu, \\sigma^2)$ distribution, after subtracting $\\mu$ and multiplying by $\\sqrt{n}/\\sigma$.\n",
    "\n",
    "The $Z$-test is often used as an _approximate_ test rather than an _exact_ test. \n",
    "An _approximate_ test is one that has approximately its nominal significance level.  \n",
    "What kinds of things have distributions that are approximately standard normal?\n",
    "\n",
    "+ Things that have a Binomial$(n, p)$ distribution, after subtracting $np$ and dividing by $\\sqrt{np(1-p)}$, provided $np$ and $n(1-p)$ are not small. For instance, consider tossing a fair coin 100 times, independently. The number $X$ of heads has a Binomial$(100, 1/2)$ distribution. The distribution of $(X-50)/5$ is approximately a standard normal. ($np(1-p) = 100\\times 1/2 \\times 1/2$, so $\\sqrt{np(1-p)} = 5$.)\n",
    "+ The sample mean of $n$ IID draws with replacement from a finite population of numbers, after subtracting the population mean $\\mu$ and dividing by $\\sigma/\\sqrt{n}$, where $\\sigma$ is the population standard deviation, provided $n$ is sufficiently large and the population distribution is sufficiently \"bell shaped.\" If nothing is known about the population, it is in general impossible to know how accurate the normal approximation to the sample mean is.\n",
    "(The Binomial distribution is a special case where the population values are known to be 0 and 1.)\n",
    "\n",
    "When the test statistic is only approximately normally distributed under the null hypothesis, the accuracy\n",
    "of the approximation matters--but is rarely addressed.\n",
    "\n",
    "### Example of an approximate $Z$-test with a built-in Type III error\n",
    "\n",
    "There are two binary populations, $\\{x_j\\}_{j=1}^n$ and $\\{y_j\\}_{j=1}^m$. (A population is _binary_ if the only possible values in the population are 0 and 1.)\n",
    "We are interested in whether the populations are \"surprisingly different.\"\n",
    "The null hypothesis is that the two populations were formed by selecting $n$ items at random from the overall group of $n+m$ items to form the first population, with the remaining $m$ items comprising the second population.\n",
    "\n",
    "Let $\\bar{x} \\equiv \\frac{1}{n} \\sum_j x_j$ and $\\bar{y} \\equiv \\frac{1}{m} y_j$.\n",
    "If $x$ and $y$ were independent random samples with replacement from the same binary \"super-population\" that had a fraction $p$ of\n",
    "1s and a fraction $(1-p)$ of zeros,\n",
    "$n\\bar{x}$ would be a random variable with a binomial distribution with parameters $n$ and $p$, and $m\\bar{y}$ would \n",
    "be a random variable with\n",
    "a binomial distribution with parameters $m$ and $p$, and would be independent of $n\\bar{x}$.\n",
    "\n",
    "The expected value of $\\bar{x}$ would be $p$ and its variance would be $p(1-p)/n$;\n",
    "the expected value of $\\bar{y}$ would be $p$ and its variance would be $p(1-p)/m$.\n",
    "The expected value of $\\bar{x} - \\bar{y}$ would be 0, and its variance would be $p(1-p)(1/n+1/m)$.\n",
    "Moreover, if $x$ and $y$ were random samples with replacement from the same binary super-population,\n",
    "$\\hat{p} = (n\\bar{x} + m\\bar{y})/(n+m)$ would be an unbiased estimate of $p$,\n",
    "and for sufficiently large $m$ and $n$, the distribution of \n",
    "\\begin{equation*}\n",
    "f(x,y) \\equiv \\frac{\\bar{x} - \\bar{y}}{\\sqrt{p(1-p)(1/n+1/m)}}\n",
    "\\end{equation*}\n",
    "would be approximately $N(0,1)$. \n",
    "(The accuracy of the approximation would depend on $n$, $m$, and $p$.)\n",
    "\n",
    "If we define the acceptance region $A \\equiv \\{ x, y: |f(x, y)| \\le z_{\\alpha/2}$, where $z_\\alpha$ is the $1-\\alpha$ \n",
    "percentage point of the standard normal distribution, we get a (two-sided) $Z$ test at nominal significance\n",
    "level $\\alpha$.\n",
    "\n",
    "But this is an approximate test of a different hypothesis: an approximate answer to the wrong question,\n",
    "a Type III error.\n",
    "The hypothesis test has little to do with the original hypothesis.\n",
    "The true significance level of the test for the original null hypothesis that the two groups are a random partition\n",
    "of the $n+m$ items could be quite different from $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical example\n",
    "\n",
    "There are two binary populations, $\\{x_j\\}_{j=1}^N$ and $\\{ y_j\\}_{j=1}^M$. \n",
    "Let $p_x$ denote the mean of the first population and $p_y$ the mean of the second.\n",
    "We wish to know whether $p_x = p_y$.\n",
    "\n",
    "A random sample with replacement of size $n = 100$ will be drawn from the first list and a random sample with replacement of size $m = 300$ will be drawn from the second list, independent of the other sample.\n",
    "Let $X$ denote the sum of the numbers in the sample from the first population and let $Y$ denote the sum of the numbers in the sample from the second population.\n",
    "Then $X \\sim \\mbox{Binom}(n, p_x)$, $Y \\sim \\mbox{Binom}(m, p_y)$, and $X$ and $Y$ are independent.\n",
    "\n",
    "If $p_x = p_y = p$, $X+Y \\sim \\mbox{Binom}(n+m, p)$ and $\\hat{p} \\equiv (X+Y)/(n+m)$ is an unbiased estimate of $p$ with standard deviation $\\sqrt{p(1-p)/(m+n)} \\le 1/(2 \\sqrt{m+n}) = 0.025$.\n",
    "\n",
    "Let $\\bar{X} \\equiv X/n$ and $\\bar{Y} \\equiv Y/m$.\n",
    "If $p_x = p_y = p$ and $p$ is not close to 0 or 1, then the distribution of $\\bar{X}$ is approximately Gaussian\n",
    "with mean $p$ and standard deviation $\\sqrt{p(1-p)/n}$ and the distribution of $\\bar{Y}$ is approximately Gaussian\n",
    "with mean $p$ and standard deviation $\\sqrt{p(1-p)/m}$, and $\\bar{X}$ and $\\bar{Y}$ are independent.\n",
    "\n",
    "It follows that the distribution of $\\bar{X}-\\bar{Y}$ is approximately Gaussian with mean $p-p = 0$ and standard deviation $\\sigma \\equiv \\sqrt{p(1-p)/n + p(1-p)/m} = \\sqrt{p(1-p)}\\sqrt{1/n + 1/m} \\le 0.0577$.\n",
    "Thus the distribution of \n",
    "\\begin{equation*}\n",
    "\\frac{\\bar{X} - \\bar{Y}}{\\sqrt{p(1-p)}\\sqrt{1/n + 1/m}}\n",
    "\\end{equation*}\n",
    "is approximately the standard normal.\n",
    "If we use the \"plug-in\" estimator of $p$, $\\hat{p}$, the distribution is also approximately normal (but the approximation is worse). \n",
    "Thus the distribution of\n",
    "\\begin{equation*}\n",
    "Z \\equiv \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\hat{p}(1-\\hat{p})}\\sqrt{1/n + 1/m}}\n",
    "\\end{equation*}\n",
    "is approximately $N(0,1)$ if the hypothesis $p_x = p_y$ is true.\n",
    "\n",
    "To perform a $Z$ test, we assume the distribution of $Z$ is exactly the standard normal [TO BE CONTINUED]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection-Union Hypotheses\n",
    "\n",
    "In many situations, a null hypothesis of interest is the intersection of simpler hypotheses. For instance, the hypothesis that a university does not discriminate in its graduate admissions might be represented as \n",
    "\n",
    "(does not discriminate in arts and humanities) $\\cap$ (does not discriminate in sciences) $\\cap$ (does not discriminate in engineering) $\\cap$ (does not discriminate in professional schools).\n",
    "\n",
    "In this example, the alternative hypothesis is a _union_, viz.,\n",
    "\n",
    "(discriminates in arts and humanities) $\\cup$ (discriminates in sciences) $\\cup$ (discriminates in engineering) $\\cup$ (discriminates in professional schools).\n",
    "\n",
    "Framing a test this way leads to an _intersection-union test_.\n",
    "The null hypothesis is the intersection\n",
    "\n",
    "\\begin{equation*} \n",
    "   H_0 \\equiv \\cap_{j=1}^J H_{0j}\n",
    "\\end{equation*}\n",
    "\n",
    "and the alternative is the union\n",
    "\n",
    "\\begin{equation*} \n",
    "   H_1 \\equiv \\cup_{j=1}^J H_{0j}^c.\n",
    "\\end{equation*}\n",
    "\n",
    "There can be good reasons for representating a null hypothesis as such an intersection. \n",
    "In the example just mentioned, the applicant pool might be quite different across disciplines, making it hard to judge at the aggregate level whether there is discrimination, while testing within each discipline is more straightforward (that is, _Simpson's Paradox_ can be an issue).\n",
    "\n",
    "Hypotheses about multivariate distributions can sometimes be expressed as the intersection of hypotheses about each dimension separately. For instance, the hypothesis that an $J$-dimensional distribution has zero mean could be represented as \n",
    "\n",
    "(1st component has zero mean) $\\cap$ (2nd component has zero mean) $\\cap$ $\\cdots$ $\\cap$ ($J$th component has zero mean)\n",
    "\n",
    "The alternative is again a union:\n",
    "\n",
    "(1st component has nonzero mean) $\\cup$ (2nd component has nonzero mean) $\\cup$ $\\cdots$ $\\cup$ ($J$th component has nonzero mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinations of experiments and stratified experiments\n",
    "\n",
    "The same kind of issue arises when combining information from different experiments.\n",
    "For instance, imagine testing whether a drug is effective. We might have several randomized, controlled trials in different places, or a large experiment involving a number of centers, each of which performs its own randomization (i.e., the randomization is stratified).\n",
    "\n",
    "How can we combine the information from the separate (independent) experiments to test the null hypothesis that the drug is ineffective? \n",
    "\n",
    "Again, the overall null hypothesis is \"the drug doesn't help,\" which can be written as an intersection of hypotheses\n",
    "\n",
    "(drug doesn't help in experiment 1) $\\cap$ (drug doesn't help in experiment 2) $\\cap$ $\\cdots$  $\\cap$ (drug doesn't help in experiment $J$),\n",
    "\n",
    "and the alternative can be written as\n",
    "\n",
    "(drug helps in experiment 1) $\\cup$ (drug helps in experiment 2) $\\cup$ $\\cdots$  $\\cup$ (drug helps in experiment $J$),\n",
    "\n",
    "a union."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining evidence\n",
    "\n",
    "Suppose we have a test of each \"partial\" null hypothesis $H_{0j}$. Clearly, if the $P$-value for one of those tests is sufficiently small, that's evidence that the overall null $H_0$ is false.\n",
    "\n",
    "But suppose none of the individual $P$-values is small, but many are \"not large.\" \n",
    "Is there a way to combine them to get sronger evidence about $H_0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining functions\n",
    "\n",
    "Let $\\lambda$ be a $J$-vector of statistics such that the distribution of $\\lambda_j$\n",
    "if hypothesis $H_{0j}$ is true is known. \n",
    "We assume that smaller values of $\\lambda_j$ are stronger evidence against $H_{0j}$.\n",
    "For instance, $\\lambda_j$ might be the $P$-value of $H_{0j}$ for some test.\n",
    "\n",
    "Consider a function\n",
    "\n",
    "\\begin{equation*}  \\phi: [0, 1]^J \\rightarrow \\Re; \\lambda = (\\lambda_1, \\ldots, \\lambda_J) \\mapsto \\phi(\\lambda)\n",
    "\\end{equation*} \n",
    "with the properties:\n",
    "\n",
    "+ $\\phi$ is non-increasing in every argument, i.e., $\\phi( \\ldots, \\lambda_j, \\ldots) \\ge \\phi(( \\ldots, \\lambda_j', \\ldots)$ if $\\lambda_j \\le \\lambda_j'$, $j = 1, \\ldots, J$.\n",
    "\n",
    "+ $\\phi$ attains its maximum if any of its arguments equals 0.\n",
    "\n",
    "+ $\\phi$ attains its minimum if all of its arguments equal 1.\n",
    "\n",
    "+ for all $\\alpha > 0$, there exist finite functions $\\phi_-(\\alpha)$, $\\phi_+(\\alpha)$ such that if every partial null hypothesis $\\{H_{0j}\\}$ is true, \n",
    "\\begin{equation*} \\Pr \\{\\phi_-(\\alpha) \\le \\phi(\\lambda) \\le \\phi_+(\\alpha) \\} \\ge 1-\\alpha\\end{equation*}\n",
    "and $[\\phi_-(\\alpha), \\phi_+(\\alpha)] \\subset [\\phi_-(\\alpha'), \\phi_+(\\alpha')]$ if $\\alpha \\ge \\alpha'$.\n",
    "\n",
    "Then we can use $\\phi(\\lambda)$ as the basis of a test of $H_0 = \\cap_{j=1}^J H_{0j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher's combining function\n",
    "\n",
    "\\begin{equation*}  \\phi_F(\\lambda) \\equiv -2 \\sum_{j=1}^J \\ln(\\lambda_j).\\end{equation*}\n",
    "\n",
    "### Liptak's combining function\n",
    "\n",
    "\\begin{equation*}  \\phi_L(\\lambda) \\equiv \\sum_{j=1}^J \\Phi^{-1}(1-\\lambda_j),\\end{equation*}\n",
    "\n",
    "where $\\Phi^{-1}$ is the inverse standard normal CDF.\n",
    "\n",
    "### Tippet's combining function\n",
    "\n",
    "\\begin{equation*}  \\phi_T(\\lambda) \\equiv \\max_{j=1}^J (1-\\lambda_j).\\end{equation*}\n",
    "\n",
    "### Direct combination of test statistics\n",
    "\n",
    "\\begin{equation*}  \\phi_D \\equiv \\sum_{j=1}^J f_j(\\lambda_j), \\end{equation*}\n",
    "\n",
    "where $\\{ f_j \\}$ are suitable decreasing functions. For instance, if $\\lambda_j$ is the $P$-value for $H_{0j}$ corresponding to some test statistic $T_j$ for which larger values are stronger evidence against $H_{0j}$, we could use $\\phi_D = \\sum_j T_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's combining function for independent $P$-values\n",
    "\n",
    "Suppose $H_0$ is true, that $\\lambda_j$ is the $P$-value of $H_{0j}$ for some pre-specified test, that the distribution of $\\lambda_j$ is continuous under $H_{0j}$, and that $\\{ \\lambda_j \\}$ are independent if $H_0$ is true.\n",
    "\n",
    "Then, if $H_0$ is true, $\\{ \\lambda_j \\}$ are IID $U[0,1]$.\n",
    "\n",
    "Under $H_{0j}$, the distribution of $-\\ln \\lambda_j$ is exponential(1):\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\Pr \\{ -\\ln \\lambda_j \\le x \\} = \\Pr \\{ \\ln \\lambda_j \\ge -x \\} = \\Pr \\{ \\lambda_j \\ge e^{-x} \\} = 1 - e^{-x}.\n",
    "\\end{equation*}\n",
    "\n",
    "The distribution of 2 times an exponential is $\\chi_2^2$:\n",
    "the pdf of a chi-square with $k$ degrees of freedom is\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\frac{1}{2^{k/2}\\Gamma(k/2)} x^{k/2-1} e^{-x/2}.\n",
    "\\end{equation*}\n",
    "\n",
    "For $k=2$, this simplifies to $e^{-x/2}/2$, the exponential density scaled by a factor of 2.\n",
    "\n",
    "Thus, under $H_0$, $\\phi_F(\\lambda)$ is the sum of $J$ independent $\\chi_2^2$ random variables. The distribution of a sum of independent chi-square random variables is a chi-square random variable with degrees of freedom equal to the sum of the degrees of freedom of the variables that were added.\n",
    "\n",
    "Hence, under $H_0$,\n",
    "\n",
    "\\begin{equation*} \n",
    "  \\phi_F(\\lambda) \\sim \\chi_{2J}^2,\n",
    "\\end{equation*}\n",
    "\n",
    "the chi-square distribution with $2n$ degrees of freedom.\n",
    "\n",
    "Let $\\chi_{k}^2(\\alpha)$ denote the $1-\\alpha$ quantile of the chi-square distribution\n",
    "with $k$ degrees of freedom.\n",
    "If we reject $H_0$ when\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\phi_F(\\lambda) \\ge \\chi_{2J}^2(\\alpha),\n",
    "\\end{equation*}\n",
    "\n",
    "that yields a significance level $\\alpha$ test of $H_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate distribution of Fisher's combining function when all nulls are true\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.polynomial import polynomial as P\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.stats import chi2, binom\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "def plot_fisher_null(n=5, reps=10000):\n",
    "    U = sp.stats.uniform.rvs(size=[reps,n])\n",
    "    vals = np.apply_along_axis(lambda x: -2*np.sum(np.log(x)), 1, U)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.hist(vals, bins=max(int(reps/40), 5), density=True, label=\"simulation\")\n",
    "    mxv = max(vals)\n",
    "    grid = np.linspace(0, mxv, 200)\n",
    "    ax.plot(grid, chi2.pdf(grid, df=2*n), 'r-', lw=3, label='chi-square pdf, df='+str(2*n))\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969bc7797ad0412db114ef610b627e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='n', max=200, min=1), IntSlider(value=10000, description=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_fisher_null(n=5, reps=10000)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_fisher_null, n=widgets.IntSlider(min=1, max=200, step=1, value=5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When $P$-values have atoms\n",
    "\n",
    "A real random variable $X$ is first-order stochastically larger than a real random variable $Y$ if for all $x \\in \\Re$,\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\Pr \\{ X \\ge x \\} \\ge \\Pr \\{ Y \\ge x \\},\n",
    "\\end{equation*}\n",
    "with strict inequality for some $x \\in \\Re$.\n",
    "\n",
    "Suppose $\\{\\lambda_j \\}$ for $\\{ H_{0j}\\}$ satisfy\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\Pr \\{ \\lambda_j \\le p  || H_{0j} \\} \\le p.\n",
    "\\end{equation*}\n",
    "\n",
    "This takes into account the possibility that $\\lambda_j$ does not have a continuous \n",
    "distribution under $H_{0j}$, ensuring that $\\lambda_j$ is still a _conservative_ $P$-value.\n",
    "\n",
    "Since $\\ln$ is monotone, it follows that for all $x \\in \\Re$\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\Pr \\{ -2 \\ln \\lambda_j \\ge x \\} \\le \\Pr \\{ -2 \\ln U \\ge x \\}.\n",
    "\\end{equation*}\n",
    "\n",
    "That is, if $\\lambda_j$ does not have a continuous distribution, \n",
    "the a $\\chi_2^2$ variable is stochastically larger than the distribution of $-2\\ln \\lambda_j$.\n",
    "\n",
    "It turns out that $X$ is stochastically larger than $Y$ if and only if\n",
    "there is some probability space on which there exist \n",
    "two random variables, $\\tilde{X}$ and $\\tilde{Y}$ such that $\\tilde{X} \\sim X$,\n",
    "$\\tilde{Y} \\sim Y$, and $\\Pr \\{\\tilde{X} \\ge \\tilde{Y} \\} = 1$. \n",
    "(See, e.g., Grimmett and Stirzaker,_Probability and Random Processes_, 3rd edition,\n",
    "Theorem 4.12.3.)\n",
    "\n",
    "Let $\\{X_j\\}_{j=1}^n$ be IID $\\chi_2^2$ random variables,\n",
    "and let $Y_j \\equiv - 2 \\ln \\lambda_j$, $j=1, \\ldots, J$.\n",
    "\n",
    "Then there is some probability space \n",
    "for which we can define $\\{\\tilde{Y_j}\\}$ and $\\{\\tilde{X_j}\\}$ such that\n",
    "\n",
    "+ $(\\tilde{Y_j})$ has the same joint distribution as $(Y_j)$\n",
    "\n",
    "+ $(\\tilde{X_j})$ has the same joint distribution as $(X_j)$\n",
    "\n",
    "+ $\\tilde{X_j} \\ge \\tilde{Y_j}$ for all $j$ with probability one.\n",
    "\n",
    "Then\n",
    "\n",
    "+ $\\sum_j \\tilde{Y_j}$ has the same distribution as $\\sum_j Y_j = -2 \\sum_j \\ln \\lambda_j$,\n",
    "\n",
    "+ $\\sum_j \\tilde{X_j}$ has the same distribution as $\\sum_j X_j$ (namely, chi-square with $2J$ degrees of freedom),\n",
    "\n",
    "+ $\\sum_j \\tilde X_j  \\ge \\sum_j \\tilde{Y_j}$.\n",
    "\n",
    "That is, \n",
    "\n",
    "\\begin{equation*} \n",
    "  \\Pr \\left \\{-2 \\sum_j \\ln \\lambda_j \\ge \\chi_{2J}^2(\\alpha) \\right \\} \\le \\alpha.\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, we still get a conservative hypothesis test if one or more of the $p$-values for the\n",
    "partial tests have atoms under their respective null hypotheses $\\{H_{0j}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $P$-values by simulation\n",
    "\n",
    "Suppose that if the null hypothesis is true, the probability distribution of the\n",
    "data is invariant under some group $\\mathcal{G}$, for instance, the reflection group or the symmetric (i.e., permutation) group.\n",
    "\n",
    "For any pre-specified test statistic $T$, we can estimate a $P$-value by generating uniformly distributed random elements of the orbit of the data under the action of the group (see [Mathematical Fundations](./math-foundations.ipynb) if these notions are unfamiliar).\n",
    "\n",
    "Suppose we generate $n$ random elements of the orbit.\n",
    "Let $x_0$ denote the original data; let $\\{\\pi_k\\}_{k=1}^K$ denote IID random elements of \n",
    "$\\mathcal{G}$ and $x_k = \\pi_k(x_0)$, $k=1, \\ldots, K$ denote $K$ random elements of the\n",
    "orbit of $x_0$ under $\\mathcal{G}$.\n",
    "\n",
    "An unbiased estimate of the $P$-value (assuming that the random elements are generated uniformly at random--see [Algorithms for Pseudo-Random Sampling](./permute-sample.ipynb) for a caveats), is\n",
    "\n",
    "\\begin{equation*} \n",
    "  \\hat{P} = \\frac{\\#\\{ k>0: T(\\pi_k(x_0)) \\ge T(x_0)\\}}{K}.\n",
    "\\end{equation*}\n",
    "\n",
    "Once $x_0$ is known, the events $\\{T(\\pi_j(x_0)) \\ge T(x_0)\\}$ are IID with probability\n",
    "$P$ of occurring, and $\\hat{P}$ is an unbiased estimate of $P$.\n",
    "\n",
    "Another estimate of $P$, arguably preferable (as discussed below), is\n",
    "\n",
    "\\begin{equation*} \n",
    "  \\hat{P}' = \\frac{\\#\\{ k \\ge 0: T(\\pi_k(x_0)) \\ge T(x_0)\\}}{K+1},\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\pi_0$ is the identity permutation.\n",
    "\n",
    "The reasoning behind this choice is that, if the null hypothesis is true, the original data are one of the equally likely elements of the orbit of the data--exactly as likely as the elements generated from it. \n",
    "Thus there are really $K+1$ values that are equally likely if the null is true,\n",
    "rather than $K$: nature provided one more random permutation, the original data. \n",
    "The estimate $\\hat{P}'$ is never smaller than $1/(K+1)$.\n",
    "Some practitioners like this because it never estimates the $P$-value to be zero.\n",
    "There are other reasons for preferring it, discussed below.\n",
    "\n",
    "The estimate $\\hat{P}'$ of $P$ is generally biased, however, since $\\hat{P}$ is unbiased and \n",
    "\n",
    "\\begin{equation*} \n",
    "   \\hat{P}' = \\frac{K\\hat{P} + 1}{K+1} = \\frac{K}{K+1} \\hat{P} + \\frac{1}{K+1},\n",
    "\\end{equation*}\n",
    "\n",
    "so\n",
    "\n",
    "\\begin{equation*} \n",
    "   \\mathbb{E} \\hat{P}' = \\frac{K}{K+1} P + \\frac{1}{K+1} =\n",
    "   P  + (1-P) \\frac{1}{K+1} > P.\n",
    "\\end{equation*}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for simulation error in stratum-wise $P$-values\n",
    "\n",
    "Suppose that the $P$-value $\\lambda_j$ for $H_{0j}$ is estimated by $b_j$ simulations instead of being known exactly.\n",
    "How can we take the uncertainty of the simulation estimate into account?\n",
    "\n",
    "Here, we will pretend that the simulation itself is perfect: that the PRNG generates true IID $U[0,1]$ variables, that pseudo-random integers on $\\{0, 1, \\ldots, N\\}$ really are equally likely, and that pseudo-random samples or permutations really are equally likely, etc. \n",
    "\n",
    "The error we are accounting for is not the imperfection of the PRNG or other algorithms, just the uncertainty due to approximating a theoretical probability $\\lambda_j$ by an estimate via (perfect) simulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A crude approach: simultaneous one-sided upper confidence bounds for every $\\lambda_j$\n",
    "\n",
    "Suppose we find, for each $j$, an upper confidence bound for $\\lambda_j$ (the \"true\" $P$-value in stratum $j$),\n",
    "for instance, by inverting binomial tests based on $\\# \\{k > 0: T(\\pi_k(x_0)) \\ge T(x_0) \\}$.\n",
    "\n",
    "Since $\\phi$ is monotonic in every coordinate, the upper confidence confidence bounds \n",
    "for $\\{ \\lambda_j \\}$ imply a lower confidence bound for $\\phi(\\lambda)$, which translates to an upper confidence bound for the combined $P$-value.\n",
    "\n",
    "What is the confidence level of the bound on the combined $P$-value? \n",
    "If the $P$-value estimates are independent, the joint coverage probability of a set of $n$ independent confidence bounds with confidence level $\\alpha$ is $1-(1-\\alpha)^n$, as we shall show.\n",
    "\n",
    "Let $A_j$ denote the event that the upper confidence bound for $\\lambda_j$ is greater than or equal to $\\lambda_j$, and suppose $\\Pr \\{A_j\\} = 1-\\alpha_j$.\n",
    "\n",
    "Regardless of the dependence among the events $\\{A_j \\}$, the chance that all of the confidence bounds cover their corresponding parameters can be bounded using Bonferroni's inequality:\n",
    "\n",
    "\\begin{equation*} \n",
    "\\Pr \\{ \\cap_j A_j \\} = 1 - \\Pr \\{ \\cup_j A_j^c \\} \\ge 1 - \\sum_j \\Pr \\{A_j^c \\} \n",
    "   = 1 - \\sum_j (1- \\Pr \\{A_j \\}) = 1 - \\sum_j \\alpha_j.\n",
    "\\end{equation*}\n",
    "\n",
    "If $\\{A_j\\}$ are independent, \n",
    "\n",
    "\\begin{equation*} \n",
    "\\Pr \\{ \\cap_j A_j \\} = \\prod_j \\Pr \\{ A_j \\} = \\prod_j (1-\\alpha_j).\n",
    "\\end{equation*}\n",
    "\n",
    "Both of those expressions tend to get small quickly as $n$ gets large;\n",
    "bounding $\\phi(\\lambda)$ by bounding the components of $\\lambda$ is inefficient.\n",
    "\n",
    "Let's look for a different approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sharper approach: use a related randomized test\n",
    "\n",
    "This section presents a different approach, based on $\\hat{P}'$ (the biased estimate of $P$) rather than\n",
    "$\\hat{P}$.\n",
    "It yields a surprisingly simple and elegant conservative test.\n",
    "\n",
    "The key is to change the test itself: instead of treating $\\hat{\\lambda}_j$ or $\\hat{\\lambda}_j'$ as an estimate of \n",
    "$\\lambda_j$--the $P$-value for $H_{0j}$ for the original test--we define a _new_ test based on \n",
    "\n",
    "\\begin{equation*} \n",
    "  \\hat{\\lambda}_j' \\equiv \\frac{\\#\\{k \\ge 0: T(\\pi_k(x_0) \\ge T(x_0)\\}}{K_j+1},\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\pi_0$ is the identity permutation and $\\{ \\pi_k \\}_{k=1}^{K_j}$ are elements of $\\mathcal{G}$ \n",
    "selected at random uniformly.\n",
    "\n",
    "While $\\hat{\\lambda}_j'$ is a biased estimate of $\\lambda_j$, we shall see that **it is itself a valid conditional $P$-value**; that is,  $\\Pr \\{ \\hat{\\lambda}_j' \\le p \\} \\le p$, given that the data are in the orbit of $x_0$.\n",
    "\n",
    "Note that this (conditional) probability involved has two sources of randomness:\n",
    "\n",
    "1. The randomness in the original data, $x_0$ (although we condition on the event that the data fall in the orbit of $x_0$)\n",
    "1. The randomness in generating the random transformations $\\{ \\pi_k \\}_{k=1}^{K_j} \\subset \\mathcal{G}$\n",
    "\n",
    "The resulting hypothesis test is a _randomized test_: it uses auxilliary randomness\n",
    "in addition to the randomness in the data.\n",
    "If the experiment were repeated and the data turned out to be $x_0$ again, \n",
    "the test will in general give a different $P$ value: $\\hat{\\lambda}_j'$ is random even if $x_0$ is known.\n",
    "The decision to reject the null hypothesis (or not) is random even after the data have been observed.\n",
    "\n",
    "Randomized tests have a number of desirable theoretical properties (related to continuity and convexity), \n",
    "but they are rarely used explicitly in practice.\n",
    "Tests involving simulated $P$-values are an example where randomized tests are used implicitly rather than explicitly--generally without recognizing that the resulting test is randomized.\n",
    "\n",
    "This section shows that the randomization involved in simulating $P$-values can be taken into account explicitly to get a conservative test.\n",
    "\n",
    "By construction, $\\{\\pi_k(x_0)\\}_{k=1}^{K_j}$ are IID uniformly distributed on the orbit of $x_0$.\n",
    "(We are ignoring imperfections in the PRNG and other algorithms.)\n",
    "Thus, $\\{T(\\pi_k(x_0))\\}_{k=0}^{K_j}$ are IID random variables.\n",
    "The event $\\hat{\\lambda}_j' \\le p$ is the event that $T(x_0)= T(\\pi_0(x_0))$ is \n",
    "larger than all but (at most) $(K_j+1)p$ of the values $\\{T(\\pi_k(x_0))\\}_{k=0}^{K_j}$.\n",
    "Under the null, $T(x_0)$ is equally likely to be any of them.\n",
    "\n",
    "Let $p' = \\lfloor (K_j+1)p \\rfloor /(K_j+1)$. Then $p' \\le p$ and $(K_j+1)p'$ is an integer.\n",
    "Sort the values $\\{T(\\pi_k(x_0))\\}_{k=0}^{K_j}$ from largest to smallest, breaking ties arbitrarily.\n",
    "Consider the $(K_j+1)p'$th element of the list.\n",
    "If it is strictly greater than the $(K_j+1)p'+1$st element of the list, then there are $(K_j+1)p'$ permutations\n",
    "$\\pi_k$ for which $T(\\pi_k(x_0))$ is strictly greater than all but $(K_j+1)p$ of the values \n",
    "$\\{T(\\pi_k(x_0))\\}_{k=0}^{K_j}$.\n",
    "If the $(K_j+1)p'$th element of the list is equal to the $(K_j+1)p'+1$ element, then there are\n",
    "_fewer_ than $(K_j+1)p'$ such permutations.\n",
    "Either way, the chance that a randomly selected element of the multiset $\\{T(\\pi_k(x_0))\\}_{k=0}^{K_j}$\n",
    "is strictly greater than all but at most $(K_j+1)p'$ of the elements is\n",
    "\\begin{equation*} \n",
    "\\Pr \\{ \\hat{\\lambda}_j' \\le p \\} = \\Pr \\{ \\hat{\\lambda}_j' \\le p' \\} \\le \\frac{(K_j+1)p'}{K_j+1} = p' \\le p.\n",
    "\\end{equation*}\n",
    "\n",
    "Thus $\\Pr \\{\\hat{\\lambda}_j' \\le p \\} \\le p$.\n",
    "That is, **$\\hat{\\lambda}_j$ is _itself_ a conservative $P$-value** for a randomized test, separate from the fact that it is a (biased) estimate of $\\lambda_j$, the $P$-value for a related non-randomized test.\n",
    "\n",
    "The test is defined implicitly: reject $H_{0j}$ at significance level $\\alpha$ if\n",
    "$\\hat{\\lambda}_j' \\le \\alpha$.\n",
    "\n",
    "It follows that applying Fisher's combining function to $\\hat{\\lambda}' = (\\hat{\\lambda}_j')_{j=1}^J$ gives a conservative test of the intersection null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent tests\n",
    "\n",
    "If $\\{ \\lambda_j \\}_{j=1}^J$ are dependent, the distribution of $\\phi_F(\\lambda)$ is no longer chi-square when the null hypotheses are true.\n",
    "Nonetheless, one can calibrate a test based on Fisher's combining function (or any other combining function) by simulation.\n",
    "This is commonly used in multivariate permutation tests involving dependent partial tests\n",
    "using \"lockstep\" permutations.\n",
    "\n",
    "See, e.g., Pesarin, F. and L. Salmaso, 2010. _Permutation Tests for Complex Data: Theory, Applications and Software_, Wiley, 978-0-470-51641-6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall illustrate how the approach can be used to construct nonparametric multivariate tests from univariate tests to address for the two-sample problem (i.e., is there evidence that two samples come from different populations, or is it plausible\n",
    "that they are a single population randomly divided into two groups?).\n",
    "This is equivalent to testing whether treatment has an effect in a controlled, randomized study\n",
    "in which the subjects who receive treatment are a simple random sample of the study group,\n",
    "using the Neyman model for causal inference.\n",
    "(The null hypothesis is that treatment makes no difference whatsoever: each subject's response would\n",
    "be the same whether that subject was assigned to treatment or to control, and without regard for the assignment of\n",
    "other subjects to treatment or control.)\n",
    "\n",
    "We have $N$ subjects of whom $N_t$ are treated and $N_c$ are controls. \n",
    "Each subject has a vector of $J$ measurements. \n",
    "For each of these $J$ \"dimensions\" we have a test statistic $T_j$ (for instance, the difference between the mean of the treated and the mean of the controls on that dimension--but we could use something else, and we don't have to use the same test statistic for different dimensions). \n",
    "\n",
    "Each $T_j$ takes the responses of the treated and the controls on dimension $j$ and yields a number. \n",
    "We will assume that larger values of $T_j$ are stronger evidence against the null hypothesis that for dimension $j$ treatment doesn't make any difference. \n",
    "\n",
    "Let $T$ denote the whole $J$-vector of test statistics. Let $t(0)$ denote the observed value of the \n",
    "$J$-vector of test statistics for the original data.\n",
    "\n",
    "Now consider randomly re-labelling $N_t$ of the $N$ subjects as treated and the remaining $N_c$ as controls\n",
    "by simple random sampling, so that all subsets of size $N_t$ are equally likely to be labeled \"treated.\"\n",
    "Each re-labelling carries the subject's entire $J$-vector of responses with it: \n",
    "the dimensions are randomized \"in lockstep.\"\n",
    "\n",
    "Let $t(k)$ denote the observed $J$-vector of test statistics for the $k$th random allocation (i.e., the $k$th random permutation). \n",
    "We permute the data $K$ times in all, each yielding a $J$-vector $t(k)$ of observed values of the test statistics. This gives $K+1$ permutations in all, including the original data (the zeroth permutation), for which the vector is $t(0)$. Let $t_j(k)$ denote the test statistic for dimension $j$ for the $k$th permutation.\n",
    "\n",
    "We now transform the $J$ by $(K+1)$ matrix $[t_j(k)]_{j=1}^J{}_{k=0}^K$ to a corresponding matrix of $P$-values for the univariate permutation tests.\n",
    "\n",
    "For $j=1, \\ldots , J$ and $k = 0, \\ldots , K$, define\n",
    "\n",
    "\\begin{equation*} \n",
    "   P_j(k) \\equiv \\frac{\\#\\{ \\ell \\in \\{0, \\ldots, K \\} : t_j(\\ell) \\ge t_j(k) \\}}{K+1}.  \n",
    "\\end{equation*}\n",
    "This is the simulated upper tail probability of the $k$th observed value of the $j$th test statistic \n",
    "under the null hypothesis. \n",
    "\n",
    "Think of the values of $P_j(k)$ as a matrix. \n",
    "Each column corresponds to a random permutation of the original data (the 0th column corresponds to the original data); each row corresponds to a dimension of measurement; each entry is a number between $1/(K+1)$ and 1.\n",
    "\n",
    "Now apply Fisher's combining function $\\phi_F$ (or Tippett's, or Stouffer's, or anything else) to each column of \n",
    "$J$ numbers. That gives $K+1$ numbers, \n",
    "$f(k), k=0, \\ldots , K$, one for each permutation of the data. \n",
    "The overall \"Non-Parametric Combination of tests\" (NPC) $P$-value is\n",
    "\\begin{equation*} \n",
    " P_{\\mbox{NPC}} \\equiv \\frac{\\#\\{ k \\in \\{0, \\ldots, K\\} : f(k) \\ge f(0) \\}}{K+1}. \n",
    "\\end{equation*}\n",
    "This is the simulated lower tail probability of the observed value of the combining function under the randomization.\n",
    "\n",
    "Ultimately, this whole thing is just a univariate permutation test that uses a complicated test statistic \n",
    "$\\phi_F$ that assigns one number to each permutation of the multivariate data.\n",
    "\n",
    "Also see [the permute Python package](http://statlab.github.io/permute/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Permutation Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two examples: \n",
    "\n",
    "+ Boring, A., K. Ottoboni, and P.B. Stark, 2016. Student Evaluations of Teaching (Mostly) Do Not Measure Teaching Effectiveness, _ScienceOpen_, doi 10.14293/S2199-1006.1.SOR-EDU.AETBZC.v1\n",
    "\n",
    "+ Hessler, M.,  D.M. PÃ¶pping, H. Hollstein, H. Ohlenburg, P.H. Arnemann, C. Massoth, L.M. Seidel, A. Zarbock & M. Wenk, 2018. Availability of cookies during an academic course session affects evaluation of teaching, _Medical Education, 52_, 1064â€“1072. doi 10.1111/medu.13627\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
