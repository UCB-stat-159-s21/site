{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Sets\n",
    "\n",
    "## This is a rough work in progress!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Parameters\n",
    "\n",
    "Many different kinds of things are called \"parameters.\" Here are several categories.\n",
    "\n",
    "### Population parameters\n",
    "\n",
    "Any property of a population may be called a _parameter_.\n",
    "Examples include the population mean, percentiles, number of modes, etc.\n",
    "\n",
    "If the population has more than one \"value\" per item, a parameter could involve more than one of them. E.g., if the population is a group of people each of whom has a height and a weight, then the population correlation between height and weight is a parameter.\n",
    "\n",
    "Similarly, consider a group of individuals and the values of some quantity (the \"response\") for each of those individuals without and with some intervention (notionally, a \"treatment\").\n",
    "The difference between the average response without the intervention and the average response with the intervention is a parameter (the _average treatment effect_).\n",
    "\n",
    "If we are sampling at random from a population, the probability distribution of the sample\n",
    "depends on the values in the population, and thus, in general, on the \n",
    "values of population parameters.\n",
    "(It will also depend on the sampling design.)\n",
    "\n",
    "### Functional parameters of probability distributions\n",
    "\n",
    "Suppose $X \\sim \\mathbb{P}$, where $\\mathbb{P}$ is a probability distribution on some space $\\mathcal{X}$ of possible outcomes.\n",
    "We assume that $\\mathbb{P} \\in \\mathcal{P}$, some known set of possible distributions.\n",
    "\n",
    "A _functional parameter_ $\\theta(\\mathbb{P})$ is a function of $\\mathbb{P}$.\n",
    "For instance the (population) mean is a functional parameter:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta(\\mathbb{P}) = \\mathbb{E}X \\equiv \\int_\\mathcal{X} x d\\mathbb{P}(x).\n",
    "\\end{equation}\n",
    "\n",
    "So are other moments of the probability distribution:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta(\\mathbb{P}) = \\mathbb{E}X^n \\equiv \\int_\\mathcal{X} x^n d\\mathbb{P}(x), \\;\\; n=1, 2, \\ldots .\n",
    "\\end{equation*}\n",
    "\n",
    "Other properties of $\\mathbb{P}$, such as percentiles of a univariate\n",
    "distribution, are also functional parameters.\n",
    "For instance, if $X$ is a real-valued random variable,\n",
    "then the $\\alpha$ percentile of $\\mathbb{P}$,\n",
    "\\begin{equation}\n",
    "\\theta(\\mathbb{P}) = \\inf \\left \\{x: \\int_{-\\infty}^x d\\mathbb{P}(x) \\ge \\alpha \\right \\},\n",
    "\\end{equation}\n",
    "is a functional parameter.\n",
    "\n",
    "For multivariate distributions, correlations among the components of $X$\n",
    "are functional parameters.\n",
    "\n",
    "In general, there can be distinct distributions $\\mathbb{P}$ and $\\mathbb{Q}$ such that \n",
    "$\\mathbb{P} \\ne \\mathbb{Q}$ but $\\theta(\\mathbb{P}) = \\theta(\\mathbb{Q})$. \n",
    "For instance there are infinitely many normal distributions with the\n",
    "same mean (but different variances).\n",
    "\n",
    "### Parameters as indices for sets of distributions\n",
    "\n",
    "Another use of the term \"parameter\" is as an abstract\n",
    "index that points to a particular distribution in\n",
    "a family of distributions.\n",
    "For instance, we might have a multiset of distributions\n",
    "$\\mathcal{P} = \\{\\mathbb{P}_\\eta\\}_{\\eta \\in \\Theta}$.\n",
    "In that case, $\\eta$ is an index parameter.\n",
    "For index parameters, if for all parameters $\\eta$, $\\nu \\in \\Theta$ such that\n",
    "$\\eta \\ne \\nu$, $\\mathbb{P}_\\eta \\ne \\mathbb{P}_\\nu$, the parameter is said to be _identifiable_.\n",
    "That is, $X$ contains enough information to\n",
    "identify the value of the parameter with arbitrarily high accuracy, given enough\n",
    "observations.\n",
    "Otherwise, the parameter is _non-identifiable_ or _unidentifiable_: the data\n",
    "do not contain enough information to distinguish among different values\n",
    "of the parameter, no matter how many observations are made.\n",
    "\n",
    "\n",
    "#### Special case: location-scale families\n",
    "\n",
    "Many indexed families of distributions are related through the\n",
    "value of their parameter in a particular way. \n",
    "For instance, suppose that the outcome space $\\mathcal{X}$ is a real vector space,\n",
    "so it makes sense to add elements of $\\mathcal{X}$ and to multiply them by scalars.\n",
    "\n",
    "If $X \\sim \\mathbb{P}$, then for any $x \\in \\mathcal{X}$ and $a \\in \\Re \\backslash \\{0\\}$,\n",
    "we could define $\\mathbb{P}_{x,a}$ to be the distribution of $aX+x$.\n",
    "Then $\\{P_{x, a} \\}_{x \\in \\mathcal{X}, a \\in \\Re \\backslash \\{0\\}}$ is a \n",
    "_location-scale family_\n",
    "with parameter $\\theta = (x, a)$\n",
    "As $x$ varies, the probability distribution \"shifts\" its location.\n",
    "As $a$ varies, the probability distribution $P$ is \"stretched\" or re-scaled.\n",
    "The family of univariate normal distributions is a location-scale family over the two-dimensional parameter $\\theta = (\\mu, \\sigma)$ with $\\mu \\in \\Re$, $\\sigma \\in \\Re \\backslash \\{0\\}$.\n",
    "\n",
    "#### Notation for index parameters\n",
    "To keep the notation for index parameters\n",
    "parallel with the notation for functional parameters, we will define $\\theta(\\mathbb{P}) \\equiv \n",
    "\\{ \\eta: \\mathbb{P} = \\mathbb{P}_\\eta\\}$.\n",
    "If $\\theta$ is identifiable, $\\theta(\\mathbb{P})$ is a singleton set; otherwise,\n",
    "it may contain more than one element. \n",
    "\n",
    "### Parametric families of distributions\n",
    "\n",
    "A _parametric family of distributions_ is an indexed collection of probability distributions\n",
    "that depends on the index parameter (which might be multidimensional) in a \n",
    "fixed functional way.\n",
    "(We can think of things like the mean and standard deviation of a normal distribution as either a multidimensional parameter or as a collection of parameters.)\n",
    "\n",
    "Most distributions that have names are parametric families, e.g., Bernoulli (the parameter $p$), Binomial (the two-dimensional parameter $(n, p)$), Geometric ($p$), Hypergeometric (the three-dimensional parameter $(N, G, n)$), Negative Binomial $(p, k)$, Normal $(\\mu, \\sigma)$, Student's $T$ $(\\mu, \\sigma, \\nu)$, continuous uniform (the endpoints of the interval of support, the two-dimensional parameter $(a, b)$), and so on. \n",
    "\n",
    "### Nuisance parameters\n",
    "\n",
    "When the probability distribution of the data depends on a multi-dimensional parameter\n",
    "but only some components of that parameter are of interest, the other components are called _nuisance parameters_. \n",
    "For instance, in estimating the mean of a normal distribution, the\n",
    "variance of the distribution is a nuisance parameter: we don't care what it is,\n",
    "but it affects the probability distribution of the data.\n",
    "\n",
    "Similarly, in estimating a population mean from a stratified sample, the means\n",
    "within the different strata are nuisance parameters.\n",
    "\n",
    "\n",
    "### Abstract parameters\n",
    "\n",
    "For most of the theory in this chapter, $\\theta$ will be an abstract parameter:\n",
    "the development applies to functional parameters, index parameters, parameters of\n",
    "parametric families, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence sets\n",
    "\n",
    "What can we learn about the value of $\\theta(\\mathbb{P})$ from observations?\n",
    "[The chapter on testing](./tests.ipynb) discusses testing hypotheses, including\n",
    "hypotheses about parameters.\n",
    "Here we explore a different approach to quantifying what a sample tells us about\n",
    "$\\theta$: confidence sets.\n",
    "The treatment will be abstract but informal. \n",
    "(For instance, we shall ignore measurability issues.)\n",
    "\n",
    "In an abuse of notation, we will let $\\theta$ denote both the value of a parameter, and\n",
    "the mapping from a distribution to the value of the parameter for that distribution,\n",
    "as if $\\theta$ were a functional parameter even if it is an index parameter (or some other\n",
    "kind of parameter).\n",
    "Thus, $\\theta: \\mathcal{P} \\rightarrow \\Theta$, $\\mathbb{P} \\mapsto \\theta(\\mathbb{P})$.\n",
    "If $\\mathbb{P} = \\mathbb{P}_\\eta$, then $\\theta(\\mathbb{P}) = \\eta$.\n",
    "The set $\\Theta$ will denote the possible values of $\\theta$. \n",
    "Lowercase Greek letters such as $\\eta$ will denote\n",
    "generic elements of $\\Theta$.\n",
    "\n",
    "We shall observe $X \\sim \\mathbb{P}$, where $X$ takes values in the outcome space $\\mathcal{X}$.\n",
    "We do not know $\\mathbb{P}$, but we know that $\\mathbb{P} \\in \\mathcal{P}$, a known set of distributions.\n",
    "Let $\\mathcal{I}(\\cdot)$ be a set-valued function that assigns a subset of $\\Theta$ to each possible observation $x \\in \\mathcal{X}$.\n",
    "For instance, we might observe $X \\sim N(\\theta, 1)$, and $\\mathcal{I}(x)$ might be $[x-c, x+c]$.\n",
    "\n",
    "Fix $\\alpha \\in (0, 1)$.\n",
    "Suppose that for all $\\eta \\in \\Theta$, if $\\theta(\\mathbb{P}) = \\eta$ then\n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\{\\mathcal{I}(X) \\ni \\eta \\} \\ge 1-\\alpha.\n",
    "\\end{equation}\n",
    "Then $\\mathcal{I}(\\cdot)$ is _a $1-\\alpha$ confidence set procedure_ for $\\theta(\\mathbb{P})$.\n",
    "It maps outcomes to sets in such a way that the chance is at least\n",
    "$1-\\alpha$ that the resulting set will contain the true value of the\n",
    "parameter $\\theta(\\mathbb{P})$.\n",
    "\n",
    "If we observe $X=x$, $\\mathcal{I}(x)$ is _a $1-\\alpha$ confidence set for $\\theta$_.\n",
    "The _confidence level_ of the set is $1-\\alpha$.\n",
    "\n",
    "When $\\mathcal{I}(x) \\ni \\theta$, we say that the confidence set _covers_ $\\theta$.\n",
    "The _coverage probability_ of the confidence set procedure $\\mathcal{I}$\n",
    "is \n",
    "\\begin{equation*}\n",
    "\\inf_{\\eta \\in \\Theta} \\inf_{\\mathbb{P} \\in \\mathcal{P} : \\theta(\\mathbb{P}) = \\eta} \\mathbb{P} \\{\\mathcal{I}(X) \\ni \\eta \\}.\n",
    "\\end{equation*}\n",
    "\n",
    "Before the data $X$ are observed, the chance that $\\mathcal{I}(X)$ will contain $\\theta$\n",
    "is the coverage probability, $1-\\alpha$. \n",
    "After the data $X=x$ are observed, the set $\\mathcal{I}(x)$ either\n",
    "does or does not contain $\\theta$: there is nothing random anymore.\n",
    "\n",
    "A _confidence interval_ is a special case of a confidence set, when the set is an interval of real numbers.\n",
    "_One-sided confidence intervals_ are the special case that the confidence set is a semi-infinite interval, i.e., a set of real numbers of the form $(-\\infty, c]$ or $[c, \\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: confidence interval for a Normal mean\n",
    "\n",
    "Suppose $X \\sim N(\\theta, 1)$: $\\theta$ is an index parameter for the (parametric) family of unit variance normal distributions ($\\mathcal{P} \\equiv \\{N(\\eta, 1)\\}_{\\eta \\in \\Re}$) and also a functional parameter, since $\\mathbb{E} X = \\theta$.\n",
    "\n",
    "Define $\\mathcal{I}(x) \\equiv [x - z_{1-\\alpha/2}, x + z_{1-\\alpha/2}]$, where\n",
    "$z_{1-\\alpha/2}$ is the $1-\\alpha/2$ percentile of the standard normal distribution.\n",
    "Then \n",
    "\\begin{equation}\n",
    "   \\mathbb{P}_\\theta \\{ \\mathcal{I}(X) \\ni \\theta \\} = 1-\\alpha\n",
    "\\end{equation}\n",
    "whatever be $\\theta \\in \\Re$.\n",
    "Thus $[x - z_{1-\\alpha/2}, x + z_{1-\\alpha/2}]$ is a $1-\\alpha$\n",
    "confidence interval for $\\theta$.\n",
    "\n",
    "Why is the coverage probability of $[X - z_{1-\\alpha/2}, X + z_{1-\\alpha/2}]$ \n",
    "equal to $1-\\alpha$?\n",
    "\n",
    "The distribution of $X-\\theta$ is a standard normal ($N(0,1)$), so\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}_\\theta  \\{|X-\\theta| \\le z_{1-\\alpha/2} \\} = 1-\\alpha.\n",
    "\\end{equation*}\n",
    "But whenever $|X-\\theta| \\le z_{1-\\alpha/2}$,\n",
    "the interval $[X - z_{1-\\alpha/2}, X + z_{1-\\alpha/2}]$ contains $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duality between hypothesis tests and confidence sets\n",
    "\n",
    "One of the most versatile ways of constructing confidence sets is to _invert_\n",
    "hypothesis tests.\n",
    "\n",
    "Suppose we have a (possibly randomized) family of significance level $\\alpha$ hypothesis tests for all possible values of a parameter $\\theta \\in \\Theta$. \n",
    "That is, for each $\\eta \\in \\Theta$, we have a _test function_ (aka _critical function_)\n",
    "$\\phi_\\eta : \\mathcal{X} \\rightarrow [0, 1]$ such that if $\\theta(\\mathbb{P}) = \\eta$,\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}_{\\mathbb{P}} \\phi_\\eta(X) \\ge 1-\\alpha.\n",
    "\\end{equation*}\n",
    "The test function\n",
    "$\\phi_\\eta(x)$ is the probability of not rejecting the hypothesis $\\theta(\\mathbb{P}) = \\eta$ \n",
    "when $X=x$.\n",
    "When $\\phi_\\eta(X) = 0$, we certainly reject the null; when \n",
    "$\\phi_\\eta(X)=1$, we certainly do not reject the null; values between 0 and 1 correspond to\n",
    "rejecting the null hypothesis with probability $1-\\phi(X)$.\n",
    "The test involves both $X$ and a uniformly distributed random variable $U \\sim U[0,1]$\n",
    "independent of $X$. The test rejects if $U \\ge \\phi(X)$.\n",
    "See [the chapter on hypothesis tests](./tests.ipynb).\n",
    "\n",
    "Consider the set \n",
    "\\begin{equation*}\n",
    "\\mathcal{I}(X,U) \\equiv \\{ \\eta \\in \\Theta : \\phi_\\eta(X) > U \\}.\n",
    "\\end{equation*}\n",
    "That is, $\\mathcal{I}$ is the set of possible parameters $\\eta \\in \\Theta$ for which the corresponding test $\\phi_\\eta$ does not reject the hypothesis that $\\theta(\\mathbb{P}) = \\eta$, for the observed values of $X$ and $U$.\n",
    "(If $\\phi$ can only take the values 0 and 1, i.e., if the test is not randomized, then $\\mathcal{I}$ does not depend on $U$.)\n",
    "\n",
    "**Claim:** $\\mathcal{I}(X,U)$ is a $1-\\alpha$ confidence procedure. That is,\n",
    "whatever the true value of $\\theta(\\mathbb{P})$ happens to be,\n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\{ \\mathcal{I}(X,U) \\ni \\theta(\\mathbb{P}) \\} \\ge 1-\\alpha.\n",
    "\\end{equation}\n",
    "\n",
    "**Proof:** The set \n",
    "$\\mathcal{I}(X,U) \\ni \\theta(\\mathbb{P})$ whenever the test of the null hypothesis that $\\theta(\\mathbb{P}) = \\theta$\n",
    "does not reject, that is, when $\\phi_\\theta(X) \\ge U$.\n",
    "But when $\\theta(\\mathbb{P}) = \\theta$, \n",
    "\\begin{equation*}\n",
    "\\mathbb{P} \\{\\phi_\\theta(X) \\ge U\\} = \\mathbb{E}_{\\mathbb{P}} \\phi(X) \\ge 1-\\alpha.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are many ways to construct a set of tests $\\{\\phi_\\eta\\}_{\\eta \\in \\Theta}$ with significance level $\\alpha$.\n",
    "Inverting different tests will lead to confidence sets with different properties.\n",
    "As a simple example, inverting 1-sided tests for a real parameter will give a 1-seded confidence interval for the parameter, while inverting 2-sided tests will yield a 2-sided confidence interval.\n",
    "\n",
    "It is often possible to design confidence sets that have desirable characteristics--such as avoiding including zero to the extent possible (so that they determine the sign of their parameter), or being on average as small as possible--by inverting suitably chosen tests. \n",
    "See, e.g., [Benjamini, Hochberg, and Stark (1996)](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1998.10474112#.YGk8ERRKg-Q), [Benjamini and Stark (1996)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476692), \n",
    "[Evans, Hansen, and Stark (19??)](), and [Benjamini, Hechtlinger, and Stark (2019)](https://arxiv.org/abs/1906.00505)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: confidence interval for Binomial $p$ (known $n$)\n",
    "\n",
    "Suppose we will observe $X \\sim \\mbox{Binom}(n, p)$, where $n$ is known but $p$ is not.\n",
    "We seek a one-sided lower confidence interval for $p$, that is, a set of the form\n",
    "$[f(X,U), \\infty)$ such that for all $q \\in [0, 1]$, if $X \\sim \\mbox{Binom}(n, q)$\n",
    "and $U$ is an independent uniform random variable, then\n",
    "\\begin{equation*}\n",
    "\\mathbb{P} \\{ [f(X,U), \\infty) \\ni q \\} \\ge 1-\\alpha.\n",
    "\\end{equation*}\n",
    "Since it is certain that $p \\le 1$, the upper endpoint of the interval can be reduced from $\\infty$ to 1 without sacrificing coverage probability. That is, the same $f$ will satisfy\n",
    "\\begin{equation*}\n",
    "\\mathbb{P} \\{ [f(X,U), 1] \\ni q \\} \\ge 1-\\alpha.\n",
    "\\end{equation*}\n",
    "\n",
    "To make such a lower confidence bound, we can invert one-sided hypothesis tests that\n",
    "reject when $X$ is \"too big.\"\n",
    "That is, we want a family of tests of the hypotheses $p = q$ for all $q \\in [0, 1]$\n",
    "that reject for large values of $X$. Such tests give evidence that $p$ is _at least_ a given size.\n",
    "\n",
    "To keep things simple, we will use conservative non-randomized tests rather than exact randomized tests. \n",
    "Because we are basing the confidence intervals on\n",
    "_conservative_ tests, we expect the coverage probability to be greater than $1-\\alpha$.\n",
    "We reject the hypothesis $p = q$ if, on the assumption that $p=q$, the chance that\n",
    "$X$ would be greater than or equal to its observed value is not greater than $\\alpha$.\n",
    "That is, \n",
    "\\begin{equation*}\n",
    "\\phi_q(x) = \\left \\{ \\begin{array}{ll}\n",
    "                      1, & \\sum_{k=x}^n \\binom{n}{k}q^k(1-q)^{n-k} \\ge \\alpha \\\\\n",
    "                      0, & \\mbox{otherwise.}\n",
    "                      \\end{array}\n",
    "            \\right .\n",
    "\\end{equation*}\n",
    "The lower endpoint of the one-sided confidence interval is the smallest value of \n",
    "$q$ for which the\n",
    "corresponding test does not reject:\n",
    "\\begin{equation*}\n",
    "f(x) \\equiv \\min \\left \\{q \\in [0, 1]: \\sum_{k=x}^n \\binom{n}{k}q^k(1-q)^{n-k} \\ge \\alpha \\right \\}.\n",
    "\\end{equation*}\n",
    "Note that the upper tail probability, $\\sum_{k=x}^n \\binom{n}{k}q^k(1-q)^{n-k}$,\n",
    "increases continuously and \n",
    "monotonically as $q$ increases, so finding where it crosses $\\alpha$ is a straightforward\n",
    "root-finding problem.\n",
    "\n",
    "Let's code this up in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.optimize import brentq  # Brent's root-finding algorithm\n",
    "from scipy.stats import binom      # the Binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binom_lower_ci(n, x, cl=0.95):\n",
    "    '''\n",
    "    lower confidence bound for a binomial p\n",
    "    \n",
    "    Assumes x is a draw from a binomial distribution with parameters\n",
    "    n (known) and p (unknown). Finds a lower confidence bound for p \n",
    "    at confidence level cl.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        number of trials, nonnegative integer\n",
    "    x : int\n",
    "        observed number of successes, nonnegative integer not larger than n\n",
    "    cl : float\n",
    "        confidence level, between 0 and 1\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    lb : float\n",
    "        lower confidence bound\n",
    "    '''\n",
    "    assert 0 <= x <= n, 'impossible arguments'\n",
    "    assert 0 < cl < 1, 'silly confidence level'\n",
    "    lb = 0\n",
    "    if x > 0:\n",
    "        lb = brentq(lambda q: binom.sf(x-1,n,q)-(1-cl), 0, 1)\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n: 10\n",
      "p: 0.01 covered:  99.55%\n",
      "p: 0.05 covered:  98.75%\n",
      "p: 0.10 covered:  98.73%\n",
      "p: 0.20 covered:  96.71%\n",
      "p: 0.40 covered:  98.86%\n",
      "p: 0.50 covered:  99.04%\n",
      "p: 0.70 covered:  97.38%\n",
      "p: 0.90 covered:  100.00%\n",
      "p: 0.95 covered:  100.00%\n",
      "p: 0.99 covered:  100.00%\n",
      "\n",
      "n: 50\n",
      "p: 0.01 covered:  98.81%\n",
      "p: 0.05 covered:  96.27%\n",
      "p: 0.10 covered:  97.44%\n",
      "p: 0.20 covered:  96.82%\n",
      "p: 0.40 covered:  96.85%\n",
      "p: 0.50 covered:  96.68%\n",
      "p: 0.70 covered:  95.77%\n",
      "p: 0.90 covered:  97.01%\n",
      "p: 0.95 covered:  100.00%\n",
      "p: 0.99 covered:  100.00%\n",
      "\n",
      "n: 100\n",
      "p: 0.01 covered:  98.25%\n",
      "p: 0.05 covered:  97.16%\n",
      "p: 0.10 covered:  95.84%\n",
      "p: 0.20 covered:  96.31%\n",
      "p: 0.40 covered:  95.78%\n",
      "p: 0.50 covered:  95.46%\n",
      "p: 0.70 covered:  95.55%\n",
      "p: 0.90 covered:  97.53%\n",
      "p: 0.95 covered:  96.29%\n",
      "p: 0.99 covered:  100.00%\n"
     ]
    }
   ],
   "source": [
    "# some simulations to test the confidence intervals\n",
    "reps = int(10**4)\n",
    "for n in [10, 50, 100]:\n",
    "    print(f'\\nn: {n}')\n",
    "    for p in [0.01, 0.05, 0.1, 0.2, 0.4, 0.5, 0.7, 0.9, 0.95, 0.99]:\n",
    "        cover = 0\n",
    "        xs = binom.rvs(n, p, size=reps)\n",
    "        for x in xs:\n",
    "            cover += (1 if binom_lower_ci(n,x) <= p\n",
    "                 else 0)\n",
    "        print(f'p: {p:.2f} covered: {100*cover/reps : .2f}%')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how you might make a 2-sided confidence interval for $p$ instead of a lower 1-sided confidence interval. \n",
    "There are countless ways of constructing acceptance regions for the underlying tests, as mentioned in the [testing](./tests.ipynb) chapter. \n",
    "For instance, we could trim the same probability $\\alpha/2$ from both tails, or use the acceptance region that contains the fewest outcomes. \n",
    "The latter choice in general will lead to shorter confidence intervals.\n",
    "\n",
    "Let's implement that approach now. The development is analogous to the randomized hypergeometric test in [the chapter on testing](./tests/ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)  # decorate the function to cache the results \n",
    "                          # of calls to the function\n",
    "def binom_accept(n, p, alpha=0.05, randomized=False):\n",
    "    '''\n",
    "    Acceptance region for a randomized binomial test\n",
    "    \n",
    "    If randomized==True, find the acceptance region for a randomized, exact \n",
    "    level-alpha test of the null hypothesis X~Binomial(n,p). \n",
    "    The acceptance region is the smallest possible. (And not, for instance, symmetric.)\n",
    "\n",
    "    If randomized==False, find the smallest conservative acceptance region.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : integer\n",
    "        number of independent trials\n",
    "    p : float\n",
    "        probability of success in each trial\n",
    "    alpha : float\n",
    "        desired significance level  \n",
    "    ramndomized : Boolean\n",
    "        return randomized exact test or conservative non-randomized test?\n",
    "  \n",
    "    Returns\n",
    "    --------\n",
    "    If randomized:\n",
    "    I : list\n",
    "        values for which the test never rejects\n",
    "    J : list \n",
    "        values for which the test sometimes rejects\n",
    "    gamma : float\n",
    "        probability the test does not reject when the value is in J\n",
    "    \n",
    "    If not randomized:\n",
    "    I : list\n",
    "        values for which the test does not reject\n",
    "    \n",
    "    '''\n",
    "    assert 0 < alpha < 1, \"bad significance level\"\n",
    "    x = np.arange(0, n+1)\n",
    "    I = list(x)                    # start with all possible outcomes (then remove some)\n",
    "    pmf = binom.pmf(x,n,p)         # \"frozen\" binomial pmf\n",
    "    bottom = 0                     # smallest outcome still in I\n",
    "    top = n                        # largest outcome still in I\n",
    "    J = []                         # outcomes for which the test is randomized\n",
    "    p_J = 0                        # probability of outcomes for which test is randomized\n",
    "    p_tail = 0                     # probability of outcomes excluded from I\n",
    "    while p_tail < alpha:          # need to remove outcomes from the acceptance region\n",
    "        pb = pmf[bottom]\n",
    "        pt = pmf[top]\n",
    "        if pb < pt:                # the smaller possibility has smaller probability\n",
    "            J = [bottom]\n",
    "            p_J = pb\n",
    "            bottom += 1\n",
    "        elif pb > pt:              # the larger possibility has smaller probability\n",
    "            J = [top]\n",
    "            p_J = pt\n",
    "            top -= 1\n",
    "        else:                      \n",
    "            if bottom < top:       # the two possibilities have equal probability\n",
    "                J = [bottom, top]\n",
    "                p_J = pb+pt\n",
    "                bottom += 1\n",
    "                top -= 1\n",
    "            else:                  # there is only one possibility left\n",
    "                J = [bottom]\n",
    "                p_J = pb\n",
    "                bottom +=1\n",
    "        p_tail += p_J\n",
    "        for j in J:                # remove outcomes from acceptance region\n",
    "            I.remove(j)\n",
    "    return_val = None\n",
    "    if randomized:\n",
    "        gamma = (p_tail-alpha)/p_J     # probability of accepting H_0 when X in J \n",
    "                                       # to get exact level alpha\n",
    "        return_val = I, J, gamma\n",
    "    else:\n",
    "        while p_tail > alpha:\n",
    "            j = J.pop()            # move the outcome into the acceptance region\n",
    "            p_tail -= pmf[j]\n",
    "            I.append(j)\n",
    "        return_val = I\n",
    "    return return_val \n",
    "\n",
    "\n",
    "def binom_ci(n, x, cl=0.95, eps=10**-3):\n",
    "    '''\n",
    "    two-sided confidence bound for a binomial p\n",
    "    \n",
    "    Assumes x is a draw from a binomial distribution with parameters\n",
    "    n (known) and p (unknown). Finds a confidence interval for p \n",
    "    at confidence level cl by inverting conservative tests\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        number of trials, nonnegative integer\n",
    "    x : int\n",
    "        observed number of successes, nonnegative integer not larger than n\n",
    "    cl : float\n",
    "        confidence level, between 1/2 and 1\n",
    "    eps : float in (0, 1)\n",
    "        resolution of the grid search\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    lb : float\n",
    "        lower confidence bound\n",
    "    ub : float\n",
    "        upper confidence bound\n",
    "    '''\n",
    "    assert 0 <= x <= n, 'impossible arguments'\n",
    "    assert 0 < cl < 1, 'silly confidence level'\n",
    "    lb = 0\n",
    "    ub = 1\n",
    "    alpha = 1-cl\n",
    "    if x > 0:\n",
    "        while x not in binom_accept(n, lb, alpha, randomized=False):\n",
    "            lb += eps\n",
    "        lb -= eps\n",
    "    if x < n:\n",
    "        while x not in binom_accept(n, ub, alpha, randomized=False):\n",
    "            ub -= eps\n",
    "        lb += eps\n",
    "    return lb, ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n: 10\n",
      "p: 0.01 covered: 99.60% mean width: 0.31\n",
      "p: 0.05 covered: 98.81% mean width: 0.36\n",
      "p: 0.10 covered: 98.65% mean width: 0.41\n",
      "p: 0.20 covered: 96.98% mean width: 0.48\n",
      "p: 0.40 covered: 98.20% mean width: 0.54\n",
      "p: 0.50 covered: 97.98% mean width: 0.55\n",
      "p: 0.70 covered: 96.27% mean width: 0.52\n",
      "p: 0.90 covered: 98.91% mean width: 0.41\n",
      "p: 0.95 covered: 98.75% mean width: 0.36\n",
      "p: 0.99 covered: 99.59% mean width: 0.31\n",
      "\n",
      "n: 50\n",
      "p: 0.01 covered: 98.52% mean width: 0.09\n",
      "p: 0.05 covered: 96.02% mean width: 0.14\n",
      "p: 0.10 covered: 97.11% mean width: 0.17\n",
      "p: 0.20 covered: 94.93% mean width: 0.22\n",
      "p: 0.40 covered: 95.21% mean width: 0.27\n",
      "p: 0.50 covered: 93.60% mean width: 0.27\n",
      "p: 0.70 covered: 95.63% mean width: 0.25\n",
      "p: 0.90 covered: 97.06% mean width: 0.17\n",
      "p: 0.95 covered: 96.24% mean width: 0.13\n",
      "p: 0.99 covered: 98.75% mean width: 0.09\n",
      "\n",
      "n: 100\n",
      "p: 0.01 covered: 98.31% mean width: 0.05\n",
      "p: 0.05 covered: 96.61% mean width: 0.09\n",
      "p: 0.10 covered: 95.53% mean width: 0.12\n",
      "p: 0.20 covered: 95.44% mean width: 0.16\n",
      "p: 0.40 covered: 95.87% mean width: 0.19\n",
      "p: 0.50 covered: 93.93% mean width: 0.20\n",
      "p: 0.70 covered: 94.80% mean width: 0.18\n",
      "p: 0.90 covered: 95.52% mean width: 0.12\n",
      "p: 0.95 covered: 96.67% mean width: 0.09\n",
      "p: 0.99 covered: 98.00% mean width: 0.05\n"
     ]
    }
   ],
   "source": [
    "cl = 0.95\n",
    "eps = 10**-4\n",
    "for n in [10, 50, 100]:\n",
    "    print(f'\\nn: {n}')\n",
    "    for p in [0.01, 0.05, 0.1, 0.2, 0.4, 0.5, 0.7, 0.9, 0.95, 0.99]:\n",
    "        cover = 0\n",
    "        mean_width = 0\n",
    "        xs = binom.rvs(n, p, size=reps)\n",
    "        for x in xs:\n",
    "            bounds = binom_ci(n, x, cl=cl, eps=eps)\n",
    "            cover += (1 if (bounds[0] <= p <= bounds[1])             \n",
    "                      else 0)\n",
    "            mean_width += bounds[1]-bounds[0]\n",
    "        mean_width /= reps\n",
    "        print(f'p: {p:.2f} covered: {100*cover/reps:.2f}% mean width: {mean_width:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I suspect there's a bug: the coverage at p=0.5 should be higher for $n=50$ and $n=100$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap approximate confidence intervals\n",
    "\n",
    "The bootstrap approximates sampling from a population by sampling with replacement from a sample from the population. That is, it approximates the population distribution by the empirical distribution of the observed sample, which is\n",
    "the nonparametric maximum likelihood estimate of the population distribution\n",
    "\n",
    "The bootstrap is commonly used to estimate the variability of an estimator of a functional parameter. It generally does a good job of estimating things like the variance of an estimator.\n",
    "\n",
    "It can also be used to construct approximate confidence intervals in a variety of ways, including the _percentile method_, which approximates percentiles of an estimator from percentiles of the resampling distribution of the estimator. However, this generally is not a good approximation.\n",
    "\n",
    "Let's code it up and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_ci(data, estimator, cl=0.95, reps=int(10**4), random_state=None):\n",
    "    \"\"\"\n",
    "    Bootstrap approximate confidence interval for a functional parameter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array-like\n",
    "        original sample\n",
    "    estimator : callable\n",
    "        function defined on the empirical distribution of a sample.\n",
    "        Applying it to the population distribution would give the\n",
    "        true value of the parameter of interest. Applying it to a \n",
    "        sample yields an estimator of the parameter of interest\n",
    "    cl : float in (0,1)\n",
    "        confidence level\n",
    "    reps : int, nonnegative\n",
    "        number of bootstrap samples to use\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        prng = np.random.randomstate()\n",
    "    else:\n",
    "        prng = random_state\n",
    "    n = len(data)\n",
    "    estimates = []\n",
    "    for j in range(reps):\n",
    "        estimates.append(estimator(prng.choice(data, size=n, replace=True)))\n",
    "    estimates = np.array(estimates)\n",
    "    tail = (1-cl)/2\n",
    "    lower = np.quantile(estimates, tail)\n",
    "    upper = np.quantile(estimates, 1- tail)\n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n: 10\n",
      "p: 0.01 covered:  11.00% mean width: 0.03\n"
     ]
    }
   ],
   "source": [
    "reps = int(10**3)\n",
    "reps_boot = int(10**3)\n",
    "seed = 12345678\n",
    "prng = np.random.RandomState(seed)\n",
    "for n in [10, 50, 100]:\n",
    "    print(f'\\nn: {n}')\n",
    "    for p in [0.01, 0.05, 0.1, 0.2, 0.4, 0.5, 0.7, 0.9, 0.95, 0.99]:\n",
    "        cover = 0\n",
    "        mean_width = 0\n",
    "        xs = binom.rvs(n, p, size=reps)\n",
    "        for x in xs:\n",
    "            sample = [1]*x + [0]*(n-x)\n",
    "            bounds = boot_ci(sample, np.mean, reps=reps_boot, random_state=prng)\n",
    "            cover += (1 if (bounds[0] <= p <= bounds[1])             \n",
    "                      else 0)\n",
    "            mean_width += bounds[1]-bounds[0]\n",
    "        mean_width /= reps\n",
    "        print(f'p: {p:.2f} covered: {100*cover/reps : .2f}% mean width: {mean_width:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the coverage probability of bootstrap percentile confidence intervals can be \n",
    "much lower than their nominal level (here, as low as about 9% when they should be 95%). Where their coverage is about right, their average width is comparable to the average width of the conservative intervals derived by inverting two-sided tests.\n",
    "\n",
    "This example is a fairly simple situation: estimating the mean of a population of zeros and ones from a random sample with replacement. In more complicated examples, bootstrap confidence intervals generally do not attain their nominal level.\n",
    "There is a substantial body of work on how to improve the coverage of bootstrap CIs.\n",
    "_Pre-pivoting_ can help substantially. See [Beran (1987)](https://www.jstor.org/stable/2336685?seq=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: confidence interval for Hypergeometric $G$ (known $N$, $n$)\n",
    "\n",
    "Suppose we will observe $X \\sim \\mbox{Hyper}(N, G, n)$, where $N$ and $n$ are known but $G$ is not.\n",
    "We seek a one-sided lower confidence interval for $G$, that is, a set of the form\n",
    "$[f(X,U), \\infty)$ such that for all $G \\in \\{0, 1, \\ldots, N\\}$, if $X \\sim \\mbox{Hyper}(N, G, n)$\n",
    "and $U$ is an independent uniform random variable, then\n",
    "\\begin{equation*}\n",
    "\\mathbb{P} \\{ [f(X,U), \\infty) \\ni G \\} \\ge 1-\\alpha.\n",
    "\\end{equation*}\n",
    "Since it is certain that $G \\le N$, the upper endpoint of the interval can be reduced from $\\infty$ to $N$ without sacrificing coverage probability. That is, the same $f$ will satisfy\n",
    "\\begin{equation*}\n",
    "\\mathbb{P} \\{ [f(X,U), N] \\ni q \\} \\ge 1-\\alpha.\n",
    "\\end{equation*}\n",
    "\n",
    "To make such a lower confidence bound, we can invert one-sided hypothesis tests that\n",
    "reject when $X$ is \"too big.\"\n",
    "That is, we want a family of tests of the hypotheses $G = H$ for all $H \\in \\{0, 1, \\ldots, N\\}$\n",
    "that reject for large values of $X$. \n",
    "Such tests give evidence that $G$ is _at least_ a given size.\n",
    "\n",
    "To keep things simple, we will use conservative non-randomized tests rather than exact randomized tests. \n",
    "Because we are basing the confidence intervals on\n",
    "_conservative_ tests, we expect the coverage probability to be greater than $1-\\alpha$.\n",
    "We reject the hypothesis $G = I$ if, on the assumption that $G=I$, the chance that\n",
    "$X$ would be greater than or equal to its observed value is not greater than $\\alpha$.\n",
    "That is, \n",
    "\\begin{equation*}\n",
    "\\phi_q(x) = \\left \\{ \\begin{array}{ll}\n",
    "                      1, & \\sum_{k=x}^n \\frac{\\binom{I}{k}\\binom{N-I}{n-k}}{\\binom{N}{n}} \\ge \\alpha \\\\\n",
    "                      0, & \\mbox{otherwise.}\n",
    "                      \\end{array}\n",
    "            \\right .\n",
    "\\end{equation*}\n",
    "(Of course, we know $G \\ge X$.)\n",
    "The lower endpoint of the one-sided confidence interval is the smallest value of \n",
    "$I$ for which the\n",
    "corresponding test does not reject:\n",
    "\\begin{equation*}\n",
    "f(x) \\equiv \\min \\left \\{I \\in \\{x, x+1, \\ldots, N\\} : \\frac{\\binom{I}{k}\\binom{N-I}{n-k}}{\\binom{N}{n}} \\ge \\alpha \\right \\}.\n",
    "\\end{equation*}\n",
    "Note that the upper tail probability does not increase monotonically as $I$ increases;\n",
    "moreover, because $I$ must be an integer, the optimization problem is discrete,\n",
    "not continuous.\n",
    "Thus standard root-finding methods will _not_ let us find where the tail probability\n",
    "crosses $\\alpha$.\n",
    "Instead, we will use a search.\n",
    "\n",
    "Let's code this up in python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypergeom_lower_ci(N, n, x, cl=0.95):\n",
    "    '''\n",
    "    lower confidence bound for a hypergeometric G\n",
    "    \n",
    "    Assumes x is a draw from a hypergeometric distribution with parameters\n",
    "    N (known), n (known), and G (unknown). Finds a lower confidence bound for G \n",
    "    at confidence level cl.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        population size, nonnegative integer\n",
    "    n : int\n",
    "        number of trials, nonnegative integer <= N\n",
    "    x : int\n",
    "        observed number of successes, nonnegative integer <= n\n",
    "    cl : float\n",
    "        confidence level, between 0 and 1\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    lb : float\n",
    "        lower confidence bound\n",
    "    '''\n",
    "    assert 0 <= x <= n, 'impossible arguments'\n",
    "    assert n <= N, 'impossible sample size'\n",
    "    assert 0 < cl < 1, 'silly confidence level'\n",
    "    lb = x\n",
    "    tail = hypergeom.sf(x-1, N, lb, n)\n",
    "    while tail < (1-cl):\n",
    "        lb += 1\n",
    "        tail = hypergeom.sf(x-1, N, lb, n)\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = [20, 50, 100]\n",
    "nn = [10, 20, 30]\n",
    "GG = [10, 30, 25]\n",
    "reps = int(10**4)\n",
    "\n",
    "for j in range(len(NN)):\n",
    "    cover = 0\n",
    "    xs = hypergeom.rvs(NN[j], GG[j], nn[j], size=reps)\n",
    "    for x in xs:\n",
    "        cover += (1 if hypergeom_lower_ci(NN[j], nn[j], x, cl=0.95) <= GG[j]\n",
    "                 else 0)\n",
    "    print(f'N={NN[j]}, G={GG[j]}, n={nn[j]}: covered {100*cover/reps : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals from permutation tests\n",
    "\n",
    "### The two-sample problem\n",
    "\n",
    "Recall from the chapter on [hypothesis tests](./tests.ipynb) that the two-sample problem asks whether two groups plausibly resulted from allocating their union randomly into two groups of their observed sizes.\n",
    "\n",
    "That is, we have two groups of data, $\\{x_j\\}_{j=1}^n$ and $\\{y_j\\}_{j=1}^m$, and hypothesize that they arose by taking the multiset of $n+m$ values $\\{x_1, \\ldots, x_n, y_1, \\ldots, y_m\\}$ and randomly selecting $n$ items to comprise the first group, with the remaining $m$ comprising the second group.\n",
    "\n",
    "This problem arises in many contexts, including randomized controlled trials:\n",
    "We have a group of $n+m$ subjects of whom $n$ are selected at random to be the control/placebo group\n",
    "and the other $m$ receive the active treatment.\n",
    "The _strong null hypothesis_ of no treatment effect is that each subject would have had the same response, no matter which treatment the subject was assigned.\n",
    "It is as if the response were determined before the assignment occurred. \n",
    "The assignment just reveals the response corresponding to the assigned treatment.\n",
    "\n",
    "Thus, testing the strong null hypothesis is an instance of the two-sample problem: did the two sets of values plausibly arise from dividing a single set of values at random into two groups by simple random sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neyman model for causal inference\n",
    "\n",
    "There are $N$ subjects and $T$ possible treatments.\n",
    "Each subject is represented by a ticket. \n",
    "Ticket $j$ lists $T$ numbers, $(x_{j1}, \\ldots, x_{jT})$.\n",
    "The value $x_{jt}$ is the response subject $j$ will have if assigned to treatment $t$.\n",
    "(One of the $T$ \"treatments\" might be control or placebo.)\n",
    "\n",
    "This mathematical set up embodies the _non-interference_ assumption, which means that\n",
    "subject $j$'s response depends only on which treatment subject $j$ receives, and not\n",
    "on the treatment other subjects receive.\n",
    "(That is not a good assumption in situations like vaccine trials, where whether one subject\n",
    "becomes infected may depend on which other subjects are vaccinated, if subjects\n",
    "mnay come in contact with each other.)\n",
    "\n",
    "This model is also called the _potential outcomes_ model, because it starts with the\n",
    "_potential_ outcomes each subject will have to each treatment. Assigning a subject to a \n",
    "treatment just reveals the potential outcome that corresponds to that treatment, for that subject. This model was introduced by Jerzy Neyman, the founder of the U.C. Berkeley Department of Statistics, in a 1923 paper in Polish [translated into English in 1990](https://projecteuclid.org/journals/statistical-science/volume-5/issue-4/On-the-Application-of-Probability-Theory-to-Agricultural-Experiments-Essay/10.1214/ss/1177012031.full).\n",
    "\n",
    "There are generalizations of this model, including one in which the \"potential outcomes\" are random, rather than deterministic, but their distributions are fixed before assignment to treatment: if subject $j$ is assigned treatment $t$, a draw from the distribution $\\mathbb{P}_{jt}$ is observed. Draws for different subjects are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null hypotheses for the Neyman model\n",
    "\n",
    "The _strong_ null hypothesis is that subject by subject, the effect of\n",
    "all $T$ treatments is the same.\n",
    "That is,\n",
    "\\begin{equation*}\n",
    "x_{j1} = x_{j2} = \\cdots = x_{jT}, \\;\\; j=1, \\ldots, N.\n",
    "\\end{equation*}\n",
    "Different subjects may have different responses ($x_{jt}$ might not equal $x_{kt}$ if $j \\ne k$), but each subject's response is the same regardless of the treatment assigned \n",
    "to that subject.\n",
    "This is the null hypothesis Fisher considered in _The Design of Experiments_ and which\n",
    "he generally considered the \"correct\" null in practice.\n",
    "\n",
    "Suppose $T=2$: we are comparing two treatments. Suppose we assign $n$ subjects at random\n",
    "to treatment 1 and the other $m = N-n$ to treatment 2.\n",
    "Let $\\{z_j\\}_{j=1}^n$ be the responses of the subjects assigned treatment 1\n",
    "and $\\{y_j\\}_{j=1}^m$ be the responses of the subjects assigned treatment 2.\n",
    "(That is, $z_1 = x_{k1}$ if $k$ is the first subject assigned treatment $1$,\n",
    "and $y_1 = x_{k2}$ if $k$ is the first subject assigned treatment $2$.)\n",
    "Then testing the strong null hypothesis is identical to the two-sample problem:\n",
    "under the strong null, each subject's response would have been the same, regardless\n",
    "of treatment, so allocating subject to treatments and observing their responses\n",
    "is just randomly partitioning a fixed set of $n$ numbers into a group of size $n$ and a group of size $m$.\n",
    "\n",
    "The _weak_ null hypothesis is that on average across subjects, all treatments have the same effect. \n",
    "That is,\n",
    "\\begin{equation*}\n",
    "\\frac{1}{N} \\sum_{j=1}^N x_{j1} = \\frac{1}{N} \\sum_{j=1}^N x_{j2} = \\ldots = \\frac{1}{N} \\sum_{j=1}^N x_{jT}.\n",
    "\\end{equation*}\n",
    "Much of Neyman's work on experiments involves this null hypothesis.\n",
    "The statistical theory is more complex for the weak null hypothesis than for the strong null.\n",
    "\n",
    "The strong null is indeed a stronger hypothesis than the weak null, because if the strong null is true, the weak null must also be true: if $T$ lists are equal, element by element, then their means are equal. \n",
    "But the converse is not true: the weak null can be true even if the strong null is false.\n",
    "For example, for $T=2$ and $N=2$, we might have potential responses $(0, 1)$ for subject 1 and $(1,0)$ for subject 2. The effect of treatment is to increase subject 1's response from 0 to 1 and to decrease subject 2's response from 1 to 0.\n",
    "The treatment affects both subjects, but the average effect of treatment is the same: the average response across subjects is 1/2, with or without treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative hypotheses\n",
    "\n",
    "#### Constant shift\n",
    "\n",
    "This alternative states that the effect of treatment is to change each subject's response by the same amount, $\\Delta$. That is, $x_{j2} = x_{j1}+\\Delta$ for all subjects $j$.\n",
    "\n",
    "Again, once the original data are observed, this completely specifies the probability distribution of the data: we know what subject $j$'s response would have been had the subject been assigned the other treatment. If the subject was assigned treatment 1, the response would have been larger by $\\Delta$ if the subject had been assigned treatment 2 instead. if the subject was assigned treatment 2, the response would have been smaller by $\\Delta$ if the subject had been assigned treatment 1 instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tractable alternative hypotheses\n",
    "\n",
    "A more general alternative is that $x_{j2} = f(x_{j1})$ for some strictly monotonic (and thus invertible) function $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence intervals when there are nuisance parameters: stratified sampling\n",
    "\n",
    "We now look at an example of constructing a confidence interval when there are nuisance parameters: finding a confidence bound for the mean of a binary population from a stratified sample from the population.\n",
    "This problem occurs in opinion surveys, financial auditing, and election auditing.\n",
    "It will let us see some of the issues that arise when there are nuisance parameters.\n",
    "\n",
    "We will use four powerful techniques:\n",
    "\n",
    "+ the duality between hypothesis tests and confidence sets\n",
    "+ maximizing $P$-values over nuisance parameters\n",
    "+ union-intersection tests\n",
    "+ combining independent $P$-values\n",
    "\n",
    "It is also an example of a combinatorial optimization problem that can be solved quickly using a greedy algorithm--which is rare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem\n",
    "\n",
    "A population of $N$ items of which $G$ are labeled \"1\" and $N-G$ are labeled \"0\"\n",
    "is partitioned into $S$ strata.\n",
    "Stratum $s$ contains $N_s$ items, of which $G_s$ are labeled \"1.\"\n",
    "Thus $N = \\sum_{s=1}^S N_s$ and $G \\equiv \\sum_{s=1}^S G_s$.\n",
    "We draw a simple random sample of size $n_s$ from stratum $s$, $s = 1, \\ldots, S$, independently across strata.\n",
    "(I.e., from stratum $s$ we draw a sample of size $n_s$ in such a way \n",
    "that every subset of $n_s$ distinct items of the $N_s$ items is equally likely;\n",
    "and the $S$ samples are drawn independently.)\n",
    "\n",
    "Let $x_{sj}$ denote the value if the $j$th item in stratum $s$, $s=1, \\ldots, S$, $j=1, \\ldots, N_s$.\n",
    "Then $G_s = \\sum_{j=1}^{N_s} x_{sj}$ and \n",
    "\\begin{equation*}\n",
    "G = \\sum_{s=1}^S \\sum_{j=1}^{N_s} x_{sj}.\n",
    "\\end{equation*}\n",
    "\n",
    "Let $Y_s$ denote the sum of the values in the sample from stratum $s$.\n",
    "The variables $\\{Y_s \\}_{s=1}^S$ are independent.\n",
    "The observed value of $Y_s$ is $y_s$.\n",
    "\n",
    "We seek hypothesis tests and confidence bounds for $G$.\n",
    "We first consider one-sided tests of the hypothesis $G = g$ against the \n",
    "alternative $G > g$, and\n",
    "corresponding lower confidence bounds for $G$.\n",
    "Reversing the roles of \"0\" and \"1\" gives upper confidence \n",
    "bounds, _mutatis mutandis_.\n",
    "\n",
    "The general strategy for testing the hypothesis $G=g$ is to \n",
    "find the largest $P$-value among all ways of allocating $g$ \n",
    "items labeled \"1\" among the $S$ strata \n",
    "(honoring the stratum sizes $\\{N_s\\}$).\n",
    "That is a $P$-value for the _composite_ hypothesis $G=g$.\n",
    "The maximum can be found by examining all such allocations and\n",
    "calculating the $P$-value for each.\n",
    "\n",
    "Maximizing the $P$-value over all allocations of $G$ ones\n",
    "across $S$ strata\n",
    "is combinatorially complex: Feller's \"bars and stars\" argument shows that there are $\\binom{G+S-1}{S-1}$ ways to allocate $G$ objects among $S$ strata.\n",
    "(Some of those can be ruled out, for instance if $G$ exceeds the size of any stratum.)\n",
    "For $S=10$ strata of size $N_s = 400$ and $G = 300$, \n",
    "there are roughly 6.3e+16 allocations: impractical by any standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Aside: Feller's Bars and Stars Argument_\n",
    "\n",
    "How many ways are there to divide $G$ objects into $S$ groups? \n",
    "[Feller (1950)](https://www.wiley.com/en-us/An+Introduction+to+Probability+Theory+and+Its+Applications%2C+Volume+2%2C+2nd+Edition-p-9780471257097) argues as follows.\n",
    "\n",
    "Consider $G$ \"stars\" lined up in a row:\n",
    "\n",
    "\\begin{equation*}\n",
    "********* \\cdots *********\n",
    "\\end{equation*}\n",
    "\n",
    "To divide them into $S$ groups, we place $S-1$ \"bars\" somewhere among them.\n",
    "For instance,\n",
    "\\begin{equation*}\n",
    "**|****|*** \\cdots ******|***\n",
    "\\end{equation*}\n",
    "corresponds to having 2 items in the first group, 4 in the second, ..., and 3 in the $S$th group.\n",
    "Similarly,\n",
    "\\begin{equation*}\n",
    "|||||********* \\cdots *********\n",
    "\\end{equation*}\n",
    "corresponds to having all the items in the $S$th group.\n",
    "The number of ways of partitioning the $G$ items into $S$ groups is thus the number of ways of choosing $S-1$ places to put bars among a total of $G+S-1$ places (the bars and stars pooled together), i.e., $\\binom{G+S-1}{S-1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now examine some approaches to making confidence intervals for the population total $G$ (equivalently, for the population mean $\\mu = G/N$) from a stratified sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wright's Method: Sum of idk Intervals\n",
    "\n",
    "One easy way to get a lower confidence bound for the sum is to take the sum of\n",
    "simultaneous lower confidence bounds for each stratum.\n",
    "Because the samples from different strata are independent, idk's adjustment works.\n",
    "Wright (1991) suggests this approach.\n",
    "\n",
    "A confidence bound for $G_s$ can be constructed from $Y$ by inverting hypergeometric tests.\n",
    "\n",
    "To have joint confidence level $1-\\alpha$, make each confidence interval at $(1-\\alpha)^{1/S}$.\n",
    "\n",
    "The chance that _all_ of the $S$ confidence bounds cover their corresponding parameters is\n",
    "\\begin{equation*}\n",
    "\\prod_{s=1}^S (1-\\alpha)^{1/S} = 1-\\alpha\n",
    "\\end{equation*}\n",
    "because the intervals all based on independent data.\n",
    "This approach to making simultaneous confidence bounds for a set of parameters using\n",
    "independent data for each parameter is called _idk's_ method.\n",
    "\n",
    "Wright's method is an example of a much more general approach: make a joint $1-\\alpha$ confidence set for all the parameters $\\{G_j\\}_{j=1}^S$, then find a lower bound on a functional of interest (here, their sum) over the joint set. \n",
    "Whenever the joint confidence set covers the parameter, the lower bound does not exceed the true value of the functional of the parameter.\n",
    "But because we do not care about the individual stratum sums, only the overall population sum, this approach is unnecessarily conservative: we are \"wasting\" effort constraining them separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wendell-Schmee Test\n",
    "\n",
    "Wendell and Schmee (1996, https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476950) proposed making inferences about the population total by maximizing a $P$-value over a set of nuisance parameters---the individual stratum totals.\n",
    "They find the $P$-value by ordering possible outcomes based on the estimated population proportion: \n",
    "the test statistic is the \"tail probability\" of $\\hat{p}$, the unbiased\n",
    "estimator of the population percentage from the stratified sample.\n",
    "They construct confidence bounds by inverting hypothesis tests.\n",
    "\n",
    "The test statistic for the Wendell-Schmee test is the unbiased estimate\n",
    "of the population proportion, $G/N$:\n",
    "\n",
    "$$\n",
    "  \\hat{p} = \\frac{1}{N} \\sum_{s=1}^S N_s y_s/n_s.\n",
    "$$\n",
    "The $P$-value of the hypothesis $G_s=g_s$, $s=1, \\ldots, S$, \n",
    "is the \"lower tail probability\" of $\\hat{p}$.\n",
    "\n",
    "Wendell and Schmee consider maximizing this lower tail probability over all allocations\n",
    "of $g$ ones across strata, either by exhaustive search, or by numerical optimization\n",
    "using a descent method from some number of random starting points.\n",
    "(They show graphically that the tail probability is not convex in the original\n",
    "parametrization.)\n",
    "\n",
    "In practice, their method is computationally intractable when there are more than $S=3$ strata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructive maximization\n",
    "\n",
    "For some test statistics, there is a much more efficient approach.\n",
    "\n",
    "Define \n",
    "$$\n",
    "   p_s(g_s) \\equiv \\Pr \\{ Y_s \\ge y_s || G_s = g_s \\} =\n",
    "   \\sum_{y = y_s}^{g_s} \\frac{\\binom{g_s}{y} \\binom{N_s-g_s}{n_s - y}}{\\binom{N_s}{n_s}},\n",
    "$$\n",
    "where $\\binom{a}{b} \\equiv 0$ if $a \\le 0$ or $b > a$.\n",
    "(The double vertical bars denote \"computed on the assumption that.\")\n",
    "This is the $P$-value of the hypothesis $G_s = g_s$ tested against the\n",
    "alternative $G_s > g_s$.\n",
    "\n",
    "A test of the conjunction hypothesis $G_s = g_s$, $s=1, \\ldots, S$ can be constructed\n",
    "using _Fisher's combining function_:\n",
    "if all $S$ hypotheses are true, the distribution of\n",
    "$$\n",
    "  X^2(\\vec{g}) \\equiv -2 \\sum_{s=1}^S \\log p_s(g_s)\n",
    "$$\n",
    "is _dominated_ by the chi-square distribution with $2S$ degrees of freedom.\n",
    "(That is, the chance $X^2 \\ge t$ is less then or equal to the chance that a random variable with\n",
    "a chi-square distribution is $\\ge t$, for all $t$.)\n",
    "\n",
    "Let $\\chi_d(z)$ denote the survival function for the chi-sqare distribution with $d$ degrees\n",
    "of freedom, i.e., the chance that a random variable with the chi-square \n",
    "distribution with $d$ degrees of freedom is greater than or equal to $z$.\n",
    "Then a conservative $P$-value for the allocation $\\vec{g}$\n",
    "$$\n",
    "   P(\\vec{g}) = \\chi_{2S}(X^2(\\vec{g})).\n",
    "$$\n",
    "The allocation $\\vec{g}$ of $g$ ones across strata that maximizes the $P$-value\n",
    "is the allocation that minimizes $X^2(\\vec{g})$ and satisfies $\\sum_s g_s = g$.\n",
    "Equivalently, it is the allocation that maximizes $\\sum_{s=1}^S \\log p_s(g_s)$.\n",
    "\n",
    "Let \n",
    "$$\n",
    "   a_s(j) \\equiv \\left \\{ \n",
    "                 \\begin{array}{ll} \n",
    "          \\log p_s(y_s), & j = y_s \\\\\n",
    "          \\log \\left (p_s(j)/p_s(j-1) \\right ), & j = y_s+1, \\ldots N_s-(n_s-y_s).\n",
    "                 \\end{array}\n",
    "                 \\right .\n",
    "$$\n",
    "\n",
    "Then $\\log p_s(g_s) = \\sum_{j=y_s}^{g_s} a_s(j)$ if \n",
    "$y_s \\le g_s \\le N-(n_s-y_s)$, and \n",
    "$\\log p_s(g_s) = -\\infty$ otherwise.\n",
    "Moreover, \n",
    "$$\n",
    "  X^2(\\vec{g}) =  -2\\sum_{s=1}^S a_s(y_s) -2\\sum_{s=1}^S \\sum_{j=y_s+1}^{g_s} a_s(j)\n",
    "$$\n",
    "provided $y_s \\le g_s \\le N-(n_s-y_s)$, $s=1, \\ldots, S$; otherwise, it is infinite.\n",
    "\n",
    "An allocation of $g$ ones across strata is inconsistent with the data unless\n",
    "$g_s \\ge y_s$, $s=1, \\ldots, S$.\n",
    "Thus, in considering how to allocate $g$ ones to maximize the $P$-value, \n",
    "the first sum above, accounting for $\\sum_s y_s$ ones, is \"mandatory,\" or the $P$-value will be zero.\n",
    "The question is how to allocate the remaining $g - \\sum_s y_s$ ones to maximize\n",
    "the $P$-value (equivalently, to minimize $X^2(\\vec{g})$).\n",
    "\n",
    "Let $b_k$ denote the $k$th largest element of the set \n",
    "\n",
    "$$\n",
    "   \\{a_s(j): j=y_s+1, \\ldots, N_s-(n_s-y_s), \\;\\; s=1, \\ldots, S \\},\n",
    "$$\n",
    "with ties broken arbitrarily.\n",
    "Define $\\tilde{g}_y \\equiv g - \\sum_{s=1}^S y_s$.\n",
    "\n",
    "**Proposition.** For every $\\vec{g}$ with $\\sum_s g_s = g$, \n",
    "$$\n",
    "X^2(\\vec{g}) \\ge X_*^2(g) \\equiv \\left \\{ \\begin{array}{ll}\n",
    "    -2 \\left ( \\sum_{s=1}^S a_s(y_s) + \\sum_{k=1}^{\\tilde{g}_y} b_k \n",
    "                \\right ), & \\sum_s y_s \\le g \\le N - \\sum_s (n_s-y_s) \\\\\n",
    "    \\infty, & \\mbox{ otherwise }\n",
    "    \\end{array}\n",
    "    \\right . .\n",
    "$$\n",
    "\n",
    "**Proof.** Any $\\vec{g}$ for which $X^2(\\vec{g})$ is finite includes the first sum\n",
    "and a sum of $\\tilde{g}_y$ elements of $\\{b_k\\}$; the latter is at most the sum of the\n",
    "$\\tilde{g}_y$ largest elements of $\\{b_k\\}$. $\\Box$\n",
    "\n",
    "Moreover, if the $\\tilde{g}_y$ largest elements of $\\{b_k \\}$ correspond to\n",
    "an allocation of $\\tilde{g}_y$ ones across the strata, the bound is sharp.\n",
    "That turns out to be true (the proof involves log convexity of the distributions).\n",
    "The second sum thus corresponds \n",
    "to a particular allocation \n",
    "$\\vec{g}$ of $g$ ones across the $S$ strata, with $y_s \\le g_s \\le N_s-(n_s-y_s)$.\n",
    "Among all allocations of $g$ items labeled \"1,\" this one minimizes has the smallest tail \n",
    "probability, because it is the exponentiation of the smallest sum of logs \n",
    "(the largest negative sum of logs). $\\Box$\n",
    "\n",
    "\n",
    "**Proposition:** For $j \\in y_s+1, \\ldots, N_s-(n_s-y_s)$, $a_s(j)$ is monotone \n",
    "decreasing in $j$.\n",
    "\n",
    "Let $P(g) \\equiv \\max_{\\vec{g}: \\sum_s g_s = g} P(\\vec{g})$.\n",
    "\n",
    "**Theorem:** If $\\sum_s y_s \\le g \\le N - \\sum_s (n_s-y_s)$, \n",
    "\n",
    "$$\n",
    "P(g) \\le \\chi_d(X_*^2(g)).\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "Immediate from the definitions.\n",
    "\n",
    "The theorem shows that a \"greedy\" approach finds a conservative $P$-value:\n",
    "\n",
    "+ Construct the values $a_s(j)$ and the set $\\{b_k\\}$. \n",
    "+ Add the $S$ values $\\{a_s(x_k)$ to the $g-g_y$ largest elements of $\\{b_k\\}$ and \n",
    "multiply the sum by $-2$.\n",
    "+ The upper tail probability of the chi-square distribution with $2S$ degrees of \n",
    "freedom is a conservative $P$-value for the hypothesis $G=g$.\n",
    "\n",
    "A conservative upper $1-\\alpha$ confidence bound for $G$ can be found by inverting the test: it is the largest $g$ for which \n",
    "$P(g) \\ge \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the direction of the test\n",
    "\n",
    "The test of the hypothesis $G=g$ given above is a one-sided test against the alternative \n",
    "$G > g$: it rejects if the chance of observing \"so few\" good objects is small.\n",
    "\n",
    "To test against the alternative $G < g$ (i.e., to reject if the chance of observing \"so many\"\n",
    "good objects is small), exchange the role of \"good\" and \"bad.\"\n",
    "The hypothesis $G < g$ is equivalent to the hypothesis $(N-G) > (N-g)$.\n",
    "\n",
    "The resulting null hypothesis is $G = N-g$, and the data are $n_s - Y_s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now code the combinatorial optimization and the greedy optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install permute and cryptorandom in the current kernel, if not already installed\n",
    "import sys\n",
    "!{sys.executable} -m pip install permute --user\n",
    "!{sys.executable} -m pip install cryptorandom --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom, hypergeom, chi2\n",
    "import itertools\n",
    "import warnings\n",
    "from permute.utils import binom_conf_interval, hypergeom_conf_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)  # decorate the function to cache the results \n",
    "                          # of calls to the function\n",
    "def strat_test_brute(strata, sams, found, good, alternative='upper'):\n",
    "    \"\"\"\n",
    "    Find p-value of the hypothesis that the number G of \"good\" objects in a \n",
    "    stratified population is less than or equal to good, using a stratified\n",
    "    random sample.\n",
    "    \n",
    "    Assumes that a simple random sample of size sams[s] was drawn from stratum s, \n",
    "    which contains strata[s] objects in all.\n",
    "    \n",
    "    The P-value is the maximum Fisher combined P-value across strata\n",
    "    over all allocations of good objects among the strata. The allocations are\n",
    "    enumerated using Feller's \"bars and stars\" construction, constrained to honor the\n",
    "    stratum sizes (each stratum can contain no more \"good\" items than it has items in all,\n",
    "    nor fewer \"good\" items than the sample contains).\n",
    "    \n",
    "    The number of allocations grows combinatorially: there can be as many as\n",
    "    [(#strata + #good items) choose (#strata-1)] allocations, making the brute-force\n",
    "    approach computationally infeasible when the number of strata and/or the number of\n",
    "    good items is large.\n",
    "    \n",
    "    The test is a union-intersection test: the null hypothesis is the union over allocations\n",
    "    of the intersection across strata of the hypothesis that the number of good items\n",
    "    in the stratum is less than or equal to a constant.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strata : list of ints\n",
    "        sizes of the strata. One int per stratum.\n",
    "    sams : list of ints\n",
    "        the sample sizes from each stratum\n",
    "    found : list of ints\n",
    "        the numbers of \"good\" items found in the samples from the strata\n",
    "    good : int\n",
    "        the hypothesized total number of \"good\" objects in the population\n",
    "    alternative : string {'lower', 'upper'}\n",
    "        test against the alternative that the true value is less than good (lower)\n",
    "        or greater than good (upper)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    p : float\n",
    "        maximum combined p-value over all ways of allocating good \"good\" objects\n",
    "        among the strata, honoring the stratum sizes.        \n",
    "    best_part : list\n",
    "        the partition that attained the maximum p-value\n",
    "    \"\"\"\n",
    "    if alternative == 'upper': # exchange roles of \"good\" and \"bad\"\n",
    "        p_func = lambda f, s, p, x: sp.stats.hypergeom.logcdf(f, s, p, x)\n",
    "    elif alternative == 'lower':\n",
    "        p_func = lambda f, s, p, x: sp.stats.hypergeom.logsf(f-1, s, p, x)\n",
    "    else:\n",
    "        raise NotImplementedError(\"alternative {} not implemented\".format(alternative))\n",
    "    best_part = found # start with what you see\n",
    "    strata = np.array(strata, dtype=int)\n",
    "    sams = np.array(sams, dtype=int)\n",
    "    found = np.array(found, dtype=int)\n",
    "    good = int(good)\n",
    "    if good < np.sum(found):     \n",
    "        p = 0 if alternative == 'lower' else 1 \n",
    "    elif good > np.sum(strata) - np.sum(sams) + np.sum(found):\n",
    "        p = 1 if alternative == 'lower' else 0\n",
    "    else:  # use Feller's \"bars and stars\" enumeration of combinations, constrained\n",
    "        log_p = np.NINF   # initial value for the max\n",
    "        n_strata = len(strata)\n",
    "        parts = sp.special.binom(good+n_strata-1, n_strata-1)\n",
    "        if parts >= 10**7:\n",
    "            print(\"warning--large number of partitions: {}\".format(parts))\n",
    "        barsNstars = good + n_strata\n",
    "        bars = [0]*n_strata + [barsNstars]\n",
    "        partition = ([bars[j+1] - bars[j] - 1 for j in range(n_strata)] \\\n",
    "            for bars[1:-1] in itertools.combinations(range(1, barsNstars), n_strata-1) \\\n",
    "            if all(((bars[j+1] - bars[j] - 1 <= strata[j]) and \\\n",
    "                   (bars[j+1] - bars[j] >= found[j])) for j in range(n_strata)))\n",
    "        for part in partition:\n",
    "            log_p_new = 0 \n",
    "            for s in range(n_strata): # should be possible to vectorize this\n",
    "                log_p_new += p_func(found[s], strata[s], part[s], sams[s])\n",
    "            if log_p_new > log_p:\n",
    "                best_part = part\n",
    "                log_p = log_p_new\n",
    "        p = sp.stats.chi2.sf(-2*log_p, df = 2*len(strata))\n",
    "    return p, best_part\n",
    "\n",
    "@lru_cache(maxsize=None)  # decorate the function to cache the results \n",
    "                          # of calls to the function\n",
    "def strat_ci_bisect(strata, sams, found, alternative='lower', cl=0.95,\n",
    "                  p_value=strat_test_brute):\n",
    "    \"\"\"\n",
    "    Confidence bound on the number of ones in a stratified population,\n",
    "    based on a stratified random sample (without replacement) from\n",
    "    the population.\n",
    "    \n",
    "    \n",
    "    If alternative=='lower', finds an upper confidence bound.\n",
    "    If alternative=='upper', finds a lower confidence bound.\n",
    "\n",
    "    Uses an integer bisection search to find an exact confidence bound.\n",
    "    The starting upper endpoint for the search is the unbiased estimate\n",
    "    of the number of ones in the population. That could be refined in various\n",
    "    ways to improve efficiency.\n",
    "    \n",
    "    The lower endpoint for the search is the idk joint lower confidence bounds,\n",
    "    which should be more conservative than the exact bound.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    strata : list of ints\n",
    "        stratum sizes\n",
    "    sams : list of ints\n",
    "        sample sizes in the strata\n",
    "    found : list of ints\n",
    "        number of ones found in each stratum in each sample\n",
    "    alternative : string {'lower', 'upper'}\n",
    "        if alternative=='lower', finds an upper confidence bound.\n",
    "        if alternative=='upper', finds a lower confidence bound.\n",
    "        While this is not mnemonic, it corresponds to the sidedness of the tests\n",
    "        that are inverted to get the confidence bound.\n",
    "    cl : float\n",
    "        confidence level. Assumed to be at least 50%.\n",
    "    p_value : callable\n",
    "        method for computing the p-value\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    b : int\n",
    "        confidence bound\n",
    "    best_part : list of ints\n",
    "        partition that attains the confidence bound\n",
    "    \"\"\"\n",
    "    assert alternative in ['lower', 'upper']\n",
    "    if alternative == 'upper':  # interchange good and bad\n",
    "        compl = np.array(sams)-np.array(found)  # bad items found\n",
    "        cb, best_part = strat_ci_bisect(strata, sams, compl, alternative='lower', \n",
    "                                       cl=cl, p_value=p_value)\n",
    "        b = np.sum(strata) - cb    # good from bad\n",
    "        best_part_b = np.array(strata, dtype=int)-np.array(best_part, dtype=int)\n",
    "    else:\n",
    "        cl_sidak = math.pow(cl, 1/len(strata))  # idk adjustment\n",
    "        tail = 1-cl\n",
    "        a = sum((hypergeom_conf_interval( \\\n",
    "                sams[s], found[s], strata[s], cl=cl_sidak, alternative=\"lower\")[0] \\\n",
    "                for s in range(len(strata)))) # idk should give a lower bound\n",
    "        b = int(np.sum(np.array(strata)*np.array(found)/np.array(sams)))-1 # expected good\n",
    "        p_a, best_part_a = p_value(strata, sams, found, a, alternative=alternative)\n",
    "        p_b, best_part_b = p_value(strata, sams, found, b, alternative=alternative)\n",
    "        tot_found = np.sum(found)\n",
    "        while p_a > tail and a > tot_found:\n",
    "            a = math.floor(a/2)\n",
    "            p_a, best_part_a = p_value(strata, sams, found, a, alternative=alternative)\n",
    "        if p_a > tail:\n",
    "            b = a\n",
    "            best_part_b = best_part_a\n",
    "        else:\n",
    "            while b-a > 1:\n",
    "                c = int((a+b)/2)\n",
    "                p_c, best_part_c = p_value(strata, sams, found, \n",
    "                                           c, alternative=alternative)\n",
    "                if p_c > tail:\n",
    "                    b, p_b, best_part_b = c, p_c, best_part_c\n",
    "                elif p_c < tail:\n",
    "                    a, p_a, best_part_a = c, p_c, best_part_c\n",
    "                elif p_c == tail:\n",
    "                    b, p_b, best_part_b = c, p_c, best_part_c\n",
    "                    break\n",
    "    return b, list(best_part_b)\n",
    "    \n",
    "@lru_cache(maxsize=None)  # decorate the function to cache the results \n",
    "                          # of calls to the function\n",
    "def strat_test(strata, sams, found, good, alternative='lower'):\n",
    "    \"\"\"\n",
    "    P-value for the hypothesis the number of ones in a stratified population is not \n",
    "    greater than (or not less than) a hypothesized value, based on a stratified \n",
    "    random sample without replacement from the population.\n",
    "    \n",
    "    Uses a fast algorithm to find (an upper bound on) the P-value constructively.\n",
    "    \n",
    "    Uses Fisher's combining function to combine stratum-level P-values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    strata : list of ints\n",
    "        stratum sizes\n",
    "    sams : list of ints\n",
    "        sample sizes in the strata\n",
    "    found : list of ints\n",
    "        number of ones found in each stratum in each sample\n",
    "    good : int\n",
    "        hypothesized number of ones in the population\n",
    "    alternative : string {'lower', 'upper'}\n",
    "        test against the alternative that the true number of \"good\" items \n",
    "        is less than (lower) or greater than (upper) the hypothesized number, good\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    p : float\n",
    "        P-value\n",
    "    best_part : list\n",
    "        the partition that attained the maximum p-value\n",
    "    \"\"\"\n",
    "    assert alternative in ['lower', 'upper']\n",
    "    if alternative == 'upper':                 # exchange roles of \"good\" and \"bad\"\n",
    "        compl = np.array(sams) - np.array(found) # bad items found \n",
    "        bad = np.sum(strata) - good            # total bad items hypothesized\n",
    "        res = strat_test(strata, sams, compl, bad, alternative='lower')\n",
    "        return res[0], list(np.array(strata, dtype=int)-np.array(res[1], dtype=int))\n",
    "    else:  \n",
    "        good = int(good)\n",
    "        best_part = []               # best partition\n",
    "        if good < np.sum(found):     # impossible\n",
    "            p = 0  \n",
    "            best_part = found\n",
    "        elif good >= np.sum(strata) - np.sum(sams) + np.sum(found): # guaranteed\n",
    "            p = 1\n",
    "            best_part = list(np.array(strata, dtype=int) - \\\n",
    "                             np.array(sams, dtype=int) + \\\n",
    "                             np.array(found, dtype=int))       \n",
    "        elif good >= np.sum(found):  # outcome is possible under the null  \n",
    "            log_p = 0                # log of joint probability\n",
    "            contrib = []             # contributions to the log joint probability\n",
    "            base = np.sum(found)     # must have at least this many good items\n",
    "            for s in range(len(strata)):\n",
    "                log_p_j = sp.stats.hypergeom.logsf(found[s]-1, strata[s], found[s], sams[s])\n",
    "                                     # baseline p for minimum number of good items in stratum\n",
    "                log_p += log_p_j     # log of the product of stratum-wise P-values\n",
    "                for j in range(found[s]+1, strata[s]-(sams[s]-found[s])+1):\n",
    "                    log_p_j1 = sp.stats.hypergeom.logsf(found[s]-1, strata[s], j, sams[s])\n",
    "                                     # tail probability for j good in stratum\n",
    "                    contrib.append([log_p_j1 - log_p_j, s])  \n",
    "                                     # relative increase in P from new item\n",
    "                    log_p_j = log_p_j1\n",
    "            sorted_contrib = sorted(contrib, key = lambda x: x[0], reverse=True)\n",
    "            best_part = np.array(found)\n",
    "            for i in range(good-base):\n",
    "                log_p += sorted_contrib[i][0]\n",
    "                best_part[int(sorted_contrib[i][1])] += 1\n",
    "            p = sp.stats.chi2.sf(-2*log_p, df = 2*len(strata))\n",
    "    return p, list(best_part)\n",
    "\n",
    "def strat_ci_search(strata, sams, found, alternative='lower', cl=0.95):\n",
    "    \"\"\"\n",
    "    Confidence bound on the number of ones in a stratified population,\n",
    "    based on a stratified random sample (without replacement) from\n",
    "    the population.\n",
    "        \n",
    "    If alternative=='lower', finds an upper confidence bound.\n",
    "    If alternative=='upper', finds a lower confidence bound.\n",
    "    \n",
    "    Searches for the allocation of items that attains the confidence bound\n",
    "    by increasing the number of ones from the minimum consistent\n",
    "    with the data (total found in the sample) until the P-value is greater\n",
    "    than 1-cl.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    strata : list of ints\n",
    "        stratum sizes\n",
    "    sams : list of ints\n",
    "        sample sizes in the strata\n",
    "    found : list of ints\n",
    "        number of ones found in each stratum in each sample\n",
    "    alternative : string {'lower', 'upper'}\n",
    "        if alternative=='lower', finds an upper confidence bound.\n",
    "        if alternative=='upper', finds a lower confidence bound.\n",
    "        While this is not mnemonic, it corresponds to the sidedness of the tests\n",
    "        that are inverted to get the confidence bound.\n",
    "    cl : float\n",
    "        confidence level. Assumed to be at least 50%.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cb : int\n",
    "        confidence bound\n",
    "    best_part : list of ints\n",
    "        partition that attains the confidence bound (give or take one item)\n",
    "    \"\"\"\n",
    "    assert alternative in ['lower', 'upper']\n",
    "    if alternative == 'upper':  # interchange good and bad\n",
    "        compl = np.array(sams)-np.array(found)  # bad items found\n",
    "        cb, best_part = strat_ci(strata, sams, compl, alternative='lower', cl=cl)\n",
    "        cb = np.sum(strata) - cb    # good from bad\n",
    "        best_part = np.array(strata, dtype=int)-np.array(best_part, dtype=int)\n",
    "    else:\n",
    "        cb = int(np.sum(np.array(strata)*np.array(found)/np.array(sams)))-1 # expected good\n",
    "        p_attained, best_part = strat_test(strata, sams, found, cb, \n",
    "                                                 alternative=alternative)\n",
    "        while p_attained >= 1-cl:\n",
    "            cb -= 1\n",
    "            p_attained, best_part = strat_test(strata, sams, found, cb, \n",
    "                                                     alternative=alternative)\n",
    "        cb += 1\n",
    "        p_attained, best_part = strat_test(strata, sams, found, cb, \n",
    "                                                 alternative=alternative)\n",
    "    return cb, list(best_part)\n",
    "\n",
    "def strat_ci(strata, sams, found, alternative='lower', cl=0.95):\n",
    "    \"\"\"\n",
    "    Conservative confidence bound on the number of ones in a population,\n",
    "    based on a stratified random sample (without replacement) from\n",
    "    the population.\n",
    "    \n",
    "    If alternative=='lower', finds an upper confidence bound.\n",
    "    If alternative=='upper', finds a lower confidence bound.\n",
    "    \n",
    "    Constructs the confidence bound directly by constructing the\n",
    "    allocation of the maximum number of ones that would not be\n",
    "    rejected at (conservative) level 1-cl.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    strata : list of ints\n",
    "        stratum sizes\n",
    "    sams : list of ints\n",
    "        sample sizes in the strata\n",
    "    found : list of ints\n",
    "        number of ones found in each stratum in each sample\n",
    "    alternative : string {'lower', 'upper'}\n",
    "        if alternative=='lower', finds an upper confidence bound.\n",
    "        if alternative=='upper', finds a lower confidence bound.\n",
    "        While this is not mnemonic, it corresponds to the sidedness of the tests\n",
    "        that are inverted to get the confidence bound.\n",
    "    cl : float\n",
    "        confidence level\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cb : int\n",
    "        confidence bound\n",
    "    best_part : list of ints\n",
    "        partition that attains the confidence bound (give or take one item)\n",
    "    \"\"\"\n",
    "    assert alternative in ['lower', 'upper']\n",
    "    if alternative == 'upper':  # interchange role of good and bad\n",
    "        compl = np.array(sams)-np.array(found)  # bad found\n",
    "        cb, best_part = strat_ci(strata, sams, compl, alternative='lower', cl=cl)\n",
    "        best_part = np.array(strata, dtype=int)-np.array(best_part, dtype=int)\n",
    "        cb = np.sum(strata) - cb   # good to bad\n",
    "    else:                \n",
    "        threshold = -sp.stats.chi2.ppf(cl, df=2*len(strata))/2\n",
    "        # g is in the set if \n",
    "        #   chi2.sf(-2*log(p), df=2*len(strata)) >= 1-cl\n",
    "        #  i.e.,   -2*log(p) <=  chi2.ppf(cl, df)\n",
    "        #  i.e.,   log(p) >= -chi2.ppf(cl, df)/2\n",
    "        log_p = 0                # log of joint probability\n",
    "        contrib = []             # contributions to the log joint probability\n",
    "        base = np.sum(found)     # must have at least this many good items\n",
    "        for s in range(len(strata)):\n",
    "            log_p_j = sp.stats.hypergeom.logsf(found[s]-1, strata[s], found[s], sams[s])\n",
    "                                 # baseline p for minimum number of good items in stratum\n",
    "            log_p += log_p_j     # log of the product of stratum-wise P-values\n",
    "            small = np.PINF      # for monotonicity check\n",
    "            for j in range(found[s]+1, strata[s]-(sams[s]-found[s])+1):\n",
    "                log_p_j1 = sp.stats.hypergeom.logsf(found[s]-1, strata[s], j, sams[s])\n",
    "                log_p_j1 = log_p_j if log_p_j1 < log_p_j else log_p_j1 # true difference is nonnegative\n",
    "                contrib.append([log_p_j1 - log_p_j, s]) \n",
    "                log_p_j = log_p_j1\n",
    "                if contrib[-1][0] > small:\n",
    "                    print(\"reversal in stratum {} for {} good; old: {} new:{}\".format(s, \n",
    "                                    j, small, contrib[-1][0]))\n",
    "                small = contrib[-1][0]\n",
    "        sorted_contrib = sorted(contrib, key = lambda x: x[0], reverse=True)\n",
    "        best_part = np.array(found)\n",
    "        added = 0\n",
    "        while log_p < threshold: \n",
    "            log_p += sorted_contrib[added][0]\n",
    "            best_part[int(sorted_contrib[added][1])] += 1\n",
    "            added += 1\n",
    "        cb = base + added \n",
    "    return cb, list(best_part)\n",
    "\n",
    "def strat_p(strata, sams, found, hypo, alternative='lower'):\n",
    "    \"\"\"\n",
    "    Finds tail probability for the hypothesized population counts <hypo> for \n",
    "    simple random samples of sizes <sams> from strata of sizes <strata> if \n",
    "    <found> ones are found in the strata.\n",
    "    \n",
    "    Uses Fisher's combining function across strata.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    strata : list of ints\n",
    "        stratum sizes\n",
    "    sams : list of ints\n",
    "        sample sizes from the strata\n",
    "    found : list of ints\n",
    "        number of ones found in each stratum in each sample\n",
    "    hypo : list of ints\n",
    "        hypothesized number of ones in the strata\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    p : float\n",
    "        appropriate tail probability\n",
    "    \"\"\"\n",
    "    assert alternative in ['lower', 'upper']\n",
    "    if alternative == 'lower':\n",
    "        p_func = lambda x, N, G, n: sp.stats.hypergeom.sf(x-1, N, G, n)\n",
    "    else:\n",
    "        p_func = lambda x, N, G, n: sp.stats.hypergeom.cdf(x, N, G, n)\n",
    "    p = 1    \n",
    "    for s in range(len(strata)):\n",
    "        p *= p_func(found[s], strata[s], hypo[s], sams[s])\n",
    "    return sp.stats.chi2.sf(-2*math.log(p), df=2*len(strata))\n",
    "\n",
    "def strat_p_ws(strata, sams, found, hypo, alternative='upper'):\n",
    "    \"\"\"\n",
    "    Finds Wendell-Schmee P-value for the hypothesized population counts <hypo> for \n",
    "    simple random samples of sizes <sams> from strata of sizes <strata> if \n",
    "    <found> ones are found in the strata.\n",
    "        \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    strata : list of ints\n",
    "        stratum sizes\n",
    "    sams : list of ints\n",
    "        sample sizes from the strata\n",
    "    found : list of ints\n",
    "        number of ones found in each stratum in each sample\n",
    "    hypo : list of ints\n",
    "        hypothesized number of ones in the strata\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    p : float\n",
    "        tail probability\n",
    "    \"\"\"\n",
    "    assert alternative in ['lower', 'upper']\n",
    "    if alternative == 'lower':                     # exchange roles of \"good\" and \"bad\"\n",
    "        compl = np.array(sams) - np.array(found)   # bad items found \n",
    "        hypo_c = np.array(strata) - np.array(hypo) # total bad items hypothesized\n",
    "        p = strat_p_ws(strata, sams, compl, hypo_c, alternative='upper')\n",
    "    else:    \n",
    "        p_hat = lambda f, st=strata, sa=sams: np.sum(np.array(st)*np.array(f)/np.array(sa))/np.sum(st) # pooled estimate\n",
    "        p_hat_0 = p_hat(found)\n",
    "        per_strat = np.array(strata)/np.array(sams)/np.sum(strata)\n",
    "        strat_max = np.floor(p_hat_0/per_strat)\n",
    "        lo_t = (t for t in itertools.product(*[range(int(s+1)) for s in strat_max]) \\\n",
    "                    if p_hat(t) <= p_hat_0)\n",
    "        p = sum(np.prod(sp.stats.hypergeom.pmf(t, strata, hypo, sams)) \\\n",
    "                    for t in lo_t)\n",
    "    return p\n",
    "\n",
    "@lru_cache(maxsize=None)  # decorate the function to cache the results \n",
    "                          # of calls to the function\n",
    "def strat_test_ws(strata, sams, found, good, alternative='lower'):\n",
    "    \"\"\"\n",
    "    Find p-value of the hypothesis that the number G of \"good\" objects in a \n",
    "    stratified population is less than or equal to good, using a stratified\n",
    "    random sample.\n",
    "    \n",
    "    Assumes that a simple random sample of size sams[s] was drawn from stratum s, \n",
    "    which contains strata[s] objects in all.\n",
    "    \n",
    "    The P-value is the maximum Windell-Schmee P-value over all allocations of \n",
    "    good objects among the strata. The allocations are enumerated using Feller's \n",
    "    \"bars and stars\" construction, constrained to honor the stratum sizes (each \n",
    "    stratum can contain no more \"good\" items than it has items in all, nor fewer \n",
    "    \"good\" items than the sample contains).\n",
    "    \n",
    "    The number of allocations grows combinatorially: there can be as many as\n",
    "    [(#strata + #good items) choose (#strata-1)] allocations, making the brute-force\n",
    "    approach computationally infeasible when the number of strata and/or the number of\n",
    "    good items is large.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strata : list of ints\n",
    "        sizes of the strata. One int per stratum.\n",
    "    sams : list of ints\n",
    "        the sample sizes from each stratum\n",
    "    found : list of ints\n",
    "        the numbers of \"good\" items found in the samples from the strata\n",
    "    good : int\n",
    "        the hypothesized total number of \"good\" objects in the population\n",
    "    alternative : string {'lower', 'upper'}\n",
    "        test against the alternative that the true value is less than good (lower)\n",
    "        or greater than good (upper)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    p : float\n",
    "        maximum combined p-value over all ways of allocating good \"good\" objects\n",
    "        among the strata, honoring the stratum sizes.        \n",
    "    best_part : list\n",
    "        the partition that attained the maximum p-value\n",
    "    \"\"\"\n",
    "    assert alternative in ['lower', 'upper']\n",
    "    if alternative == 'lower':                   # exchange roles of \"good\" and \"bad\"\n",
    "        compl = np.array(sams) - np.array(found) # bad items found \n",
    "        bad = np.sum(strata) - good              # total bad items hypothesized\n",
    "        res = strat_test_ws(strata, sams, compl, bad, alternative='upper')\n",
    "        return res[0], list(np.array(strata, dtype=int)-np.array(res[1], dtype=int))        \n",
    "    best_part = found # start with what you see\n",
    "    good = int(good)\n",
    "    if good < np.sum(found):     \n",
    "        p = 0 if alternative == 'lower' else 1 \n",
    "    elif good > np.sum(strata) - np.sum(sams) + np.sum(found):\n",
    "        p = 1 if alternative == 'lower' else 0\n",
    "    else:  # use Feller's \"bars and stars\" enumeration of combinations, constrained\n",
    "        p_hat = lambda f, st=strata, sa=sams: np.sum(np.array(st)*np.array(f)/np.array(sa))/np.sum(st) # pooled estimate\n",
    "        p_hat_0 = p_hat(found)\n",
    "        per_strat = np.array(strata)/np.array(sams)/np.sum(strata)\n",
    "        strat_max = np.floor(p_hat_0/per_strat)\n",
    "        p = 0   # initial value for the max\n",
    "        n_strata = len(strata)\n",
    "        parts = sp.special.binom(good+n_strata-1, n_strata-1)\n",
    "        if parts >= 10**7:\n",
    "            print(\"warning--large number of partitions: {}\".format(parts))\n",
    "        barsNstars = good + n_strata\n",
    "        bars = [0]*n_strata + [barsNstars]\n",
    "        partition = ([bars[j+1] - bars[j] - 1 for j in range(n_strata)] \\\n",
    "            for bars[1:-1] in itertools.combinations(range(1, barsNstars), n_strata-1) \\\n",
    "            if all(((bars[j+1] - bars[j] - 1 <= strata[j]) and \\\n",
    "                   (bars[j+1] - bars[j] >= found[j])) for j in range(n_strata)))\n",
    "        for part in partition:\n",
    "            lo_t = (t for t in itertools.product(*[range(int(s+1)) for s in strat_max]) \\\n",
    "                    if p_hat(t) <= p_hat_0)\n",
    "            p_new = 0\n",
    "            for t in lo_t:\n",
    "                p_temp = 1\n",
    "                for s in range(len(strata)):\n",
    "                    p_temp *= sp.stats.hypergeom.pmf(t[s], strata[s], part[s], sams[s])\n",
    "                p_new += p_temp\n",
    "            if p_new > p:\n",
    "                best_part = part\n",
    "                p = p_new\n",
    "    return p, list(best_part)\n",
    "\n",
    "def strat_ci_wright(strata, sams, found, alternative='lower', cl=0.95):\n",
    "    \"\"\"\n",
    "    Confidence bound on the number of ones in a stratified population,\n",
    "    based on a stratified random sample (without replacement) from\n",
    "    the population.\n",
    "    \n",
    "    If alternative=='lower', finds an upper confidence bound.\n",
    "    If alternative=='upper', finds a lower confidence bound.\n",
    "    \n",
    "    Constructs the confidence bound by finding idk multiplicity-adjusted\n",
    "    joint lower confidence bounds for the number of ones in each stratum.\n",
    "    \n",
    "    This approach is mentioned in Wright, 1991.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------    \n",
    "    strata : list of ints\n",
    "        stratum sizes\n",
    "    sams : list of ints\n",
    "        sample sizes in the strata\n",
    "    found : list of ints\n",
    "        number of ones found in each stratum in each sample\n",
    "    alternative : string {'lower', 'upper'}\n",
    "        if alternative=='lower', finds an upper confidence bound.\n",
    "        if alternative=='upper', finds a lower confidence bound.\n",
    "        While this is not mnemonic, it corresponds to the sidedness of the tests\n",
    "        that are inverted to get the confidence bound.\n",
    "    cl : float\n",
    "        confidence level\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cb : int\n",
    "        confidence bound\n",
    "    \"\"\"\n",
    "    assert alternative in ['lower', 'upper']\n",
    "    inx = 0 if alternative == 'lower' else 1\n",
    "    cl_sidak = math.pow(cl, 1/len(strata))  # idk-adjusted confidence level per stratum\n",
    "    cb = sum((hypergeom_conf_interval(\n",
    "                sams[s], found[s], strata[s], cl=cl_sidak, alternative=alternative)[inx] \n",
    "                for s in range(len(strata))))\n",
    "    return cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the empirical coverage of the greedy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test empirical coverage\n",
    "reps = int(10**3)\n",
    "cl = 0.95\n",
    "alternative = 'upper'\n",
    "strata = [5000, 3000, 2000]\n",
    "sams = [75,50,25]\n",
    "good = [100, 100, 500]\n",
    "g = np.sum(good)\n",
    "cover = 0\n",
    "verb = False\n",
    "for i in range(reps):\n",
    "    found = sp.stats.hypergeom.rvs(strata, good, sams)\n",
    "    ub = strat_ci(strata, sams, found, alternative=alternative, cl=cl)\n",
    "    if verb: \n",
    "        print(\"f: {} ub: {} best: {}\".format(found, ub[0], ub[1]))\n",
    "    cover = cover+1 if ub[0] >= g else cover\n",
    "    if i % 100 == 0:\n",
    "        print(i+1, cover, cover/(i+1))\n",
    "cover/reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some numerical tests of the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_strat_test(verbose = False):\n",
    "    strata = [[10, 20, 30, 40], [20, 20, 20]]\n",
    "    sams = [[2, 3, 4, 5], [5, 5, 10]]\n",
    "    found = [[1, 2, 3, 4], [0, 3, 2]]\n",
    "    for i in range(len(strata)):\n",
    "        compl = np.array(sams[i]) - np.array(found[i])\n",
    "        for j in list(range(4, 20)) + [60]:\n",
    "            for alternative in ['lower', 'upper']:\n",
    "                if verbose:\n",
    "                    print(\"i: {} j: {} alternative: {}\".format(i, j, alternative))\n",
    "                p_exact = strat_test_brute(strata[i], sams[i], found[i], j, alternative=alternative)\n",
    "                p_exact_c = strat_test_brute(strata[i], sams[i], compl, j, alternative=alternative)\n",
    "                p_lkp = strat_test(strata[i], sams[i], found[i], j, alternative=alternative)\n",
    "                p_lkp_c = strat_test(strata[i], sams[i], compl, j, alternative=alternative)\n",
    "                np.testing.assert_allclose(p_exact[0], p_lkp[0])\n",
    "                np.testing.assert_allclose(p_exact_c[0], p_lkp_c[0])\n",
    "\n",
    "def test_strat_ci(verbose = False):\n",
    "    strata = [[10, 20], [10, 20, 30, 40], [10, 20, 20, 30]]\n",
    "    sams = [[5, 5], [2, 3, 4, 5], [2, 4, 5, 6]]\n",
    "    found = [[2, 2], [1, 2, 3, 4], [0, 1, 2, 3]]\n",
    "    for i in range(len(strata)):\n",
    "        for alternative in ['lower','upper']:\n",
    "            brute = strat_ci_bisect(strata[i], sams[i], found[i], alternative=alternative)\n",
    "            lkp = strat_ci(strata[i], sams[i], found[i], alternative=alternative)\n",
    "            lkp_s = strat_ci_search(strata[i], sams[i], found[i], alternative=alternative)\n",
    "            if verbose:\n",
    "                print(\"{}-{}\".format(i, alternative))\n",
    "                print(\"i: {} brute: {} lkp: {} lkp_s: {} \\nbest_brute: {} best_lkp: {} best_lkp_s: {}\".format(\n",
    "                   i, brute[0], lkp[0], lkp_s[0], brute[1], lkp[1], lkp_s[1]))\n",
    "            np.testing.assert_allclose(brute[0], lkp[0])\n",
    "            np.testing.assert_allclose(brute[0], lkp_s[0])\n",
    "            np.testing.assert_allclose(brute[1], lkp[1])\n",
    "            np.testing.assert_allclose(brute[1], lkp_s[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
