
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Hypothesis Testing, Combinations of Tests, and Stratified Tests &#8212; Collaborative and Reproducible Data Science</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Collaborative and Reproducible Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../overview.html">
   Statistics 159/259, Spring 2021 Course Summary
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../schedule.html">
   Statistics 159/259: Weekly Plan
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Syllabus
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../syllabus.html">
   Syllabus for Statistics 159/259: Reproducible and Collaborative Statistical Data Science
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Homework Assignments
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw01-background.html">
   1. Homework 1. Stats review and intro to Git
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw02-election-fraud.html">
   2. Homework 2. Election Fraud
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw03-testing.html">
   3. Homework 3: Coding style, docstrings, algorithmic choices, and unit tests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw04-phil-sci-covid.html">
   4. Homework 4: Reproducibility, Philosophy of Science, and COVID-19 Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw05-selection-outliers.html">
   5. Homework 5: Selective inference and outlier rejection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw06-cryptorandom-contrib.html">
   6. Homework 6: Cryptorandom - contributing to an open source project
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw07-permute-contrib.html">
   7. Homework 7: Permute - contributing to an open source project
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Lecture Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   An Idiosyncratic Sample of Applied Statistics
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Lecture Index
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Index/index.html">
   Index of Lectures and Materials
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Notes/tests.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Notes/tests.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hypotheses">
   Hypotheses
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-statistical-hypotheses">
     Types of Statistical Hypotheses
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#test-functions-versus-acceptance-regions">
     Test functions versus acceptance regions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aside-on-notation">
     Aside on notation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#significance-level">
   Significance level
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#power">
   Power
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#type-i-and-type-ii-errors">
   Type I and Type II errors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#type-iii-errors">
   Type III errors
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#randomized-tests">
     Randomized tests
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#p-values">
   <span class="math notranslate nohighlight">
    \(P\)
   </span>
   -values
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-z-test">
   The
   <span class="math notranslate nohighlight">
    \(Z\)
   </span>
   -test
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-of-an-approximate-z-test-with-a-built-in-type-iii-error">
     Example of an approximate
     <span class="math notranslate nohighlight">
      \(Z\)
     </span>
     -test with a built-in Type III error
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#numerical-example">
     Numerical example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#an-exact-conditional-test-based-on-invariance-permutation-methods">
     An exact conditional test based on invariance: permutation methods
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#illustration">
     Illustration
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unconditional-tests-from-conditional-tests">
     Unconditional tests from conditional tests
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#numerical-comparison">
     Numerical comparison
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithmic-considerations">
     Algorithmic considerations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#finding-mathcal-i-and-mathcal-j">
       Finding
       <span class="math notranslate nohighlight">
        \(\mathcal{I}\)
       </span>
       and
       <span class="math notranslate nohighlight">
        \(\mathcal{J}\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Algorithmic considerations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intersection-union-hypotheses">
   Intersection-Union Hypotheses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combinations-of-experiments-and-stratified-experiments">
   Combinations of experiments and stratified experiments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combining-evidence">
   Combining evidence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combining-functions">
   Combining functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fisher-s-combining-function">
     Fisher’s combining function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#liptak-s-combining-function">
     Liptak’s combining function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tippet-s-combining-function">
     Tippet’s combining function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#direct-combination-of-test-statistics">
     Direct combination of test statistics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fisher-s-combining-function-for-independent-p-values">
   Fisher’s combining function for independent
   <span class="math notranslate nohighlight">
    \(P\)
   </span>
   -values
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#when-p-values-have-atoms">
   When
   <span class="math notranslate nohighlight">
    \(P\)
   </span>
   -values have atoms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-p-values-by-simulation">
     Estimating
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     -values by simulation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#accounting-for-simulation-error-in-stratum-wise-p-values">
   Accounting for simulation error in stratum-wise
   <span class="math notranslate nohighlight">
    \(P\)
   </span>
   -values
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-crude-approach-simultaneous-one-sided-upper-confidence-bounds-for-every-lambda-j">
     A crude approach: simultaneous one-sided upper confidence bounds for every
     <span class="math notranslate nohighlight">
      \(\lambda_j\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-sharper-approach-use-a-related-randomized-test">
     A sharper approach: use a related randomized test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dependent-tests">
   Dependent tests
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stratified-permutation-tests">
   Stratified Permutation Tests
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="hypothesis-testing-combinations-of-tests-and-stratified-tests">
<h1>Hypothesis Testing, Combinations of Tests, and Stratified Tests<a class="headerlink" href="#hypothesis-testing-combinations-of-tests-and-stratified-tests" title="Permalink to this headline">¶</a></h1>
<div class="section" id="hypotheses">
<h2>Hypotheses<a class="headerlink" href="#hypotheses" title="Permalink to this headline">¶</a></h2>
<p>A <em>scientific hypothesis</em> is an assertion about the world, for instance “this drug increases longevity in patients with Alzheimer’s Disease,” “the outcome of the 2020 presidential election would have been different, but for fraud,” “to 6 significant digits, the gravitational constant is <span class="math notranslate nohighlight">\(6.67430e-11 Nm^2kg^{-2}\)</span>,” “the Moderna COVID-19 vaccine is at least 90% effective for at least 6 months for all ages,” “students give female instructorss lower ratings than equally effective male instructors,” “the universe is expanding at an increasing rate,” “there is a black hole at the center of the universe,” or “female faculty job applicants are interrupted more frequently during their job talks than male applicants are.”</p>
<p>A <em>statistical hypothesis</em> is an assertion about the probability distribution <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> of data <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Generally, statistical hypotheses are interesting only insofar as they help us evaluate scientific
hypotheses.
Whenever a statistical hypothesis is being tested, pay attention to the connection between that hypothesis and the scientific hypothesis it purports to represent.</p>
<div class="section" id="types-of-statistical-hypotheses">
<h3>Types of Statistical Hypotheses<a class="headerlink" href="#types-of-statistical-hypotheses" title="Permalink to this headline">¶</a></h3>
<p>A <em>simple</em> statistical hypothesis is an assertion that completely specifies the probability distribution of the data, e.g., <span class="math notranslate nohighlight">\(\mathbb{P} = \mathbb{P}_0\)</span> for some distribution <span class="math notranslate nohighlight">\(\mathbb{P}_0\)</span>. Here are some simple hypotheses:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X \sim N(0,1)\)</span> The data have a standard normal distribution.</p></li>
<li><p><span class="math notranslate nohighlight">\(X \sim U[0,1]\)</span> The data have a standard uniform distribution.</p></li>
<li><p><span class="math notranslate nohighlight">\(X \sim N(\mu, \Sigma)\)</span> for a given <span class="math notranslate nohighlight">\(\mu \in \Re^n\)</span> and positive semi-definite <span class="math notranslate nohighlight">\(n \times n\)</span> matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>. The data are jointly normally distributed with mean <span class="math notranslate nohighlight">\(\mu\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\{X_j\}\; \mbox{IID} \; U[0, 17]\)</span>. The components of the data are independent and identically distributed with a uniform distribution on <span class="math notranslate nohighlight">\([0, 17]\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(X \sim \mbox{Binom}(n, p)\)</span> for given values of <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(p\)</span>. The data have a binomial distribution with parameters <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(p\)</span>.</p></li>
</ul>
<p>A <em>composite</em> hypothesis is an assertion that does not completely specify the distribution; it only says the
distribution is in some specified set of distributions.
E.g., <span class="math notranslate nohighlight">\(\mathbb{P} \in \mathcal{P}_0\)</span> for some set
<span class="math notranslate nohighlight">\(\mathcal{P}_0\)</span> of distributions.
Here are some composite null hypotheses:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X \sim N(\mu,1)\)</span>, <span class="math notranslate nohighlight">\(\mu \in [0, 7]\)</span> The data have a normal distribution with variance 1 and mean between 0 and 7.</p></li>
<li><p><span class="math notranslate nohighlight">\(X \sim \mbox{Binom}(n, p)\)</span>, <span class="math notranslate nohighlight">\(p &gt; 1/2\)</span> (<span class="math notranslate nohighlight">\(n\)</span> given). The data have a binomial distribution with known parameter <span class="math notranslate nohighlight">\(n\)</span> and unknown parameter <span class="math notranslate nohighlight">\(p &gt; 1/2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}X=0\)</span> The expected value of the data is zero.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}X \le 1/2\)</span> The expected value of the data is at most 1/2.</p></li>
<li><p>The distribution <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> of <span class="math notranslate nohighlight">\(X\)</span> has at most 3 modes.</p></li>
<li><p><span class="math notranslate nohighlight">\(\{X_j\}_{j=1}^n\)</span> are <em>exchangeable</em> (A collection of random variables is <em>exchangeable</em> if their joint distribution is invariant under permuting their labels. If a collection of random variables is IID, it is also exchangeable.)</p></li>
<li><p>The distribution of <span class="math notranslate nohighlight">\(X\)</span> is spherically symmetric</p></li>
<li><p>The <span class="math notranslate nohighlight">\(q\)</span>th quantile of the probability distribution of the data is <span class="math notranslate nohighlight">\(x\)</span></p></li>
</ul>
<p>A common statistical question is to use data to assess whether a statistical hypothesis is true: to <em>test</em> the hypothesis.
There is always more than one explanation for any particular set of data, so in general, it is not
possible to prove that a (simple) statistical hypothesis is <em>true</em>.
But the data might provide evidence that a hypothesis is <em>false</em>.
This is tied to Popper’s notion that only hypotheses that can be falsified by evidence (potentially shown to be false) are scientific hypotheses.</p>
<p>We speak of “rejecting” a hypothesis when there is sufficiently strong evidence that the hypothesis is false.
But if data do not cast doubt on a hypothesis, that is not evidence that the hypothesis is true–it is only absense of evidence that the hypothesis is false.
Additional evidence might cast doubt on the hypothesis.
When we speak (informally) of “accepting” a hypothesis, it means only that we did not reject it on the basis of a particular set of data, not that we
have affirmative evidence that it is true.</p>
<p>To test a statistical hypothesis, one specifies a set <span class="math notranslate nohighlight">\(A\)</span> of possible data values (before examining the data).
If the data fall inside that set, i.e., if <span class="math notranslate nohighlight">\(X \in A\)</span>, the null hypothesis is <em>not rejected</em>; otherwise, the null hypothesis is <em>rejected</em>.
Each (measurable) set <span class="math notranslate nohighlight">\(A\)</span> implicitly defines a hypothesis test.
The set <span class="math notranslate nohighlight">\(A\)</span> is called the <em>acceptance region</em> for the test.</p>
<p>Sometimes the set <span class="math notranslate nohighlight">\(A\)</span> is defined explicitly, but
more often it is defined implicitly in terms of a <em>test statistic</em>, a function of the data that does not depend on any unknown parameters.</p>
<p>For instance an acceptance region for testing the hypothesis that <span class="math notranslate nohighlight">\(\{X_j\}_{j=1}^n\)</span> are IID <span class="math notranslate nohighlight">\(N(0,1)\)</span> might be</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{eqnarray*}
A \equiv \left \{x = \{x_1, \ldots, x_n\}: \frac{1}{n} \sum_{j=1}^n x_j \le \frac{c}{\sqrt{n}} \right \}.
\end{eqnarray*}\]</div>
<p>In this example, the test statistic is the sample mean, and the region <span class="math notranslate nohighlight">\(A\)</span> is the set of all data for which the
sample mean does not exceed <span class="math notranslate nohighlight">\(c/\sqrt{n}\)</span>.</p>
</div>
<div class="section" id="test-functions-versus-acceptance-regions">
<h3>Test functions versus acceptance regions<a class="headerlink" href="#test-functions-versus-acceptance-regions" title="Permalink to this headline">¶</a></h3>
<p>Instead of working with a set <span class="math notranslate nohighlight">\(A\)</span>, we can work with the indicator function of the set <span class="math notranslate nohighlight">\(A\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{eqnarray*}
1_A &amp;:&amp; \mathcal{X} \rightarrow \{0, 1\} \\
       &amp;&amp; x  \mapsto \left \{ \begin{array}{ll}
                    1, &amp; x \in A \\
                    0, &amp; \mbox{ otherwise. } 
                    \end{array} \right .
\end{eqnarray*}\]</div>
<p>Then we reject the null hypothesis if <span class="math notranslate nohighlight">\(1_A(X) = 0\)</span> and do not reject it if <span class="math notranslate nohighlight">\(1_A(X) = 1\)</span>.</p>
<p>This can be generalized by using a <em>test function</em> or <em>critical function</em> <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span> that take
values in <span class="math notranslate nohighlight">\([0,1]\)</span> rather than just <span class="math notranslate nohighlight">\(\{0, 1\}\)</span>, as described below.
In that case, when <span class="math notranslate nohighlight">\(X=x\)</span>, the test rejects the null with probability <span class="math notranslate nohighlight">\(\phi(x)\)</span>, i.e.,
if <span class="math notranslate nohighlight">\(\phi(X) = 0\)</span>, the test certainly does not reject the null;
if <span class="math notranslate nohighlight">\(\phi(X) = 1\)</span>, the test certainly rejects the null; and if <span class="math notranslate nohighlight">\(\phi(X) = q \in (0, 1)\)</span>,
it rejects the null with probability <span class="math notranslate nohighlight">\(q\)</span>.
This is called a <em>randomized test</em>, discussed further below.</p>
</div>
<div class="section" id="aside-on-notation">
<h3>Aside on notation<a class="headerlink" href="#aside-on-notation" title="Permalink to this headline">¶</a></h3>
<p>The following all mean the same thing, namely, the probability that the random variable <span class="math notranslate nohighlight">\(X\)</span> takes a value in the set <span class="math notranslate nohighlight">\(A\)</span> if the distribution of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\mathbb{P}_0\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\Pr \{X \in A || X \sim \mathbb{P}_0 \},
\end{equation*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\Pr_{X \sim \mathbb{P}_0} \{X \in A\},
\end{equation*}\]</div>
<p>and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{P}_0 \{X \in A\}.
\end{equation*}\]</div>
<p>For expectations, we use similar notation:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E} (X || X \sim \mathbb{P}_0)
\end{equation*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}_{X \sim \mathbb{P}_0} X ,
\end{equation*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}_{\mathbb{P}_0} X,
\end{equation*}\]</div>
<p>and sometimes</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{E}_{0} X.
\end{equation*}\]</div>
</div>
</div>
<div class="section" id="significance-level">
<h2>Significance level<a class="headerlink" href="#significance-level" title="Permalink to this headline">¶</a></h2>
<p>The <em>significance level</em> of the test <span class="math notranslate nohighlight">\(A\)</span> of the simple null hypothesis <span class="math notranslate nohighlight">\(X \sim \mathbb{P}_0\)</span> is the probability that the test rejects the null hypothesis when the null hypothesis is true:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   \alpha \equiv \mathbb{P}_0 \{X \notin A \}.
\end{equation*}\]</div>
<p>Because the expected value of an indicator function is the probability of the set, this can be written</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   \alpha \equiv 1-\mathbb{E}_{\mathbb{P}_0} 1_A(X).
\end{equation*}\]</div>
<p>For a test based on the test function <span class="math notranslate nohighlight">\(\phi: \mathcal{X} \rightarrow [0,1]\)</span>,
the significance level of the simple null hypothesis <span class="math notranslate nohighlight">\(X \sim \mathbb{P}_0\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   \alpha \equiv 1-\mathbb{E}_{\mathbb{P}_0} \phi(X).
\end{equation*}\]</div>
<p>Why is this expression the probability of rejecting the null when the null is true?
The test rejects when <span class="math notranslate nohighlight">\(U \ge \phi(X)\)</span>.
By the law of total expectation (see <a class="reference internal" href="math-inequalities.html"><span class="doc std std-doc">Inequalities and Identities</span></a>) and using the fact that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(U\)</span> are independent,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{P}_{0,U} \{ U \ge \phi(X)\} = 
\mathbb{E}_{\mathbb{P}_0, U} 1_{U \ge \phi(X)} = 
\mathbb{E}_{\mathbb{P}_0} \mathbb{E}_U (1_{U \ge \phi(X)} | \phi(X)) =
\mathbb{E}_{\mathbb{P}_0} (1-\phi(X)) = 1 - \mathbb{E}_{\mathbb{P}_0} \phi(X).
\end{equation*}\]</div>
<p>The <em>significance level</em> of the test <span class="math notranslate nohighlight">\(A\)</span> of the composite null hypothesis <span class="math notranslate nohighlight">\(H_0: X \sim \mathbb{P} \in \mathcal{P}_0\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   \alpha \equiv \sup_{\mathbb{P_0} \in \mathcal{P}_0} \mathbb{P}_0 \{X \notin A \}.
\end{equation*}\]</div>
<p>That is, it is the largest probability that that the test rejects the null hypothesis when the null hypothesis is true.
For a test based on a a test function <span class="math notranslate nohighlight">\(\phi: \mathcal{X} \rightarrow [0,1]\)</span>,
the significance level of the simple null hypothesis <span class="math notranslate nohighlight">\(X \sim \mathbb{P}_0\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   \alpha \equiv 1- \inf_{\mathbb{P_0} \in \mathcal{P}_0} \mathbb{E}_{\mathbb{P}_0} \phi(X).
\end{equation*}\]</div>
<p>When <span class="math notranslate nohighlight">\(\mathcal{P}_0\)</span> contains only one distribution, the two definitions of significance level coincide.</p>
</div>
<div class="section" id="power">
<h2>Power<a class="headerlink" href="#power" title="Permalink to this headline">¶</a></h2>
<p>The <em>power</em> of the test <span class="math notranslate nohighlight">\(A\)</span> against the simple hypothesis <span class="math notranslate nohighlight">\(H_1: X \sim \mathbb{P}_1\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   1-\beta \equiv \mathbb{P}_1 \{X \notin A \}
\end{equation*}\]</div>
<p>That is, the power is the chance that the test rejects the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> when the alternative hypothesis <span class="math notranslate nohighlight">\(H_1\)</span> is true.</p>
<p>For a test based on a test function <span class="math notranslate nohighlight">\(\phi\)</span>, the power is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   1-\beta \equiv 1 - \mathbb{E}_{\mathbb{P}_1} \phi(X).
\end{equation*}\]</div>
<p>The <em>power</em> of the test <span class="math notranslate nohighlight">\(A\)</span> against the composite alternative hypothesis <span class="math notranslate nohighlight">\(H_1: X \sim \mathbb{P} \in \mathcal{P}_1\)</span> is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   1-\beta \equiv \inf_{\mathbb{P_1} \in \mathcal{P}_1} \mathbb{P}_1 \{X \notin A \}.
\end{equation*}\]</div>
<p>That is, it is the smallest probability that that the test rejects the null hypothesis when the alternative hypothesis is true.</p>
<p>For a test based on a test function <span class="math notranslate nohighlight">\(\phi\)</span>, the power is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
   1-\beta \equiv 1 - \sup_{\mathbb{P_1} \in \mathcal{P}_1}\mathbb{E}_{\mathbb{P}_1} \phi(X).
\end{equation*}\]</div>
<p>When the alternative <span class="math notranslate nohighlight">\(\mathcal{P}_1\)</span> contains only one distribution, the two definitions coincide.</p>
<p>Power and significance level are the (extremal) probability of the same event–namely, rejecting the null hypothesis–but computed under different assumptions.
The power is computed under the assumption that the alternative hypothesis is true; the significance level is computed under the assumption that the null hypothesis is true.</p>
<p>Seminal work by Jerzy Neyman (the founder of Berkeley’s Department of Statistics) and Egon Pearson showed how to find the most powerful test of a simple null hypothesis against a simple alternative hypothesis among all tests with a given significance level. They showed that the most powerful test had an acceptance region characterized by the likelihood ratio; see below.</p>
</div>
<div class="section" id="type-i-and-type-ii-errors">
<h2>Type I and Type II errors<a class="headerlink" href="#type-i-and-type-ii-errors" title="Permalink to this headline">¶</a></h2>
<p>A type I error occurs when a test rejects the null hypothesis but the null hypothesis is true.
The chance of a type I error is the significance level of the test.</p>
<p>A type II error occurs when a test does not reject the null hypothesis the null hypothesis is false.
The chance of a type II error when a particular alternative is true is 100% minus the power of the test
against that alternative, i.e., <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
</div>
<div class="section" id="type-iii-errors">
<h2>Type III errors<a class="headerlink" href="#type-iii-errors" title="Permalink to this headline">¶</a></h2>
<p>There are a number of informal definitions of type III errors.</p>
<p>One is that a Type III error occurs when a test correctly rejects the null hypothesis “for the wrong reason.” For instance, suppose that we test the null hypothesis <span class="math notranslate nohighlight">\(X \sim N(0, 1)\)</span>
at significance level <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> by defining <span class="math notranslate nohighlight">\(A \equiv \{x: |x| \le 1.96\}\)</span>.
Suppose that in reality <span class="math notranslate nohighlight">\(X \sim N(-1, 1)\)</span> and we observe <span class="math notranslate nohighlight">\(X = 2\)</span>.
Then we would <em>correctly</em> reject <span class="math notranslate nohighlight">\(H_0\)</span>, but because <span class="math notranslate nohighlight">\(X\)</span> was “too big,” while it was much more likely to be “too small” since in reality the distribution of <span class="math notranslate nohighlight">\(X\)</span> has mean <span class="math notranslate nohighlight">\(-1\)</span> instead of <span class="math notranslate nohighlight">\(0\)</span>. (If, after rejecting <span class="math notranslate nohighlight">\(H_0\)</span>, we concluded that <span class="math notranslate nohighlight">\(\mathbb{E}X &gt; 0\)</span>, that directional conclusion would be wrong.)</p>
<p>Similarly, suppose we want to test whether the mean of some finite population is zero from a simple random sample <span class="math notranslate nohighlight">\(X = (X_1, \ldots, X_n)\)</span>.
We set <span class="math notranslate nohighlight">\(A = \{x \in \Re^n : |\bar{x}| \le z_\alpha/\sqrt{n} \}\)</span>, where <span class="math notranslate nohighlight">\(z_\alpha\)</span> is the <span class="math notranslate nohighlight">\(1-\alpha\)</span> percentage
point of the standard normal distribution.
This is a significance level <span class="math notranslate nohighlight">\(\alpha\)</span> test of the null hypothesis that <span class="math notranslate nohighlight">\(X \sim N(0, 1)\)</span> from an IID sample, not
of the null hypothesis that the population mean is zero from a random sample without replacement.
Nonetheless, this incorrect test might correctly reject the null hypothesis.</p>
<p>Another informal definition is that a Type III error occurs when one gets the right answer to the wrong question.
One example is testing the hypothesis <span class="math notranslate nohighlight">\(\mathbb{E}X=0\)</span> by testing the hypothesis <span class="math notranslate nohighlight">\(X \sim N(0, \sigma^2)\)</span> when
there is no reason to think that <span class="math notranslate nohighlight">\(X\)</span> has a normal distribution. Even if the test is performed correctly,
it is testing the wrong null.</p>
<p>In my experience, the most frequent and pernicious Type III errors involve testing a <em>statistical hypothesis</em> that has little or nothing to do with the <em>scientific hypothesis</em> (aside, perhaps, from having some words in common).
<em>Many</em> hypothesis tests in applications have this sort of Type III error baked in.
Indeed, most analyses of clinical trial data I’ve seen test a statistical null hypothesis that involves selecting subjects at random from a superpopulation–which did not occur–rather than test a statistical hypothesis based on the randomization of subjects into treatment or control–which actually did occur.</p>
<p>It is quite common to base <span class="math notranslate nohighlight">\(\mathbb{P}_0\)</span> on a statistical model that has no connection to the scientific hypothesis and how the data were generated, then to claim that rejecting <span class="math notranslate nohighlight">\(\mathbb{P}_0\)</span> says something about the world.
That logic is flawed.</p>
<div class="section" id="randomized-tests">
<h3>Randomized tests<a class="headerlink" href="#randomized-tests" title="Permalink to this headline">¶</a></h3>
<p>Sometimes it is useful for a hypothesis test to depend not only on the data but also on “auxilliary” randomness, typically a <span class="math notranslate nohighlight">\(U[0, 1]\)</span> variable <span class="math notranslate nohighlight">\(U\)</span> that is independent of the data <span class="math notranslate nohighlight">\(X\)</span>.
Then the acceptance region <span class="math notranslate nohighlight">\(A\)</span> is a subset of the Cartesian product of the data space and <span class="math notranslate nohighlight">\([0, 1]\)</span>.
The null hypothesis is not rejected if <span class="math notranslate nohighlight">\((X, U) \in A\)</span>; otherwise it is rejected.</p>
<p>Randomized tests arise in a number of situations, including tests involving discrete distributions.
Randomized tests are also useful for proving theorems about tests.
For instance, <em>the Neyman-Pearson lemma</em> shows that the most powerful test of a simple null hypothesis <span class="math notranslate nohighlight">\(X \sim \mathbb{P}_0\)</span> against a simple alternative hypothesis <span class="math notranslate nohighlight">\(X \sim \mathbb{P}_1\)</span> at significance level <span class="math notranslate nohighlight">\(\alpha\)</span> is
a randomized test of the form:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
A = \{ (x, u): \mathcal{L}_0(x)/\mathcal{L}_1(x) &gt; c \} \cup \{ (x, u): (\mathcal{L}_0(x)/\mathcal{L}_1(x) = c) \mbox{ and }
  (u \le d) \}
\end{equation*}\]</div>
<p>for suitable choices of <span class="math notranslate nohighlight">\(c\)</span> and <span class="math notranslate nohighlight">\(d\)</span>,
where <span class="math notranslate nohighlight">\(\mathcal{L}_j(x)\)</span> is the likelihood of hypothesis <span class="math notranslate nohighlight">\(j\)</span> for data <span class="math notranslate nohighlight">\(x\)</span>.
(This assumes <span class="math notranslate nohighlight">\(\mathbb{P}_0\)</span> and <span class="math notranslate nohighlight">\(\mathbb{P}_1\)</span> are <em>absolutely continuous</em> with respect to each other.)
For a definition of the likelihood function, see <a class="reference internal" href="bayes.html"><span class="doc std std-doc">Bayesian and Frequentist Estimation and Inference</span></a>.</p>
<p>A test based on a test function <span class="math notranslate nohighlight">\(\phi: \mathcal{X} \rightarrow [0, 1]\)</span> is implicitly
a randomized test if <span class="math notranslate nohighlight">\(\phi\)</span> can have a value strictly between 0 and 1.
Then, we also use an auxilliary randomness, <span class="math notranslate nohighlight">\(U \sim U[0,1]\)</span> and reject the null
if <span class="math notranslate nohighlight">\(U \ge \phi(X)\)</span>.
This is completely equivalent to the description above, but instead of thinking
in terms of a <em>set</em> <span class="math notranslate nohighlight">\(A \subset \mathcal{X} \times [0, 1]\)</span>, we think of a <em>function</em>
<span class="math notranslate nohighlight">\(\phi: \mathcal{X} \rightarrow [0, 1]\)</span>.
In both formulations, we use <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(U\)</span> to decide whether to reject <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
</div>
</div>
<div class="section" id="p-values">
<h2><span class="math notranslate nohighlight">\(P\)</span>-values<a class="headerlink" href="#p-values" title="Permalink to this headline">¶</a></h2>
<p>Here are two approaches to defining <span class="math notranslate nohighlight">\(P\)</span>-values, in terms of families of hypothesis tests and in terms of a test statistic.</p>
<p>Family of tests:</p>
<ul class="simple">
<li><p>Suppose you have a set of nested (monotone) hypothesis tests:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\{A_\alpha : \alpha \in (0, 1] \}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}_0 \{ X \notin A_\alpha \} \le \alpha\)</span> (or more generally, <span class="math notranslate nohighlight">\(\mathbb{P} \{ X \notin A_\alpha \} \le \alpha, \; \forall \mathbb{P} \in \mathcal{P}_0\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(A_\alpha \subset A_\beta\)</span> if <span class="math notranslate nohighlight">\(\beta &lt; \alpha\)</span> (Can always re-define <span class="math notranslate nohighlight">\(A_\alpha \leftarrow \cup_{\beta \ge \alpha } A_\beta\)</span>)</p></li>
</ul>
</li>
<li><p>If we observe <span class="math notranslate nohighlight">\(X = x\)</span>, the <span class="math notranslate nohighlight">\(P\)</span>-value is <span class="math notranslate nohighlight">\(\sup \{ \alpha: x \in A_\alpha \}\)</span>.</p></li>
</ul>
<p>Test statistic definition of a <span class="math notranslate nohighlight">\(P\)</span>-value:</p>
<p>If <span class="math notranslate nohighlight">\(P = P(X)\)</span> is a random variable whose probability distribution is <em>dominated</em> by the uniform distribution on <span class="math notranslate nohighlight">\([0, 1]\)</span> when
the null hypothesis is true, then <span class="math notranslate nohighlight">\(P\)</span> is a <span class="math notranslate nohighlight">\(P\)</span>-value.</p>
<p>That is, <span class="math notranslate nohighlight">\(P\)</span> is a <span class="math notranslate nohighlight">\(P\)</span>-value if</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  
 \mathbb{P}_0 \{ P(X) \le x \} \le x \;\; \forall x \in [0, 1].
\end{equation*}\]</div>
<p>You may hear someone say that a <span class="math notranslate nohighlight">\(P\)</span>-value is the chance that a test statistic is “as extreme or more extreme than
observed.”
That is not really a precise mathematical definition; moreover, not every <span class="math notranslate nohighlight">\(P\)</span>-value can be expressed naturally
in that form and not everything of that form is a <span class="math notranslate nohighlight">\(P\)</span>-value.</p>
<p>For randomized tests expressed in terms of acceptance regions, <span class="math notranslate nohighlight">\(P(X, U)\)</span> is a <span class="math notranslate nohighlight">\(P\)</span>-value if</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  
 \mathbb{P}_0 \{ P(X,U) \le x \} \le x \;\; \forall x \in [0, 1].
\end{equation*}\]</div>
</div>
<div class="section" id="the-z-test">
<h2>The <span class="math notranslate nohighlight">\(Z\)</span>-test<a class="headerlink" href="#the-z-test" title="Permalink to this headline">¶</a></h2>
<p>A <span class="math notranslate nohighlight">\(Z\)</span>-test tests the hypothesis that some function of the data has a standard Normal distribution, that is,
the hypothesis that for some given test statistic <span class="math notranslate nohighlight">\(f(x)\)</span>, <span class="math notranslate nohighlight">\(f(X) \sim N(0,1)\)</span>.</p>
<p>What kinds of things have a standard normal distribution?</p>
<ul class="simple">
<li><p>A random draw from a population that has a standard normal distribution.</p></li>
<li><p>The sample mean of <span class="math notranslate nohighlight">\(n\)</span> IID draws from a population that has a standard normal distribution, after multiplying the sample mean by <span class="math notranslate nohighlight">\(\sqrt{n}\)</span>.</p></li>
<li><p>The sample mean of <span class="math notranslate nohighlight">\(n\)</span> IID draws from a population that has a <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span> distribution, after subtracting <span class="math notranslate nohighlight">\(\mu\)</span> and multiplying by <span class="math notranslate nohighlight">\(\sqrt{n}/\sigma\)</span>.</p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(Z\)</span>-test is often used as an <em>approximate</em> test rather than an <em>exact</em> test.
An <em>approximate</em> test is one that has approximately its nominal significance level.<br />
What kinds of things have distributions that are approximately standard normal?</p>
<ul class="simple">
<li><p>Things that have a Binomial<span class="math notranslate nohighlight">\((n, p)\)</span> distribution, after subtracting <span class="math notranslate nohighlight">\(np\)</span> and dividing by <span class="math notranslate nohighlight">\(\sqrt{np(1-p)}\)</span>, provided <span class="math notranslate nohighlight">\(np\)</span> and <span class="math notranslate nohighlight">\(n(1-p)\)</span> are not small. For instance, consider tossing a fair coin 100 times, independently. The number <span class="math notranslate nohighlight">\(X\)</span> of heads has a Binomial<span class="math notranslate nohighlight">\((100, 1/2)\)</span> distribution. The distribution of <span class="math notranslate nohighlight">\((X-50)/5\)</span> is approximately a standard normal. (<span class="math notranslate nohighlight">\(np(1-p) = 100\times 1/2 \times 1/2\)</span>, so <span class="math notranslate nohighlight">\(\sqrt{np(1-p)} = 5\)</span>.)</p></li>
<li><p>The sample mean of <span class="math notranslate nohighlight">\(n\)</span> IID draws with replacement from a finite population of numbers, after subtracting the population mean <span class="math notranslate nohighlight">\(\mu\)</span> and dividing by <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>, where <span class="math notranslate nohighlight">\(\sigma\)</span> is the population standard deviation, provided <span class="math notranslate nohighlight">\(n\)</span> is sufficiently large and the population distribution is sufficiently “bell shaped.” If nothing is known about the population, it is in general impossible to know how accurate the normal approximation to the sample mean is.
(The Binomial distribution is a special case where the population values are known to be 0 and 1.)</p></li>
</ul>
<p>When the test statistic is only approximately normally distributed under the null hypothesis, the accuracy
of the approximation matters–but is rarely addressed.</p>
<div class="section" id="example-of-an-approximate-z-test-with-a-built-in-type-iii-error">
<h3>Example of an approximate <span class="math notranslate nohighlight">\(Z\)</span>-test with a built-in Type III error<a class="headerlink" href="#example-of-an-approximate-z-test-with-a-built-in-type-iii-error" title="Permalink to this headline">¶</a></h3>
<p>There are two binary populations, <span class="math notranslate nohighlight">\(\{x_j\}_{j=1}^n\)</span> and <span class="math notranslate nohighlight">\(\{y_j\}_{j=1}^m\)</span>. (A population is <em>binary</em> if the only possible values in the population are 0 and 1.)
We are interested in whether the populations are “surprisingly different.”
The null hypothesis is that the two populations were formed by selecting <span class="math notranslate nohighlight">\(n\)</span> items at random from the overall group of <span class="math notranslate nohighlight">\(n+m\)</span> items to form the first population, with the remaining <span class="math notranslate nohighlight">\(m\)</span> items comprising the second population.</p>
<p>Let <span class="math notranslate nohighlight">\(\bar{x} \equiv \frac{1}{n} \sum_j x_j\)</span> and <span class="math notranslate nohighlight">\(\bar{y} \equiv \frac{1}{m} y_j\)</span>.
If <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> were independent random samples with replacement from the same binary “super-population” that had a fraction <span class="math notranslate nohighlight">\(p\)</span> of
1s and a fraction <span class="math notranslate nohighlight">\((1-p)\)</span> of zeros,
<span class="math notranslate nohighlight">\(n\bar{x}\)</span> would be a random variable with a binomial distribution with parameters <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(p\)</span>, and <span class="math notranslate nohighlight">\(m\bar{y}\)</span> would
be a random variable with
a binomial distribution with parameters <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(p\)</span>, and would be independent of <span class="math notranslate nohighlight">\(n\bar{x}\)</span>.</p>
<p>The expected value of <span class="math notranslate nohighlight">\(\bar{x}\)</span> would be <span class="math notranslate nohighlight">\(p\)</span> and its variance would be <span class="math notranslate nohighlight">\(p(1-p)/n\)</span>;
the expected value of <span class="math notranslate nohighlight">\(\bar{y}\)</span> would be <span class="math notranslate nohighlight">\(p\)</span> and its variance would be <span class="math notranslate nohighlight">\(p(1-p)/m\)</span>.
The expected value of <span class="math notranslate nohighlight">\(\bar{x} - \bar{y}\)</span> would be 0, and its variance would be <span class="math notranslate nohighlight">\(p(1-p)(1/n+1/m)\)</span>.
Moreover, if <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> were random samples with replacement from the same binary super-population,
<span class="math notranslate nohighlight">\(\hat{p} = (n\bar{x} + m\bar{y})/(n+m)\)</span> would be an unbiased estimate of <span class="math notranslate nohighlight">\(p\)</span>,
and for sufficiently large <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(n\)</span>, the distribution of</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
f(x,y) \equiv \frac{\bar{x} - \bar{y}}{\sqrt{\hat{p}(1-\hat{p})(1/n+1/m)}}
\end{equation*}\]</div>
<p>would be approximately <span class="math notranslate nohighlight">\(N(0,1)\)</span>.
(The accuracy of the approximation would depend on <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(m\)</span>, and <span class="math notranslate nohighlight">\(p\)</span>.)</p>
<p>If we define the acceptance region <span class="math notranslate nohighlight">\(A \equiv \{ x, y: |f(x, y)| \le z_{\alpha/2}\)</span>, where <span class="math notranslate nohighlight">\(z_\alpha\)</span> is the <span class="math notranslate nohighlight">\(1-\alpha\)</span>
percentage point of the standard normal distribution, we get a (two-sided) <span class="math notranslate nohighlight">\(Z\)</span> test at nominal significance
level <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>But this is an approximate test of a different hypothesis: an approximate answer to a different question,
a Type III error.
The hypothesis test has little to do with the original hypothesis.
The true significance level of the test for the original null hypothesis that the two groups are a random partition
of the <span class="math notranslate nohighlight">\(n+m\)</span> items could be quite different from <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</div>
<div class="section" id="numerical-example">
<h3>Numerical example<a class="headerlink" href="#numerical-example" title="Permalink to this headline">¶</a></h3>
<p>There are two binary populations, <span class="math notranslate nohighlight">\(\{x_j\}_{j=1}^N\)</span> and <span class="math notranslate nohighlight">\(\{ y_j\}_{j=1}^M\)</span>.
Let <span class="math notranslate nohighlight">\(p_x\)</span> denote the mean of the first population and <span class="math notranslate nohighlight">\(p_y\)</span> the mean of the second.
We wish to know whether <span class="math notranslate nohighlight">\(p_x = p_y\)</span>.</p>
<p>A random sample with replacement of size <span class="math notranslate nohighlight">\(n = 100\)</span> will be drawn from the first list and a random sample with replacement of size <span class="math notranslate nohighlight">\(m = 300\)</span> will be drawn from the second list, independent of the other sample.
Let <span class="math notranslate nohighlight">\(X\)</span> denote the sum of the numbers in the sample from the first population and let <span class="math notranslate nohighlight">\(Y\)</span> denote the sum of the numbers in the sample from the second population.
Then <span class="math notranslate nohighlight">\(X \sim \mbox{Binom}(n, p_x)\)</span>, <span class="math notranslate nohighlight">\(Y \sim \mbox{Binom}(m, p_y)\)</span>, and <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.</p>
<p>(An equivalent problem is that there are two coins, one with chance <span class="math notranslate nohighlight">\(p_x\)</span> of landing heads and one with chance <span class="math notranslate nohighlight">\(p_y\)</span> of landing heads. The first coin is tossed <span class="math notranslate nohighlight">\(n\)</span> times and the second coin is tossed <span class="math notranslate nohighlight">\(m\)</span> times.
All <span class="math notranslate nohighlight">\(m+n\)</span> tosses are independent. Let <span class="math notranslate nohighlight">\(X\)</span> be the number of times the first coin lands heads and let <span class="math notranslate nohighlight">\(Y\)</span>
denote the number of times the second coin lands heads.)</p>
<p>If <span class="math notranslate nohighlight">\(p_x = p_y = p\)</span>, <span class="math notranslate nohighlight">\(X+Y \sim \mbox{Binom}(n+m, p)\)</span> and <span class="math notranslate nohighlight">\(\hat{p} \equiv (X+Y)/(n+m)\)</span> is an unbiased estimate of <span class="math notranslate nohighlight">\(p\)</span> with standard deviation <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/(m+n)} \le 1/(2 \sqrt{m+n}) = 0.025\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\bar{X} \equiv X/n\)</span> and <span class="math notranslate nohighlight">\(\bar{Y} \equiv Y/m\)</span>.
If <span class="math notranslate nohighlight">\(p_x = p_y = p\)</span> and <span class="math notranslate nohighlight">\(p\)</span> is not close to 0 or 1, then the distribution of <span class="math notranslate nohighlight">\(\bar{X}\)</span> is approximately Gaussian
with mean <span class="math notranslate nohighlight">\(p\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/n}\)</span> and the distribution of <span class="math notranslate nohighlight">\(\bar{Y}\)</span> is approximately Gaussian
with mean <span class="math notranslate nohighlight">\(p\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sqrt{p(1-p)/m}\)</span>, and <span class="math notranslate nohighlight">\(\bar{X}\)</span> and <span class="math notranslate nohighlight">\(\bar{Y}\)</span> are independent.</p>
<p>It follows that the distribution of <span class="math notranslate nohighlight">\(\bar{X}-\bar{Y}\)</span> is approximately Gaussian with mean <span class="math notranslate nohighlight">\(p-p = 0\)</span> and standard deviation</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{eqnarray*}
\sigma \equiv \sqrt{p(1-p)/n + p(1-p)/m} = \sqrt{p(1-p)}\sqrt{1/n + 1/m} \le 0.0577.
\end{eqnarray*}\]</div>
<p>Thus the distribution of</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\frac{\bar{X} - \bar{Y}}{\sqrt{p(1-p)}\sqrt{1/n + 1/m}}
\end{equation*}\]</div>
<p>is approximately the standard normal.
If we use the “plug-in” estimator of <span class="math notranslate nohighlight">\(p\)</span>, <span class="math notranslate nohighlight">\(\hat{p}\)</span>, the distribution is also approximately normal (but the accuracy of the approximation is, in general, worse).
Thus the distribution of</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
Z \equiv \frac{\bar{X} - \bar{Y}}{\sqrt{\hat{p}(1-\hat{p})}\sqrt{1/n + 1/m}}
\end{equation*}\]</div>
<p>is approximately <span class="math notranslate nohighlight">\(N(0,1)\)</span> if <span class="math notranslate nohighlight">\(p_x = p_y\)</span>.</p>
<p>Note that if <span class="math notranslate nohighlight">\(p\)</span> is close to 0 or to 1, <span class="math notranslate nohighlight">\(\hat{p}\)</span> could end up equal to 0 or 1, in which case the denominator will
vanish (clearly underestimating the standard deviation of <span class="math notranslate nohighlight">\(\bar{X}-\bar{Y}\)</span>).</p>
<p>To perform a <span class="math notranslate nohighlight">\(Z\)</span> test, we pretend that the distribution of <span class="math notranslate nohighlight">\(Z\)</span> is exactly the standard normal.
If <span class="math notranslate nohighlight">\(p_x &gt; p_y\)</span>, we expect <span class="math notranslate nohighlight">\(\bar{X} &gt; \bar{Y}\)</span> and hence <span class="math notranslate nohighlight">\(Z &gt; 0\)</span>. If <span class="math notranslate nohighlight">\(p_x &lt; p_y\)</span>, we expect <span class="math notranslate nohighlight">\(\bar{X} &lt; \bar{Y}\)</span> and hence <span class="math notranslate nohighlight">\(Z &lt; 0\)</span>.
If we want the test to have power against both the possibility that <span class="math notranslate nohighlight">\(p_x &gt; p_y\)</span> and the possibility that <span class="math notranslate nohighlight">\(p_x &lt; p_y\)</span>, we would design the test to reject when <span class="math notranslate nohighlight">\(|Z|\)</span> is large.
To test at (approximate) level <span class="math notranslate nohighlight">\(\alpha\)</span>, we would reject with <span class="math notranslate nohighlight">\(|Z| &gt; z_{1-\alpha/2}\)</span>, the <span class="math notranslate nohighlight">\(1-\alpha/2\)</span> quantile of the standard normal distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># simulating the significance level of the Z-test</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span> <span class="c1"># the normal distribution</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">default_rng</span><span class="p">,</span> <span class="n">binomial</span>  <span class="c1"># this is the Mersenne Twister; there&#39;s much to consider in picking a PRNG.</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">default_rng</span><span class="p">(</span><span class="mi">1592592021</span><span class="p">)</span>   <span class="c1"># set the seed for reproducibility</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="c1"># significance level 5%</span>
<span class="n">z_c</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># normal quantile, critical value for the test</span>

<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="o">.</span><span class="mi">01</span><span class="p">,</span> <span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">.</span><span class="mi">95</span><span class="p">,</span> <span class="o">.</span><span class="mi">99</span><span class="p">]</span>  <span class="c1"># assortment of values for p</span>

<span class="n">reps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>  
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span> 

<span class="k">def</span> <span class="nf">absZ</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    absolute value of the Z statistic for the difference in sample sums from two binary populations</span>
<span class="sd">    </span>
<span class="sd">    Used in a Z test of the hypothesis that x and y are the sample sums of IID draws from binary</span>
<span class="sd">    populations with the same (unspecified) population fraction of 1s against the alternative</span>
<span class="sd">    hypothesis that the two population fractions differ.</span>
<span class="sd">    </span>
<span class="sd">    When the population percentages are close to 0 or 1, the sample percentages can be 0 or 1,</span>
<span class="sd">    in which case the estimated variance will be zero. In that situation, return -inf or inf, </span>
<span class="sd">    as appropriate.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n : int</span>
<span class="sd">        sample size from first population</span>
<span class="sd">    m : int</span>
<span class="sd">        sample size from second population</span>
<span class="sd">    x : float</span>
<span class="sd">        sample sum of the draws from the first population</span>
<span class="sd">    y : float</span>
<span class="sd">        sample sum of the draws from the second population</span>
<span class="sd">        </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    z : float</span>
<span class="sd">        absolute value of the Z statistic</span>
<span class="sd">        </span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">pHat</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="n">m</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">abs</span><span class="p">((</span><span class="n">x</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">y</span><span class="o">/</span><span class="n">m</span><span class="p">)</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pHat</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pHat</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)))</span>
           <span class="k">if</span> <span class="n">pHat</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pHat</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
           <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">y</span><span class="o">/</span><span class="n">m</span><span class="p">))</span>

<span class="c1"># how precise should we expect the result to be?</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;max SD of alpha-hat: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">reps</span><span class="p">)))))</span>  <span class="c1"># max SD of a binomial is 1/2</span>

<span class="k">for</span> <span class="n">pj</span> <span class="ow">in</span> <span class="n">p</span><span class="p">:</span>
    <span class="n">reject</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">pj</span><span class="p">)</span> <span class="c1"># simulate X</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">pj</span><span class="p">)</span> <span class="c1"># simulate Y. X and Y could be simulated together but the code would be less clear</span>
        <span class="n">reject</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> 
                  <span class="k">if</span> <span class="n">absZ</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">z_c</span>
                  <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;p=</span><span class="si">{}</span><span class="s1"> estimated alpha=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pj</span><span class="p">,</span> <span class="n">reject</span><span class="o">/</span><span class="n">reps</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>max SD of alpha-hat: 0.0005
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in double_scalars
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p=0.01 estimated alpha=0.033563
p=0.05 estimated alpha=0.039816
p=0.1 estimated alpha=0.049005
p=0.3 estimated alpha=0.050683
p=0.4 estimated alpha=0.052481
p=0.5 estimated alpha=0.057022
p=0.6 estimated alpha=0.052372
p=0.7 estimated alpha=0.050798
p=0.9 estimated alpha=0.049088
p=0.95 estimated alpha=0.039562
p=0.99 estimated alpha=0.03344
</pre></div>
</div>
</div>
</div>
<p>As you can see, the simulation estimate of the actual significance level differs from the nominal significance level, 5%, by more than simulation variability accounts for: the test is only approximate, and the accuracy of the approximation depends on the true value of <span class="math notranslate nohighlight">\(p\)</span>. The actual significance level is below 5% when the true <span class="math notranslate nohighlight">\(p\)</span> is near 0 or 1, and above 5% when the true <span class="math notranslate nohighlight">\(p\)</span> is near 1/2.</p>
<p>Now let’s look at the distribution of approximate <span class="math notranslate nohighlight">\(P\)</span>-values for (a one-sided version of) this test.
If <span class="math notranslate nohighlight">\(Z\)</span> really had a standard normal distribution, then for <span class="math notranslate nohighlight">\(z \ge 0\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\Pr \{|Z| \le z \} = 1 - 2\Phi(-z) =  2\Phi(z) - 1,
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi\)</span> is the standard normal CDF.
Let <span class="math notranslate nohighlight">\(x \equiv 2\Phi(z) - 1\)</span>.
Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
x = \Pr \{ |Z| \le \Phi^{-1}((x+1)/2) \}
\end{equation*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
 = \Pr \{ \Phi(|Z|) \le (x+1)/2 \}
\end{equation*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
 = \Pr \{ 2\Phi(|Z|)-1 \le x \}.
\end{equation*}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(2\Phi(|Z|)-1\)</span> would have a uniform distribution: it would be a <span class="math notranslate nohighlight">\(P\)</span>-value.</p>
<p>Let’s check whether that’s true.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># one of the worst offenders</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">reps</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># simulate X</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># simulate Y</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">absZ</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([136995.,  89683.,  86794.,  82063.,  78132., 139366.,  60161.,
        136581.,  95286.,  94939.]),
 array([0.        , 0.09999988, 0.19999976, 0.29999964, 0.39999952,
        0.49999939, 0.59999927, 0.69999915, 0.79999903, 0.89999891,
        0.99999879]),
 &lt;BarContainer object of 10 artists&gt;)
</pre></div>
</div>
<img alt="../_images/tests_20_1.png" src="../_images/tests_20_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">cumul</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">yi</span><span class="p">)</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span><span class="o">/</span><span class="n">reps</span>  <span class="c1"># simulated CDF</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">cumul</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simulated distribution of nominal $P$ value for $P$=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/tests_21_0.png" src="../_images/tests_21_0.png" />
</div>
</div>
<p>The (simulated) distribution is not dominated by the uniform.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># one of the worst offenders</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">reps</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># simulate X</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="c1"># simulate Y</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">absZ</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in double_scalars
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([5.78410e+04, 1.00000e+00, 2.34340e+04, 1.13167e+05, 4.68700e+03,
        2.37444e+05, 1.13710e+05, 6.48320e+04, 1.24849e+05, 3.86530e+04]),
 array([0.        , 0.0999593 , 0.19991861, 0.29987791, 0.39983722,
        0.49979652, 0.59975583, 0.69971513, 0.79967444, 0.89963374,
        0.99959305]),
 &lt;BarContainer object of 10 artists&gt;)
</pre></div>
</div>
<img alt="../_images/tests_23_2.png" src="../_images/tests_23_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">cumul</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">yi</span><span class="p">)</span> <span class="k">for</span> <span class="n">yi</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span><span class="o">/</span><span class="n">reps</span>  <span class="c1"># simulated CDF</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">cumul</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simulated distribution of nominal $P$ value for $P$=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/tests_24_0.png" src="../_images/tests_24_0.png" />
</div>
</div>
<p>Again, the (simulated) distribution is not dominated by the uniform. The <em>nominal</em> <span class="math notranslate nohighlight">\(P\)</span>-value based on treating the <span class="math notranslate nohighlight">\(Z\)</span>-statistic as if it had a standard normal distribution is not an <em>actual</em> <span class="math notranslate nohighlight">\(P\)</span>-value for this problem.</p>
</div>
<div class="section" id="an-exact-conditional-test-based-on-invariance-permutation-methods">
<h3>An exact conditional test based on invariance: permutation methods<a class="headerlink" href="#an-exact-conditional-test-based-on-invariance-permutation-methods" title="Permalink to this headline">¶</a></h3>
<p>We now explore a different approach that yields an exact test rather than approximate test.</p>
<p>Let <span class="math notranslate nohighlight">\(N \equiv n+m\)</span>.
Let <span class="math notranslate nohighlight">\(I_1, \ldots, I_n\)</span> be the values of the draws from the first population and <span class="math notranslate nohighlight">\(I_{n+1}, \ldots, I_N\)</span> be
the values of the draws from the second population.
Because the draws are all independent, <span class="math notranslate nohighlight">\(\{I_j\}\)</span> are independent random variables with a Bernoulli distribution.
If <span class="math notranslate nohighlight">\(p_x = p_y\)</span>, they are identically distributed also.</p>
<p>Define the random vector <span class="math notranslate nohighlight">\(I \equiv (I_j)_{j=1}^N\)</span>.
Let <span class="math notranslate nohighlight">\(\pi\)</span> be a permutation of <span class="math notranslate nohighlight">\(1, 2, \ldots, N\)</span>.
(That is, <span class="math notranslate nohighlight">\(\pi = (\pi_1, \ldots, \pi_N)\)</span> is a vector of length <span class="math notranslate nohighlight">\(N\)</span> in which every number between <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(N\)</span> appears exactly once.)
Define the vector <span class="math notranslate nohighlight">\(I_\pi \equiv (I_{\pi_j})_{j=1}^N\)</span>.
This vector has the same components as <span class="math notranslate nohighlight">\(I\)</span>, but in a different order (unless <span class="math notranslate nohighlight">\(\pi\)</span> is the identity permutation).
Because the draws are IID, the joint probability distribution of this permutation of the draws is
the same as the joint probability distribution of the original draws: the components of <span class="math notranslate nohighlight">\(I\)</span> are <em>exchangeable</em>
random variables if the null hypothesis is true.</p>
<p>Equivalently, let <span class="math notranslate nohighlight">\(z\)</span> be a vector of length <span class="math notranslate nohighlight">\(N\)</span>.
Then <span class="math notranslate nohighlight">\(\mathbb{P}_0 \{I = z \} = \mathbb{P}_0 \{I = z_\pi \}\)</span> for all <span class="math notranslate nohighlight">\(N!\)</span> permutations <span class="math notranslate nohighlight">\(\pi\)</span>.
Whatever vector of values <span class="math notranslate nohighlight">\(z\)</span> was actually observed, if the null hypothesis is true, all permutations of those values
were equally likely to have been observed instead.
In turn, that implies that all <span class="math notranslate nohighlight">\(\binom{N}{n}\)</span> multisubsets of size <span class="math notranslate nohighlight">\(n\)</span> of the <span class="math notranslate nohighlight">\(n+m\)</span> components of <span class="math notranslate nohighlight">\(z\)</span>
were equally likely to be the sample from the first population (with the other <span class="math notranslate nohighlight">\(m\)</span> values
comprising the sample from the second population).</p>
<p>Let <span class="math notranslate nohighlight">\(\{z\}\)</span> denote the <em>multiset</em> of elements of <span class="math notranslate nohighlight">\(x\)</span>.
(See, e.g., <a class="reference internal" href="math-foundations.html"><span class="doc std std-doc">Mathematical Foundations</span></a>
for the distinction between a set and a multiset.)
Consider the event that <span class="math notranslate nohighlight">\(\{I\} = \{z\}\)</span>, that is, that the multiset of observed data is equal to the multiset of
elements of <span class="math notranslate nohighlight">\(z\)</span>.
Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
  \mathbb{P}_0(I = z_\pi | \{I\} = \{z\}) = \mathbb{P}_0(I = z | \{I\} = \{z\})
\end{equation*}\]</div>
<p>for all permutations <span class="math notranslate nohighlight">\(\pi\)</span> of <span class="math notranslate nohighlight">\(\{1, \ldots, n+m\}\)</span>.</p>
<p>It follows that the sample from the first population is equally likely to be any multisubset of <span class="math notranslate nohighlight">\(n\)</span> of the
<span class="math notranslate nohighlight">\(n+m\)</span> elements of the pooled sample, given the elements of the pooled sample.
That is, the conditional probability distribution of the sample from the first population is like that of a simple random sample of size <span class="math notranslate nohighlight">\(n\)</span> from the <span class="math notranslate nohighlight">\(N\)</span> elements of the pooled sample, given the elements of the pooled sample.</p>
<p>The pooled sample in this example consists of <span class="math notranslate nohighlight">\(G = \sum_{j=1}^N I_j\)</span> 1s and <span class="math notranslate nohighlight">\(N-G\)</span> 0s.
Given <span class="math notranslate nohighlight">\(G\)</span>, the conditional distribution of the number of 1s in the sample from the first population is hypergeometric with parameters <span class="math notranslate nohighlight">\(N=N\)</span> (population size), <span class="math notranslate nohighlight">\(G=G\)</span> (number of “good” items in the population), and <span class="math notranslate nohighlight">\(n=n\)</span> (sample size from the population).
Recall that <span class="math notranslate nohighlight">\(X\)</span> is the sum of the draws from the first population and <span class="math notranslate nohighlight">\(Y\)</span> is the sum of the draws from the second
population.
We have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{P}_0 \{ X = k | G=g \} = \left \{ \begin{array}{ll}
   \frac{\binom{g}{k}\binom{N-g}{n-k}}{\binom{N}{n}}, &amp; \max(0, g-m) \le k \le \min(n,g) \\
   0, &amp; \mbox{otherwise.}
   \end{array}
   \right .
\end{equation*}\]</div>
<p>Moreover, given <span class="math notranslate nohighlight">\(G\)</span>, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are dependent, since <span class="math notranslate nohighlight">\(X+Y = G\)</span>.</p>
<p>We can base a (conditional) test of the hypothesis <span class="math notranslate nohighlight">\(p_0 = p_1\)</span> on the conditional
hypergeometric distribution of <span class="math notranslate nohighlight">\(X\)</span>.
Because the hypergeometric is discrete, for some values of <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(m\)</span>, <span class="math notranslate nohighlight">\(G\)</span>, and <span class="math notranslate nohighlight">\(\alpha\)</span> we will need a randomized
test <span class="math notranslate nohighlight">\(A=A_{\alpha;g}(\cdot, \cdot)\)</span> to attain exactly level <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>What shall we use as the acceptance region?
To have power against the alternative that <span class="math notranslate nohighlight">\(p_x &gt; p_y\)</span> and the alternative <span class="math notranslate nohighlight">\(p_x &lt; p_y\)</span>, we should
reject if <span class="math notranslate nohighlight">\(X\)</span> is “too small” or if <span class="math notranslate nohighlight">\(X\)</span> is “too big.”
There are any number of ways we could trade off small and big.
For instance, we could make the acceptance region <span class="math notranslate nohighlight">\(A = A_{\alpha;g}(x,u)\)</span> nearly symmetric around
<span class="math notranslate nohighlight">\(\mathbb{E}(X|G) = nG/N\)</span>.
Or we could make the chance of rejecting when <span class="math notranslate nohighlight">\(X\)</span> is too small equal to the chance of rejecting when <span class="math notranslate nohighlight">\(X\)</span> is too big.</p>
<p>We will do something else: pick the acceptance region to include
as few values in <span class="math notranslate nohighlight">\(\{0, \ldots, n\}\)</span> as possible.
That means omitting from <span class="math notranslate nohighlight">\(A\)</span> the possible values of <span class="math notranslate nohighlight">\(X\)</span> that have the lowest (conditional) probabilities.
Because the hypergeometric distribution is unimodal, this yelds a test that rejects when <span class="math notranslate nohighlight">\(X\)</span> is “in the tails” of the hypergeometric distribution, as desired.
Let <span class="math notranslate nohighlight">\(\mathcal{I}\)</span> denote the smallest subset of <span class="math notranslate nohighlight">\(\{0, \ldots, n\}\)</span> such that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{P}_0 \{X \in \mathcal{I} | G=g \} &lt; 1-\alpha
\end{equation*}\]</div>
<p>Define</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathcal{J} \equiv \arg \max_{j \notin \mathcal{I}} \mathbb{P}_0 \{ X = j | G=g \}
\end{equation*}\]</div>
<p>be the outcome or outcomes with largest conditional probability not included in <span class="math notranslate nohighlight">\(\mathcal{I}\)</span>.
Let</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\gamma \equiv \frac{(1-\alpha) - \mathbb{P}_0 \{X \in \mathcal{I} | G=g \}}{\mathbb{P}_0 \{X \in \mathcal{J} |G=g\}}
\end{equation*}\]</div>
<p>be the fraction of the probability of <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> that needs to be added to the probability of <span class="math notranslate nohighlight">\(\mathcal{I}\)</span> to make
the sum equal <span class="math notranslate nohighlight">\(1-\alpha\)</span>.
Define the acceptance region</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
A_{\alpha; g} = \{ (x,u) \in \mathcal{I} \times [0,1] \} \cup \{ (x, u): x \in \mathcal{J} \mbox{ and } u \le \gamma \}.
\end{equation*}\]</div>
<p>Let <span class="math notranslate nohighlight">\(U\)</span> be a <span class="math notranslate nohighlight">\(U[0,1]\)</span> random variable independent of <span class="math notranslate nohighlight">\(X\)</span>.
Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{P}_0 \{ (X, U) \in A_{\alpha; g} | G=g \} = 1-\alpha.
\end{equation*}\]</div>
<p>That is, <span class="math notranslate nohighlight">\(A\)</span> is an acceptance region for a test of the null hypothesis with conditional significance level <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>Conditioning on <span class="math notranslate nohighlight">\(G\)</span> eliminates the nuisance parameter <span class="math notranslate nohighlight">\(p\)</span>, the fraction of ones in the two populations
from which the sample was drawn: whatever <span class="math notranslate nohighlight">\(p\)</span> might be, and whatever the fraction of ones in the sample, every random sample of <span class="math notranslate nohighlight">\(n\)</span> of the <span class="math notranslate nohighlight">\(n+m\)</span> observations is equally likely to have been the sample from the first population, if the null hypothesis is true.
See <a class="reference internal" href="permute-intro.html"><span class="doc std std-doc">Introduction to Permutation Tests</span></a>.</p>
<p>This test, in a slightly different form, is called <em>Fisher’s Exact Test</em>.
It is useful for a wide range of problems, including clinical trials and A/B testing.</p>
</div>
<div class="section" id="illustration">
<h3>Illustration<a class="headerlink" href="#illustration" title="Permalink to this headline">¶</a></h3>
<p>To visualize what’s going on, suppose <span class="math notranslate nohighlight">\(n=m=10\)</span> and <span class="math notranslate nohighlight">\(G=5\)</span>. Under the null, the conditional distribution of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(G=5\)</span> is hypergeometric with parameters <span class="math notranslate nohighlight">\(N=n+m=20\)</span>, <span class="math notranslate nohighlight">\(G=5\)</span>, and <span class="math notranslate nohighlight">\(n=10\)</span>.
The pmf of that distribution is plotted below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">hypergeom</span> <span class="c1"># the hypergeometric distribution</span>
<span class="n">n</span><span class="o">=</span><span class="mi">10</span>
<span class="n">m</span><span class="o">=</span><span class="mi">10</span>
<span class="n">N</span><span class="o">=</span><span class="n">n</span><span class="o">+</span><span class="n">m</span>
<span class="n">G</span><span class="o">=</span><span class="mi">5</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">pmf</span> <span class="o">=</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">G</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">pmf</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;outcomes&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;hypergeometric pmf&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;pmf: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">pmf</span><span class="p">))))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/tests_28_0.png" src="../_images/tests_28_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>pmf: [(0, 0.016253869969040196), (1, 0.13544891640866824), (2, 0.34829721362229055), (3, 0.34829721362229055), (4, 0.13544891640866824), (5, 0.016253869969040196), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># probability of rejecting when X \in {1, 5} to get overall level 0.05</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="p">(</span><span class="n">alpha</span><span class="o">-</span><span class="p">(</span><span class="n">pmf</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">pmf</span><span class="p">[</span><span class="mi">5</span><span class="p">]))</span><span class="o">/</span><span class="p">(</span><span class="n">pmf</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">pmf</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0645714285714292
</pre></div>
</div>
</div>
</div>
<p>To construct the acceptance region for a level <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> test, first notice that <span class="math notranslate nohighlight">\(\mathbb{P}_0\{X &gt; 5 | G=5\} = 0\)</span>, so the outcomes 6, 7, …, 10 should be outside <span class="math notranslate nohighlight">\(\mathcal{I}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\mathbb{P}_0\{X = 0 | G=5\} = \mathbb{P}_0\{X = 5 | G=5\} = 0.01625\)</span>, so if we exclude those outcomes from <span class="math notranslate nohighlight">\(\mathcal{I}\)</span>, the chance of rejecting the null is <span class="math notranslate nohighlight">\(2 \times 0.01625387 = 0.032508\)</span>.
For the test to have level <span class="math notranslate nohighlight">\(\alpha\)</span>, we need to reject more often: an additional <span class="math notranslate nohighlight">\(0.017492\)</span> of the time.
If we always rejected when <span class="math notranslate nohighlight">\(X=1\)</span> or <span class="math notranslate nohighlight">\(X=4\)</span>, we would reject too often, because each of those possibilities has probability <span class="math notranslate nohighlight">\(0.13545\)</span>.
If when <span class="math notranslate nohighlight">\(X \in \{1, 4\}\)</span> we reject with probability <span class="math notranslate nohighlight">\(\gamma = .01749/(2\times 0.13545) = 0.06457\)</span>, the overall chance of erroneously rejecting the null will be 5%, as desired.</p>
</div>
<div class="section" id="unconditional-tests-from-conditional-tests">
<h3>Unconditional tests from conditional tests<a class="headerlink" href="#unconditional-tests-from-conditional-tests" title="Permalink to this headline">¶</a></h3>
<p>If you always test conditionally at level not greater than <span class="math notranslate nohighlight">\(\alpha\)</span>, that yields a test that has unconditional level not greater than <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>Suppose we condition on the value of a discrete random variable, such as <span class="math notranslate nohighlight">\(G\)</span> in the previous example, then test at level not greater than <span class="math notranslate nohighlight">\(\alpha\)</span>, conditional on the value of <span class="math notranslate nohighlight">\(G\)</span>.
The unconditional significance level of the test is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
\mathbb{P}_0 \{\mbox{reject null}\} =
\sum_g \mathbb{P}_0 \{ \mbox{reject null} | G=g \} \mathbb{P}_0 \{G=g\}
\le \sum_g \alpha \mathbb{P}_0 \{G=g\} = \alpha \sum_g \mathbb{P}_0 \{G=g\} = \alpha,
\end{equation*}\]</div>
<p>where the sums are over all values <span class="math notranslate nohighlight">\(g\)</span> that <span class="math notranslate nohighlight">\(G\)</span> can take.</p>
<p>A similar proof establishes the result when <span class="math notranslate nohighlight">\(G\)</span> does not have a discrete distribution.</p>
</div>
<div class="section" id="numerical-comparison">
<h3>Numerical comparison<a class="headerlink" href="#numerical-comparison" title="Permalink to this headline">¶</a></h3>
<p>We saw that the <span class="math notranslate nohighlight">\(Z\)</span>-test does not necessarily have its nominal level in this problem.
We now implement the permutation test (a randomized version of Fisher’s exact test) for comparison.</p>
</div>
<div class="section" id="algorithmic-considerations">
<h3>Algorithmic considerations<a class="headerlink" href="#algorithmic-considerations" title="Permalink to this headline">¶</a></h3>
<div class="section" id="finding-mathcal-i-and-mathcal-j">
<h4>Finding <span class="math notranslate nohighlight">\(\mathcal{I}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{J}\)</span><a class="headerlink" href="#finding-mathcal-i-and-mathcal-j" title="Permalink to this headline">¶</a></h4>
<p>In principle, we could sort the <span class="math notranslate nohighlight">\(n+1\)</span> values of the pmf to find the outcomes with the largest probabilities to include in <span class="math notranslate nohighlight">\(\mathcal{I}\)</span>, and the largest-probability outcomes not included in <span class="math notranslate nohighlight">\(\mathcal{I}\)</span> to be <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> (if the test needs to be randomized to attain the desired significance level).</p>
<p>But the best sorting algorithms still take <span class="math notranslate nohighlight">\(n\log n\)</span> operations, and we don’t need a complete sort–we only need to divide the probabilities into two groups, the biggest and the rest.</p>
<p>However, because the hypergeomtric distribution is unimodal, the smallest probability will either be <span class="math notranslate nohighlight">\(\mathbb{P}_0 \{X = 0\}\)</span> or <span class="math notranslate nohighlight">\(\mathbb{P}_0 \{X = n\}\)</span> (or they will be equal).
After removing that outcome from consideration, the second-smallest probability will be either that of the largest remaining outcome or the smallest remaining outcome, etc. This means we can identify the outcomes to include in <span class="math notranslate nohighlight">\(\mathcal{I}\)</span> with a number of operations that is linear in <span class="math notranslate nohighlight">\(n\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># simulate the significance level of the permutation test</span>
<span class="k">def</span> <span class="nf">fisher_accept</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Acceptance region for randomized hypergeometric test</span>
<span class="sd">    </span>
<span class="sd">    Find the acceptance region for a randomized, exact level alpha test of </span>
<span class="sd">    the null hypothesis X~Hypergeometric(N, G, n). The acceptance region is</span>
<span class="sd">    the smallest possible. (And not, for instance, symmetric.)</span>

<span class="sd">    If a non-randomized, conservative test is desired, use the union of I and J </span>
<span class="sd">    as the acceptance region.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    N:  integer</span>
<span class="sd">        population size</span>
<span class="sd">    G:  integer</span>
<span class="sd">        number of &quot;good&quot; items in the population</span>
<span class="sd">    n:  integer</span>
<span class="sd">        sample size</span>
<span class="sd">    alpha : float</span>
<span class="sd">        desired significance level    </span>
<span class="sd">  </span>
<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    I:  list</span>
<span class="sd">        observed values for which the test never rejects</span>
<span class="sd">    J:  list </span>
<span class="sd">        observed values for which the test sometimes rejects</span>
<span class="sd">    gamma : float</span>
<span class="sd">        probability the test does not reject when the observed value is in J</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">I</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                    <span class="c1"># start with all possible outcomes, then remove some</span>
    <span class="n">pmf</span> <span class="o">=</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">G</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>   <span class="c1"># hypergeometric pmf</span>
    <span class="n">bottom</span> <span class="o">=</span> <span class="mi">0</span>                     <span class="c1"># smallest outcome still in I</span>
    <span class="n">top</span> <span class="o">=</span> <span class="n">n</span>                        <span class="c1"># largest outcome still in I</span>
    <span class="n">J</span> <span class="o">=</span> <span class="p">[]</span>                         <span class="c1"># outcomes for which the test is randomized</span>
    <span class="n">p_J</span> <span class="o">=</span> <span class="mi">0</span>                        <span class="c1"># probability of the outcomes for which test is randomized</span>
    <span class="n">p_tail</span> <span class="o">=</span> <span class="mi">0</span>                     <span class="c1"># probability of outcomes not in I</span>
    <span class="k">while</span> <span class="n">p_tail</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>          <span class="c1"># still need to remove outcomes from the acceptance region</span>
        <span class="n">pb</span> <span class="o">=</span> <span class="n">pmf</span><span class="p">[</span><span class="n">bottom</span><span class="p">]</span>
        <span class="n">pt</span> <span class="o">=</span> <span class="n">pmf</span><span class="p">[</span><span class="n">top</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">pb</span> <span class="o">&lt;</span> <span class="n">pt</span><span class="p">:</span>                <span class="c1"># the smaller possibility has smaller probability</span>
            <span class="n">J</span> <span class="o">=</span> <span class="p">[</span><span class="n">bottom</span><span class="p">]</span>
            <span class="n">p_J</span> <span class="o">=</span> <span class="n">pb</span>
            <span class="n">bottom</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">pb</span> <span class="o">&gt;</span> <span class="n">pt</span><span class="p">:</span>              <span class="c1"># the larger possibility has smaller probability</span>
            <span class="n">J</span> <span class="o">=</span> <span class="p">[</span><span class="n">top</span><span class="p">]</span>
            <span class="n">p_J</span> <span class="o">=</span> <span class="n">pt</span>
            <span class="n">top</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>                      
            <span class="k">if</span> <span class="n">bottom</span> <span class="o">&lt;</span> <span class="n">top</span><span class="p">:</span>       <span class="c1"># the two possibilities have equal probability</span>
                <span class="n">J</span> <span class="o">=</span> <span class="p">[</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">]</span>
                <span class="n">p_J</span> <span class="o">=</span> <span class="n">pb</span><span class="o">+</span><span class="n">pt</span>
                <span class="n">bottom</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">top</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>                  <span class="c1"># there is only one possibility left</span>
                <span class="n">J</span> <span class="o">=</span> <span class="p">[</span><span class="n">bottom</span><span class="p">]</span>
                <span class="n">p_J</span> <span class="o">=</span> <span class="n">pb</span>
                <span class="n">bottom</span> <span class="o">+=</span><span class="mi">1</span>
        <span class="n">p_tail</span> <span class="o">+=</span> <span class="n">p_J</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">J</span><span class="p">:</span>
            <span class="n">I</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_tail</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span><span class="o">/</span><span class="n">p_J</span>     <span class="c1"># probability of accepting H_0 when X in J to get </span>
                                   <span class="c1"># exact level alpha</span>
    <span class="k">return</span> <span class="n">I</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">gamma</span>
        
    
<span class="k">def</span> <span class="nf">apply_test</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">U</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Apply a randomized test A to data x using auxiliary uniform randomness U</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    A:  triple</span>
<span class="sd">        first element is a list or set, the values of x for which the test never rejects</span>
<span class="sd">        second element is a list or set, the values of x for which the test sometimes rejects</span>
<span class="sd">        third element is a float in [0, 1], the probability of rejecting when x is in the second element</span>
<span class="sd">    x:  number-like</span>
<span class="sd">        observed data</span>
<span class="sd">    U:  float in [0, 1]</span>
<span class="sd">        observed value of an independent U[0,1] random variable</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    reject: Boolean</span>
<span class="sd">        True if the test rejects</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;probability out of range:</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">U</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;uniform variable out of range:</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    <span class="k">return</span> <span class="ow">not</span><span class="p">(</span><span class="n">x</span> <span class="ow">in</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="ow">in</span> <span class="n">A</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">U</span> <span class="o">&lt;=</span> <span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise: write unit tests of the two functions.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set up the simulations</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="c1"># significance level 5%</span>
<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="o">.</span><span class="mi">01</span><span class="p">,</span> <span class="o">.</span><span class="mi">05</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">,</span> <span class="o">.</span><span class="mi">4</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">7</span><span class="p">,</span> <span class="o">.</span><span class="mi">9</span><span class="p">,</span> <span class="o">.</span><span class="mi">95</span><span class="p">,</span> <span class="o">.</span><span class="mi">99</span><span class="p">]</span>  <span class="c1"># assortment of values for p</span>

<span class="n">reps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">n</span><span class="o">+</span><span class="n">m</span>

<span class="c1"># how precise should we expect the result to be?</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;max SD of p-hat: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">reps</span><span class="p">)))))</span>  <span class="c1"># max SD of a binomial is 1/2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>max SD of p-hat: 0.0005
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># brute force: re-compute the test for each replication</span>
<span class="k">for</span> <span class="n">pj</span> <span class="ow">in</span> <span class="n">p</span><span class="p">:</span>
    <span class="n">reject</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">pj</span><span class="p">)</span> <span class="c1"># simulate X</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">pj</span><span class="p">)</span> <span class="c1"># simulate Y</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">fisher_accept</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">X</span><span class="o">+</span><span class="n">Y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">reject</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">apply_test</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">())</span>
                   <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;p=</span><span class="si">{}</span><span class="s1"> estimated alpha=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pj</span><span class="p">,</span> <span class="n">reject</span><span class="o">/</span><span class="n">reps</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p=0.01 estimated alpha=0.05001
p=0.05 estimated alpha=0.049823
p=0.1 estimated alpha=0.050227
p=0.3 estimated alpha=0.049927
p=0.4 estimated alpha=0.050298
p=0.5 estimated alpha=0.049955
p=0.6 estimated alpha=0.049959
p=0.7 estimated alpha=0.049856
p=0.9 estimated alpha=0.049854
p=0.95 estimated alpha=0.050085
p=0.99 estimated alpha=0.049817
CPU times: user 47min 31s, sys: 15.4 s, total: 47min 46s
Wall time: 47min 56s
</pre></div>
</div>
</div>
</div>
<p>This test has true significance level equal to its nominal significance level: it is an <em>exact</em> test rather than an <em>approximate</em> test.</p>
<p>Randomized tests have some drawbacks in practice.
For instance, it would be hard to explain to a consulting client or a judge that for a given set of data, the test you
propose to use sometimes
rejects the null hypothesis and sometimes does not, depending on a random factor (<span class="math notranslate nohighlight">\(U\)</span>) that has nothing to do
with the data or the experiment.</p>
<p>In such situations, it might make sense to use a <em>conservative</em> test rather than an exact randomized test.
A conservative test is one for which the chance of a Type I error is not greater than <span class="math notranslate nohighlight">\(\alpha\)</span>.
In the previous example, if the test rejects only when the test</p>
</div>
</div>
<div class="section" id="id1">
<h3>Algorithmic considerations<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The previous simulation is <strong>slow</strong>, in part because it finds the conditional acceptance region
<code class="docutils literal notranslate"><span class="pre">reps</span></code> times for each <code class="docutils literal notranslate"><span class="pre">pj</span> <span class="pre">in</span> <span class="pre">p</span></code>.</p>
<p>But there are relatively few possible acceptance regions, one for each possible value of <span class="math notranslate nohighlight">\(G\)</span>, i.e., <span class="math notranslate nohighlight">\(n+m+1\)</span>.</p>
<p>What happens if we pre-compute the acceptance regions for all possible values of <span class="math notranslate nohighlight">\(G\)</span>, and look them up as needed?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># Smarter approach: pre-compute the tests. There are only n+m+1 possible tests, far fewer than reps</span>
<span class="c1"># Include the &quot;cost&quot; of precomputing the tests in the comparison.</span>

<span class="n">AA</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">AA</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fisher_accept</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>

<span class="k">for</span> <span class="n">pj</span> <span class="ow">in</span> <span class="n">p</span><span class="p">:</span>
    <span class="n">reject</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">pj</span><span class="p">)</span> <span class="c1"># simulate X</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">pj</span><span class="p">)</span> <span class="c1"># simulate Y</span>
        <span class="n">reject</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> 
                   <span class="k">if</span> <span class="n">apply_test</span><span class="p">(</span><span class="n">AA</span><span class="p">[</span><span class="n">X</span><span class="o">+</span><span class="n">Y</span><span class="p">],</span> <span class="n">X</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">())</span> 
                   <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;p=</span><span class="si">{}</span><span class="s1"> estimated alpha=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pj</span><span class="p">,</span> <span class="n">reject</span><span class="o">/</span><span class="n">reps</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p=0.01 estimated alpha=0.050358
p=0.05 estimated alpha=0.050045
p=0.1 estimated alpha=0.049983
p=0.3 estimated alpha=0.050171
p=0.4 estimated alpha=0.05002
p=0.5 estimated alpha=0.049699
p=0.6 estimated alpha=0.049793
p=0.7 estimated alpha=0.049458
p=0.9 estimated alpha=0.049673
p=0.95 estimated alpha=0.050049
p=0.99 estimated alpha=0.050015
CPU times: user 1min 17s, sys: 295 ms, total: 1min 18s
Wall time: 1min 18s
</pre></div>
</div>
</div>
</div>
<p>This easy change speeds up the simulation by a factor of more than 40.</p>
<p>Alternatively, we can use the <code class="docutils literal notranslate"><span class="pre">functools</span></code> library to automatically cache the acceptance regions as they are computed.</p>
<p>Here’s the acceptance region function with the <code class="docutils literal notranslate"><span class="pre">&#64;lru_cache</span></code> decorator:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">lru_cache</span>

<span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># decorate the function to cache the results of calls to the function</span>
<span class="k">def</span> <span class="nf">fisher_accept</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Acceptance region for randomized hypergeometric test</span>
<span class="sd">    </span>
<span class="sd">    Find the acceptance region for a randomized, exact level alpha test of </span>
<span class="sd">    the null hypothesis X~Hypergeometric(N, G, n). The acceptance region is</span>
<span class="sd">    the smallest possible. (And not, for instance, symmetric.)</span>

<span class="sd">    If a non-randomized, conservative test is desired, use the union of I and J as </span>
<span class="sd">    the acceptance region.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    N:  integer</span>
<span class="sd">        population size</span>
<span class="sd">    G:  integer</span>
<span class="sd">        number of &quot;good&quot; items in the population</span>
<span class="sd">    n:  integer</span>
<span class="sd">        sample size</span>
<span class="sd">    alpha : float</span>
<span class="sd">        desired significance level    </span>
<span class="sd">  </span>
<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    I:  list</span>
<span class="sd">        values for which the test never rejects</span>
<span class="sd">    J:  list </span>
<span class="sd">        values for which the test sometimes rejects</span>
<span class="sd">    gamma : float</span>
<span class="sd">        probability the test does not reject when the value is in J</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>          <span class="c1"># all possible values of X</span>
    <span class="n">I</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                    <span class="c1"># start with all possible outcomes, then remove some</span>
    <span class="n">pmf</span> <span class="o">=</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">G</span><span class="p">,</span><span class="n">n</span><span class="p">)</span>   <span class="c1"># hypergeometric pmf</span>
    <span class="n">bottom</span> <span class="o">=</span> <span class="mi">0</span>                     <span class="c1"># smallest outcome still in I</span>
    <span class="n">top</span> <span class="o">=</span> <span class="n">n</span>                        <span class="c1"># largest outcome still in I</span>
    <span class="n">J</span> <span class="o">=</span> <span class="p">[]</span>                         <span class="c1"># outcome for which the test is randomized</span>
    <span class="n">p_J</span> <span class="o">=</span> <span class="mi">0</span>                        <span class="c1"># probability of the randomized outcome</span>
    <span class="n">p_tail</span> <span class="o">=</span> <span class="mi">0</span>                     <span class="c1"># probability of outcomes excluded from I</span>
    <span class="k">while</span> <span class="n">p_tail</span> <span class="o">&lt;</span> <span class="n">alpha</span><span class="p">:</span>          <span class="c1"># still need to remove outcomes from the acceptance region</span>
        <span class="n">pb</span> <span class="o">=</span> <span class="n">pmf</span><span class="p">[</span><span class="n">bottom</span><span class="p">]</span>
        <span class="n">pt</span> <span class="o">=</span> <span class="n">pmf</span><span class="p">[</span><span class="n">top</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">pb</span> <span class="o">&lt;</span> <span class="n">pt</span><span class="p">:</span>                <span class="c1"># the lower possibility has smaller probability</span>
            <span class="n">J</span> <span class="o">=</span> <span class="p">[</span><span class="n">bottom</span><span class="p">]</span>
            <span class="n">p_J</span> <span class="o">=</span> <span class="n">pb</span>
            <span class="n">bottom</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">pb</span> <span class="o">&gt;</span> <span class="n">pt</span><span class="p">:</span>              <span class="c1"># the upper possibility has smaller probability</span>
            <span class="n">J</span> <span class="o">=</span> <span class="p">[</span><span class="n">top</span><span class="p">]</span>
            <span class="n">p_J</span> <span class="o">=</span> <span class="n">pt</span>
            <span class="n">top</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>                      
            <span class="k">if</span> <span class="n">bottom</span> <span class="o">&lt;</span> <span class="n">top</span><span class="p">:</span>       <span class="c1"># the two possibilities have equal probability</span>
                <span class="n">J</span> <span class="o">=</span> <span class="p">[</span><span class="n">bottom</span><span class="p">,</span> <span class="n">top</span><span class="p">]</span>
                <span class="n">p_J</span> <span class="o">=</span> <span class="n">pb</span><span class="o">+</span><span class="n">pt</span>
                <span class="n">bottom</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">top</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>                  <span class="c1"># there is only one possibility left</span>
                <span class="n">J</span> <span class="o">=</span> <span class="p">[</span><span class="n">bottom</span><span class="p">]</span>
                <span class="n">p_J</span> <span class="o">=</span> <span class="n">pb</span>
                <span class="n">bottom</span> <span class="o">+=</span><span class="mi">1</span>
        <span class="n">p_tail</span> <span class="o">+=</span> <span class="n">p_J</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">J</span><span class="p">:</span>
            <span class="n">I</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_tail</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span><span class="o">/</span><span class="n">p_J</span>     <span class="c1"># probability of accepting H_0 when X in J to get exact level alpha</span>
    <span class="k">return</span> <span class="n">I</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">gamma</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># re-run the original &quot;brute-force&quot; code but with function caching</span>
<span class="k">for</span> <span class="n">pj</span> <span class="ow">in</span> <span class="n">p</span><span class="p">:</span>
    <span class="n">reject</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">pj</span><span class="p">)</span> <span class="c1"># simulate X</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">pj</span><span class="p">)</span> <span class="c1"># simulate Y</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">fisher_accept</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">X</span><span class="o">+</span><span class="n">Y</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">reject</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">apply_test</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">())</span>
                   <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;p=</span><span class="si">{}</span><span class="s1"> estimated alpha=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pj</span><span class="p">,</span> <span class="n">reject</span><span class="o">/</span><span class="n">reps</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>p=0.01 estimated alpha=0.050092
p=0.05 estimated alpha=0.050045
p=0.1 estimated alpha=0.049978
p=0.3 estimated alpha=0.050282
p=0.4 estimated alpha=0.050424
p=0.5 estimated alpha=0.049985
p=0.6 estimated alpha=0.050117
p=0.7 estimated alpha=0.049918
p=0.9 estimated alpha=0.049929
p=0.95 estimated alpha=0.050044
p=0.99 estimated alpha=0.0501
CPU times: user 1min 21s, sys: 329 ms, total: 1min 21s
Wall time: 1min 22s
</pre></div>
</div>
</div>
</div>
<p>Again, this speeds up the simulation by a factor of more than 40, with even less coding effort.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exercise: How would you find a $P$-value for this test?</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="intersection-union-hypotheses">
<h2>Intersection-Union Hypotheses<a class="headerlink" href="#intersection-union-hypotheses" title="Permalink to this headline">¶</a></h2>
<p>In many situations, a null hypothesis of interest is the intersection of simpler hypotheses. For instance, the hypothesis that a university does not discriminate in its graduate admissions might be represented as</p>
<p>(does not discriminate in arts and humanities) <span class="math notranslate nohighlight">\(\cap\)</span> (does not discriminate in sciences) <span class="math notranslate nohighlight">\(\cap\)</span> (does not discriminate in engineering) <span class="math notranslate nohighlight">\(\cap\)</span> (does not discriminate in professional schools).</p>
<p>In this example, the alternative hypothesis is a <em>union</em>, viz.,</p>
<p>(discriminates in arts and humanities) <span class="math notranslate nohighlight">\(\cup\)</span> (discriminates in sciences) <span class="math notranslate nohighlight">\(\cup\)</span> (discriminates in engineering) <span class="math notranslate nohighlight">\(\cup\)</span> (discriminates in professional schools).</p>
<p>Framing a test this way leads to an <em>intersection-union test</em>.
The null hypothesis is the intersection</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   H_0 \equiv \cap_{j=1}^J H_{0j}
\end{equation*}\]</div>
<p>and the alternative is the union</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   H_1 \equiv \cup_{j=1}^J H_{0j}^c.
\end{equation*}\]</div>
<p>There can be good reasons for representating a null hypothesis as such an intersection.
In the example just mentioned, the applicant pool might be quite different across disciplines, making it hard to judge at the aggregate level whether there is discrimination, while testing within each discipline is more straightforward (that is, <em>Simpson’s Paradox</em> can be an issue).</p>
<p>Hypotheses about multivariate distributions can sometimes be expressed as the intersection of hypotheses about each dimension separately. For instance, the hypothesis that a <span class="math notranslate nohighlight">\(J\)</span>-dimensional distribution has zero mean could be represented as</p>
<p>(1st component has zero mean) <span class="math notranslate nohighlight">\(\cap\)</span> (2nd component has zero mean) <span class="math notranslate nohighlight">\(\cap\)</span> <span class="math notranslate nohighlight">\(\cdots\)</span> <span class="math notranslate nohighlight">\(\cap\)</span> (<span class="math notranslate nohighlight">\(J\)</span>th component has zero mean)</p>
<p>The alternative is again a union:</p>
<p>(1st component has nonzero mean) <span class="math notranslate nohighlight">\(\cup\)</span> (2nd component has nonzero mean) <span class="math notranslate nohighlight">\(\cup\)</span> <span class="math notranslate nohighlight">\(\cdots\)</span> <span class="math notranslate nohighlight">\(\cup\)</span> (<span class="math notranslate nohighlight">\(J\)</span>th component has nonzero mean)</p>
</div>
<div class="section" id="combinations-of-experiments-and-stratified-experiments">
<h2>Combinations of experiments and stratified experiments<a class="headerlink" href="#combinations-of-experiments-and-stratified-experiments" title="Permalink to this headline">¶</a></h2>
<p>The same kind of issue arises when combining information from different experiments.
For instance, imagine testing whether a drug is effective. We might have several randomized, controlled trials in different places, or a large experiment involving a number of centers, each of which performs its own randomization (i.e., the randomization is stratified).</p>
<p>How can we combine the information from the separate (independent) experiments to test the null hypothesis that the drug is ineffective?</p>
<p>Again, the overall null hypothesis is “the drug doesn’t help,” which can be written as an intersection of hypotheses</p>
<p>(drug doesn’t help in experiment 1) <span class="math notranslate nohighlight">\(\cap\)</span> (drug doesn’t help in experiment 2) <span class="math notranslate nohighlight">\(\cap\)</span> <span class="math notranslate nohighlight">\(\cdots\)</span>  <span class="math notranslate nohighlight">\(\cap\)</span> (drug doesn’t help in experiment <span class="math notranslate nohighlight">\(J\)</span>),</p>
<p>and the alternative can be written as</p>
<p>(drug helps in experiment 1) <span class="math notranslate nohighlight">\(\cup\)</span> (drug helps in experiment 2) <span class="math notranslate nohighlight">\(\cup\)</span> <span class="math notranslate nohighlight">\(\cdots\)</span>  <span class="math notranslate nohighlight">\(\cup\)</span> (drug helps in experiment <span class="math notranslate nohighlight">\(J\)</span>),</p>
<p>a union.</p>
</div>
<div class="section" id="combining-evidence">
<h2>Combining evidence<a class="headerlink" href="#combining-evidence" title="Permalink to this headline">¶</a></h2>
<p>Suppose we have a test of each “partial” null hypothesis <span class="math notranslate nohighlight">\(H_{0j}\)</span>. Clearly, if the <span class="math notranslate nohighlight">\(P\)</span>-value for one of those tests is sufficiently small, that’s evidence that the overall null <span class="math notranslate nohighlight">\(H_0\)</span> is false.</p>
<p>But suppose none of the individual <span class="math notranslate nohighlight">\(P\)</span>-values is small, but many are “not large.”
Is there a way to combine them to get sronger evidence about <span class="math notranslate nohighlight">\(H_0\)</span>?</p>
</div>
<div class="section" id="combining-functions">
<h2>Combining functions<a class="headerlink" href="#combining-functions" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\lambda\)</span> be a <span class="math notranslate nohighlight">\(J\)</span>-vector of statistics such that the distribution of <span class="math notranslate nohighlight">\(\lambda_j\)</span>
if hypothesis <span class="math notranslate nohighlight">\(H_{0j}\)</span> is true is known.
We assume that smaller values of <span class="math notranslate nohighlight">\(\lambda_j\)</span> are stronger evidence against <span class="math notranslate nohighlight">\(H_{0j}\)</span>.
For instance, <span class="math notranslate nohighlight">\(\lambda_j\)</span> might be the <span class="math notranslate nohighlight">\(P\)</span>-value of <span class="math notranslate nohighlight">\(H_{0j}\)</span> for some test.</p>
<p>Consider a function</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi: [0, 1]^J \rightarrow \Re; \lambda = (\lambda_1, \ldots, \lambda_J) \mapsto \phi(\lambda)
\end{equation*}\]</div>
<p>with the properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> is non-increasing in every argument, i.e., <span class="math notranslate nohighlight">\(\phi( \ldots, \lambda_j, \ldots) \ge \phi(( \ldots, \lambda_j', \ldots)\)</span> if <span class="math notranslate nohighlight">\(\lambda_j \le \lambda_j'\)</span>, <span class="math notranslate nohighlight">\(j = 1, \ldots, J\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> attains its maximum if any of its arguments equals 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> attains its minimum if all of its arguments equal 1.</p></li>
<li><p>for all <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>, there exist finite functions <span class="math notranslate nohighlight">\(\phi_-(\alpha)\)</span>, <span class="math notranslate nohighlight">\(\phi_+(\alpha)\)</span> such that if every partial null hypothesis <span class="math notranslate nohighlight">\(\{H_{0j}\}\)</span> is true,</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \Pr \{\phi_-(\alpha) \le \phi(\lambda) \le \phi_+(\alpha) \} \ge 1-\alpha\end{equation*}\]</div>
<p>and <span class="math notranslate nohighlight">\([\phi_-(\alpha), \phi_+(\alpha)] \subset [\phi_-(\alpha'), \phi_+(\alpha')]\)</span> if <span class="math notranslate nohighlight">\(\alpha \ge \alpha'\)</span>.</p>
<p>Then we can use <span class="math notranslate nohighlight">\(\phi(\lambda)\)</span> as the basis of a test of <span class="math notranslate nohighlight">\(H_0 = \cap_{j=1}^J H_{0j}\)</span>.</p>
<div class="section" id="fisher-s-combining-function">
<h3>Fisher’s combining function<a class="headerlink" href="#fisher-s-combining-function" title="Permalink to this headline">¶</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi_F(\lambda) \equiv -2 \sum_{j=1}^J \ln(\lambda_j).\end{equation*}\]</div>
</div>
<div class="section" id="liptak-s-combining-function">
<h3>Liptak’s combining function<a class="headerlink" href="#liptak-s-combining-function" title="Permalink to this headline">¶</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi_L(\lambda) \equiv \sum_{j=1}^J \Phi^{-1}(1-\lambda_j),\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> is the inverse standard normal CDF.</p>
</div>
<div class="section" id="tippet-s-combining-function">
<h3>Tippet’s combining function<a class="headerlink" href="#tippet-s-combining-function" title="Permalink to this headline">¶</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi_T(\lambda) \equiv \max_{j=1}^J (1-\lambda_j).\end{equation*}\]</div>
</div>
<div class="section" id="direct-combination-of-test-statistics">
<h3>Direct combination of test statistics<a class="headerlink" href="#direct-combination-of-test-statistics" title="Permalink to this headline">¶</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi_D \equiv \sum_{j=1}^J f_j(\lambda_j), \end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\{ f_j \}\)</span> are suitable decreasing functions. For instance, if <span class="math notranslate nohighlight">\(\lambda_j\)</span> is the <span class="math notranslate nohighlight">\(P\)</span>-value for <span class="math notranslate nohighlight">\(H_{0j}\)</span> corresponding to some test statistic <span class="math notranslate nohighlight">\(T_j\)</span> for which larger values are stronger evidence against <span class="math notranslate nohighlight">\(H_{0j}\)</span>, we could use <span class="math notranslate nohighlight">\(\phi_D = \sum_j T_j\)</span>.</p>
</div>
</div>
<div class="section" id="fisher-s-combining-function-for-independent-p-values">
<h2>Fisher’s combining function for independent <span class="math notranslate nohighlight">\(P\)</span>-values<a class="headerlink" href="#fisher-s-combining-function-for-independent-p-values" title="Permalink to this headline">¶</a></h2>
<p>Suppose <span class="math notranslate nohighlight">\(H_0\)</span> is true, that <span class="math notranslate nohighlight">\(\lambda_j\)</span> is the <span class="math notranslate nohighlight">\(P\)</span>-value of <span class="math notranslate nohighlight">\(H_{0j}\)</span> for some pre-specified test, that the distribution of <span class="math notranslate nohighlight">\(\lambda_j\)</span> is continuous under <span class="math notranslate nohighlight">\(H_{0j}\)</span>, and that <span class="math notranslate nohighlight">\(\{ \lambda_j \}\)</span> are independent if <span class="math notranslate nohighlight">\(H_0\)</span> is true.</p>
<p>Then, if <span class="math notranslate nohighlight">\(H_0\)</span> is true, <span class="math notranslate nohighlight">\(\{ \lambda_j \}\)</span> are IID <span class="math notranslate nohighlight">\(U[0,1]\)</span>.</p>
<p>Under <span class="math notranslate nohighlight">\(H_{0j}\)</span>, the distribution of <span class="math notranslate nohighlight">\(-\ln \lambda_j\)</span> is exponential(1):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \Pr \{ -\ln \lambda_j \le x \} = \Pr \{ \ln \lambda_j \ge -x \} = \Pr \{ \lambda_j \ge e^{-x} \} = 1 - e^{-x}.
\end{equation*}\]</div>
<p>The distribution of 2 times an exponential is <span class="math notranslate nohighlight">\(\chi_2^2\)</span>:
the pdf of a chi-square with <span class="math notranslate nohighlight">\(k\)</span> degrees of freedom is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \frac{1}{2^{k/2}\Gamma(k/2)} x^{k/2-1} e^{-x/2}.
\end{equation*}\]</div>
<p>For <span class="math notranslate nohighlight">\(k=2\)</span>, this simplifies to <span class="math notranslate nohighlight">\(e^{-x/2}/2\)</span>, the exponential density scaled by a factor of 2.</p>
<p>Thus, under <span class="math notranslate nohighlight">\(H_0\)</span>, <span class="math notranslate nohighlight">\(\phi_F(\lambda)\)</span> is the sum of <span class="math notranslate nohighlight">\(J\)</span> independent <span class="math notranslate nohighlight">\(\chi_2^2\)</span> random variables. The distribution of a sum of independent chi-square random variables is a chi-square random variable with degrees of freedom equal to the sum of the degrees of freedom of the variables that were added.</p>
<p>Hence, under <span class="math notranslate nohighlight">\(H_0\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \phi_F(\lambda) \sim \chi_{2J}^2,
\end{equation*}\]</div>
<p>the chi-square distribution with <span class="math notranslate nohighlight">\(2n\)</span> degrees of freedom.</p>
<p>Let <span class="math notranslate nohighlight">\(\chi_{k}^2(\alpha)\)</span> denote the <span class="math notranslate nohighlight">\(1-\alpha\)</span> quantile of the chi-square distribution
with <span class="math notranslate nohighlight">\(k\)</span> degrees of freedom.
If we reject <span class="math notranslate nohighlight">\(H_0\)</span> when</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \phi_F(\lambda) \ge \chi_{2J}^2(\alpha),
\end{equation*}\]</div>
<p>that yields a significance level <span class="math notranslate nohighlight">\(\alpha\)</span> test of <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Simulate distribution of Fisher&#39;s combining function when all nulls are true</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.polynomial</span> <span class="kn">import</span> <span class="n">polynomial</span> <span class="k">as</span> <span class="n">P</span>

<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span><span class="p">,</span> <span class="n">binom</span>

<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span><span class="p">,</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>

<span class="k">def</span> <span class="nf">plot_fisher_null</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">reps</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">reps</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">U</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">reps</span><span class="o">/</span><span class="mi">40</span><span class="p">),</span> <span class="mi">5</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;simulation&quot;</span><span class="p">)</span>
    <span class="n">mxv</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mxv</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">chi2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;chi-square pdf, df=&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interact</span><span class="p">(</span><span class="n">plot_fisher_null</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "5b028f19b3a14734b3741099ff1d07ff", "version_major": 2, "version_minor": 0}
</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function __main__.plot_fisher_null(n=5, reps=10000)&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="when-p-values-have-atoms">
<h2>When <span class="math notranslate nohighlight">\(P\)</span>-values have atoms<a class="headerlink" href="#when-p-values-have-atoms" title="Permalink to this headline">¶</a></h2>
<p>A real random variable <span class="math notranslate nohighlight">\(X\)</span> is first-order stochastically larger than a real random variable <span class="math notranslate nohighlight">\(Y\)</span> if for all <span class="math notranslate nohighlight">\(x \in \Re\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \Pr \{ X \ge x \} \ge \Pr \{ Y \ge x \},
\end{equation*}\]</div>
<p>with strict inequality for some <span class="math notranslate nohighlight">\(x \in \Re\)</span>.</p>
<p>Suppose <span class="math notranslate nohighlight">\(\{\lambda_j \}\)</span> for <span class="math notranslate nohighlight">\(\{ H_{0j}\}\)</span> satisfy</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \Pr \{ \lambda_j \le p  || H_{0j} \} \le p.
\end{equation*}\]</div>
<p>This takes into account the possibility that <span class="math notranslate nohighlight">\(\lambda_j\)</span> does not have a continuous
distribution under <span class="math notranslate nohighlight">\(H_{0j}\)</span>, ensuring that <span class="math notranslate nohighlight">\(\lambda_j\)</span> is still a <em>conservative</em> <span class="math notranslate nohighlight">\(P\)</span>-value.</p>
<p>Since <span class="math notranslate nohighlight">\(\ln\)</span> is monotone, it follows that for all <span class="math notranslate nohighlight">\(x \in \Re\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \Pr \{ -2 \ln \lambda_j \ge x \} \le \Pr \{ -2 \ln U \ge x \}.
\end{equation*}\]</div>
<p>That is, if <span class="math notranslate nohighlight">\(\lambda_j\)</span> does not have a continuous distribution,
the a <span class="math notranslate nohighlight">\(\chi_2^2\)</span> variable is stochastically larger than the distribution of <span class="math notranslate nohighlight">\(-2\ln \lambda_j\)</span>.</p>
<p>It turns out that <span class="math notranslate nohighlight">\(X\)</span> is stochastically larger than <span class="math notranslate nohighlight">\(Y\)</span> if and only if
there is some probability space on which there exist
two random variables, <span class="math notranslate nohighlight">\(\tilde{X}\)</span> and <span class="math notranslate nohighlight">\(\tilde{Y}\)</span> such that <span class="math notranslate nohighlight">\(\tilde{X} \sim X\)</span>,
<span class="math notranslate nohighlight">\(\tilde{Y} \sim Y\)</span>, and <span class="math notranslate nohighlight">\(\Pr \{\tilde{X} \ge \tilde{Y} \} = 1\)</span>.
(See, e.g., Grimmett and Stirzaker,<em>Probability and Random Processes</em>, 3rd edition,
Theorem 4.12.3.)</p>
<p>Let <span class="math notranslate nohighlight">\(\{X_j\}_{j=1}^n\)</span> be IID <span class="math notranslate nohighlight">\(\chi_2^2\)</span> random variables,
and let <span class="math notranslate nohighlight">\(Y_j \equiv - 2 \ln \lambda_j\)</span>, <span class="math notranslate nohighlight">\(j=1, \ldots, J\)</span>.</p>
<p>Then there is some probability space
for which we can define <span class="math notranslate nohighlight">\(\{\tilde{Y_j}\}\)</span> and <span class="math notranslate nohighlight">\(\{\tilde{X_j}\}\)</span> such that</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\tilde{Y_j})\)</span> has the same joint distribution as <span class="math notranslate nohighlight">\((Y_j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\tilde{X_j})\)</span> has the same joint distribution as <span class="math notranslate nohighlight">\((X_j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\tilde{X_j} \ge \tilde{Y_j}\)</span> for all <span class="math notranslate nohighlight">\(j\)</span> with probability one.</p></li>
</ul>
<p>Then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sum_j \tilde{Y_j}\)</span> has the same distribution as <span class="math notranslate nohighlight">\(\sum_j Y_j = -2 \sum_j \ln \lambda_j\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_j \tilde{X_j}\)</span> has the same distribution as <span class="math notranslate nohighlight">\(\sum_j X_j\)</span> (namely, chi-square with <span class="math notranslate nohighlight">\(2J\)</span> degrees of freedom),</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_j \tilde X_j  \ge \sum_j \tilde{Y_j}\)</span>.</p></li>
</ul>
<p>That is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \Pr \left \{-2 \sum_j \ln \lambda_j \ge \chi_{2J}^2(\alpha) \right \} \le \alpha.
\end{equation*}\]</div>
<p>Thus, we still get a conservative hypothesis test if one or more of the <span class="math notranslate nohighlight">\(p\)</span>-values for the
partial tests have atoms under their respective null hypotheses <span class="math notranslate nohighlight">\(\{H_{0j}\}\)</span>.</p>
<div class="section" id="estimating-p-values-by-simulation">
<h3>Estimating <span class="math notranslate nohighlight">\(P\)</span>-values by simulation<a class="headerlink" href="#estimating-p-values-by-simulation" title="Permalink to this headline">¶</a></h3>
<p>Suppose that if the null hypothesis is true, the probability distribution of the
data is invariant under some group <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, for instance, the reflection group or the symmetric (i.e., permutation) group.</p>
<p>For any pre-specified test statistic <span class="math notranslate nohighlight">\(T\)</span>, we can estimate a <span class="math notranslate nohighlight">\(P\)</span>-value by generating uniformly distributed random elements of the orbit of the data under the action of the group (see <a class="reference internal" href="math-foundations.html"><span class="doc std std-doc">Mathematical Fundations</span></a> if these notions are unfamiliar).</p>
<p>Suppose we generate <span class="math notranslate nohighlight">\(n\)</span> random elements of the orbit.
Let <span class="math notranslate nohighlight">\(x_0\)</span> denote the original data; let <span class="math notranslate nohighlight">\(\{\pi_k\}_{k=1}^K\)</span> denote IID random elements of
<span class="math notranslate nohighlight">\(\mathcal{G}\)</span> and <span class="math notranslate nohighlight">\(x_k = \pi_k(x_0)\)</span>, <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span> denote <span class="math notranslate nohighlight">\(K\)</span> random elements of the
orbit of <span class="math notranslate nohighlight">\(x_0\)</span> under <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>.</p>
<p>An unbiased estimate of the <span class="math notranslate nohighlight">\(P\)</span>-value (assuming that the random elements are generated uniformly at random–see <a class="reference internal" href="permute-sample.html"><span class="doc std std-doc">Algorithms for Pseudo-Random Sampling</span></a> for a caveats), is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \hat{P} = \frac{\#\{ k&gt;0: T(\pi_k(x_0)) \ge T(x_0)\}}{K}.
\end{equation*}\]</div>
<p>Once <span class="math notranslate nohighlight">\(x_0\)</span> is known, the events <span class="math notranslate nohighlight">\(\{T(\pi_j(x_0)) \ge T(x_0)\}\)</span> are IID with probability
<span class="math notranslate nohighlight">\(P\)</span> of occurring, and <span class="math notranslate nohighlight">\(\hat{P}\)</span> is an unbiased estimate of <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>Another estimate of <span class="math notranslate nohighlight">\(P\)</span> that can also be interpreted as an exact <span class="math notranslate nohighlight">\(P\)</span>-value for
a randomized test (as discussed below), is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \hat{P}' = \frac{\#\{ k \ge 0: T(\pi_k(x_0)) \ge T(x_0)\}}{K+1},
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_0\)</span> is the identity permutation.</p>
<p>The reasoning behind this choice is that, if the null hypothesis is true, the original data are one of the equally likely elements of the orbit of the data–exactly as likely as the elements generated from it.
Thus there are really <span class="math notranslate nohighlight">\(K+1\)</span> values that are equally likely if the null is true,
rather than <span class="math notranslate nohighlight">\(K\)</span>: nature provided one more random permutation, the original data.
The estimate <span class="math notranslate nohighlight">\(\hat{P}'\)</span> is never smaller than <span class="math notranslate nohighlight">\(1/(K+1)\)</span>.
Some practitioners like this because it never estimates the <span class="math notranslate nohighlight">\(P\)</span>-value to be zero.
There are other reasons for preferring it, discussed below.</p>
<p>The estimate <span class="math notranslate nohighlight">\(\hat{P}'\)</span> of <span class="math notranslate nohighlight">\(P\)</span> is generally biased, however, since <span class="math notranslate nohighlight">\(\hat{P}\)</span> is unbiased and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \hat{P}' = \frac{K\hat{P} + 1}{K+1} = \frac{K}{K+1} \hat{P} + \frac{1}{K+1},
\end{equation*}\]</div>
<p>so</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \mathbb{E} \hat{P}' = \frac{K}{K+1} P + \frac{1}{K+1} =
   P  + (1-P) \frac{1}{K+1} &gt; P.
\end{equation*}\]</div>
</div>
</div>
<div class="section" id="accounting-for-simulation-error-in-stratum-wise-p-values">
<h2>Accounting for simulation error in stratum-wise <span class="math notranslate nohighlight">\(P\)</span>-values<a class="headerlink" href="#accounting-for-simulation-error-in-stratum-wise-p-values" title="Permalink to this headline">¶</a></h2>
<p>Suppose that the <span class="math notranslate nohighlight">\(P\)</span>-value <span class="math notranslate nohighlight">\(\lambda_j\)</span> for <span class="math notranslate nohighlight">\(H_{0j}\)</span> is estimated by <span class="math notranslate nohighlight">\(b_j\)</span> simulations instead of being known exactly.
How can we take the uncertainty of the simulation estimate into account?</p>
<p>Here, we will pretend that the simulation itself is perfect: that the PRNG generates true IID <span class="math notranslate nohighlight">\(U[0,1]\)</span> variables, that pseudo-random integers on <span class="math notranslate nohighlight">\(\{0, 1, \ldots, N\}\)</span> really are equally likely, and that pseudo-random samples or permutations really are equally likely, etc.</p>
<p>The error we are accounting for is not the imperfection of the PRNG or other algorithms, just the uncertainty due to approximating a theoretical probability <span class="math notranslate nohighlight">\(\lambda_j\)</span> by an estimate via (perfect) simulation.</p>
<div class="section" id="a-crude-approach-simultaneous-one-sided-upper-confidence-bounds-for-every-lambda-j">
<h3>A crude approach: simultaneous one-sided upper confidence bounds for every <span class="math notranslate nohighlight">\(\lambda_j\)</span><a class="headerlink" href="#a-crude-approach-simultaneous-one-sided-upper-confidence-bounds-for-every-lambda-j" title="Permalink to this headline">¶</a></h3>
<p>Suppose we find, for each <span class="math notranslate nohighlight">\(j\)</span>, an upper confidence bound for <span class="math notranslate nohighlight">\(\lambda_j\)</span> (the “true” <span class="math notranslate nohighlight">\(P\)</span>-value in stratum <span class="math notranslate nohighlight">\(j\)</span>),
for instance, by inverting binomial tests based on <span class="math notranslate nohighlight">\(\# \{k &gt; 0: T(\pi_k(x_0)) \ge T(x_0) \}\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\phi\)</span> is monotonic in every coordinate, the upper confidence confidence bounds
for <span class="math notranslate nohighlight">\(\{ \lambda_j \}\)</span> imply a lower confidence bound for <span class="math notranslate nohighlight">\(\phi(\lambda)\)</span>, which translates to an upper confidence bound for the combined <span class="math notranslate nohighlight">\(P\)</span>-value.</p>
<p>What is the confidence level of the bound on the combined <span class="math notranslate nohighlight">\(P\)</span>-value?
If the <span class="math notranslate nohighlight">\(P\)</span>-value estimates are independent, the joint coverage probability of a set of <span class="math notranslate nohighlight">\(n\)</span> independent confidence bounds with confidence level <span class="math notranslate nohighlight">\(\alpha\)</span> is <span class="math notranslate nohighlight">\(1-(1-\alpha)^n\)</span>, as we shall show.</p>
<p>Let <span class="math notranslate nohighlight">\(A_j\)</span> denote the event that the upper confidence bound for <span class="math notranslate nohighlight">\(\lambda_j\)</span> is greater than or equal to <span class="math notranslate nohighlight">\(\lambda_j\)</span>, and suppose <span class="math notranslate nohighlight">\(\Pr \{A_j\} = 1-\alpha_j\)</span>.</p>
<p>Regardless of the dependence among the events <span class="math notranslate nohighlight">\(\{A_j \}\)</span>, the chance that all of the confidence bounds cover their corresponding parameters can be bounded using Bonferroni’s inequality:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Pr \{ \cap_j A_j \} = 1 - \Pr \{ \cup_j A_j^c \} \ge 1 - \sum_j \Pr \{A_j^c \} 
   = 1 - \sum_j (1- \Pr \{A_j \}) = 1 - \sum_j \alpha_j.
\end{equation*}\]</div>
<p>If <span class="math notranslate nohighlight">\(\{A_j\}\)</span> are independent,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Pr \{ \cap_j A_j \} = \prod_j \Pr \{ A_j \} = \prod_j (1-\alpha_j).
\end{equation*}\]</div>
<p>Both of those expressions tend to get small quickly as <span class="math notranslate nohighlight">\(n\)</span> gets large;
bounding <span class="math notranslate nohighlight">\(\phi(\lambda)\)</span> by bounding the components of <span class="math notranslate nohighlight">\(\lambda\)</span> is inefficient.</p>
<p>Let’s look for a different approach.</p>
</div>
<div class="section" id="a-sharper-approach-use-a-related-randomized-test">
<h3>A sharper approach: use a related randomized test<a class="headerlink" href="#a-sharper-approach-use-a-related-randomized-test" title="Permalink to this headline">¶</a></h3>
<p>This section presents a different approach, based on <span class="math notranslate nohighlight">\(\hat{P}'\)</span> (the biased estimate of <span class="math notranslate nohighlight">\(P\)</span>) rather than
<span class="math notranslate nohighlight">\(\hat{P}\)</span>.
It yields a surprisingly simple and elegant conservative test.</p>
<p>The key is to change the test itself: instead of treating <span class="math notranslate nohighlight">\(\hat{\lambda}_j\)</span> or <span class="math notranslate nohighlight">\(\hat{\lambda}_j'\)</span> as an estimate of
<span class="math notranslate nohighlight">\(\lambda_j\)</span>–the <span class="math notranslate nohighlight">\(P\)</span>-value for <span class="math notranslate nohighlight">\(H_{0j}\)</span> for the original test–we define a <em>new</em> test based on</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \hat{\lambda}_j' \equiv \frac{\#\{k \ge 0: T(\pi_k(x_0)) \ge T(x_0)\}}{K_j+1},
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_0\)</span> is the identity permutation and <span class="math notranslate nohighlight">\(\{ \pi_k \}_{k=1}^{K_j}\)</span> are elements of <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>
selected at random uniformly.</p>
<p>While <span class="math notranslate nohighlight">\(\hat{\lambda}_j'\)</span> is a biased estimate of <span class="math notranslate nohighlight">\(\lambda_j\)</span>, we shall see that <strong>it is itself a valid conditional <span class="math notranslate nohighlight">\(P\)</span>-value</strong>; that is,  <span class="math notranslate nohighlight">\(\Pr \{ \hat{\lambda}_j' \le p \} \le p\)</span>, given that the data are in the orbit of <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>Note that this (conditional) probability involved has two sources of randomness:</p>
<ol class="simple">
<li><p>The randomness in the original data, <span class="math notranslate nohighlight">\(x_0\)</span> (although we condition on the event that the data fall in the orbit of <span class="math notranslate nohighlight">\(x_0\)</span>)</p></li>
<li><p>The randomness in generating the random transformations <span class="math notranslate nohighlight">\(\{ \pi_k \}_{k=1}^{K_j} \subset \mathcal{G}\)</span></p></li>
</ol>
<p>The resulting hypothesis test is a <em>randomized test</em>: it uses auxilliary randomness
in addition to the randomness in the data.
If the experiment were repeated and the data turned out to be <span class="math notranslate nohighlight">\(x_0\)</span> again,
the test will in general give a different <span class="math notranslate nohighlight">\(P\)</span> value: <span class="math notranslate nohighlight">\(\hat{\lambda}_j'\)</span> is random even if <span class="math notranslate nohighlight">\(x_0\)</span> is known.
The decision to reject the null hypothesis (or not) is random even after the data have been observed.</p>
<p>Randomized tests have a number of desirable theoretical properties (related to continuity and convexity),
but they are rarely used explicitly in practice.
Tests involving simulated <span class="math notranslate nohighlight">\(P\)</span>-values are an example where randomized tests are used implicitly rather than explicitly–generally without recognizing that the resulting test is randomized.</p>
<p>This section shows that the randomization involved in simulating <span class="math notranslate nohighlight">\(P\)</span>-values can be taken into account explicitly to get a conservative test.</p>
<p>By construction, <span class="math notranslate nohighlight">\(\{\pi_k(x_0)\}_{k=1}^{K_j}\)</span> are IID uniformly distributed on the orbit of <span class="math notranslate nohighlight">\(x_0\)</span>.
(We are ignoring imperfections in the PRNG and other algorithms.)
Thus, <span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span> are IID random variables.
The event <span class="math notranslate nohighlight">\(\hat{\lambda}_j' \le p\)</span> is the event that <span class="math notranslate nohighlight">\(T(x_0)= T(\pi_0(x_0))\)</span> is
larger than all but (at most) <span class="math notranslate nohighlight">\((K_j+1)p\)</span> of the values <span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span>.
Under the null, <span class="math notranslate nohighlight">\(T(x_0)\)</span> is equally likely to be any of them.</p>
<p>Let <span class="math notranslate nohighlight">\(p' = \lfloor (K_j+1)p \rfloor /(K_j+1)\)</span>. Then <span class="math notranslate nohighlight">\(p' \le p\)</span> and <span class="math notranslate nohighlight">\((K_j+1)p'\)</span> is an integer.
Sort the values <span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span> from largest to smallest, breaking ties arbitrarily.
Consider the <span class="math notranslate nohighlight">\((K_j+1)p'\)</span>th element of the list.
If it is strictly greater than the <span class="math notranslate nohighlight">\((K_j+1)p'+1\)</span>st element of the list, then there are <span class="math notranslate nohighlight">\((K_j+1)p'\)</span> permutations
<span class="math notranslate nohighlight">\(\pi_k\)</span> for which <span class="math notranslate nohighlight">\(T(\pi_k(x_0))\)</span> is strictly greater than all but <span class="math notranslate nohighlight">\((K_j+1)p\)</span> of the values
<span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span>.
If the <span class="math notranslate nohighlight">\((K_j+1)p'\)</span>th element of the list is equal to the <span class="math notranslate nohighlight">\((K_j+1)p'+1\)</span> element, then there are
<em>fewer</em> than <span class="math notranslate nohighlight">\((K_j+1)p'\)</span> such permutations.
Either way, the chance that a randomly selected element of the multiset <span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span>
is strictly greater than all but at most <span class="math notranslate nohighlight">\((K_j+1)p'\)</span> of the elements is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Pr \{ \hat{\lambda}_j' \le p \} = \Pr \{ \hat{\lambda}_j' \le p' \} \le \frac{(K_j+1)p'}{K_j+1} = p' \le p.
\end{equation*}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\Pr \{\hat{\lambda}_j' \le p \} \le p\)</span>.
That is, <strong><span class="math notranslate nohighlight">\(\hat{\lambda}_j\)</span> is <em>itself</em> a conservative <span class="math notranslate nohighlight">\(P\)</span>-value</strong> for a randomized test, separate from the fact that it is a (biased) estimate of <span class="math notranslate nohighlight">\(\lambda_j\)</span>, the <span class="math notranslate nohighlight">\(P\)</span>-value for a related non-randomized test.</p>
<p>The test is defined implicitly: reject <span class="math notranslate nohighlight">\(H_{0j}\)</span> at significance level <span class="math notranslate nohighlight">\(\alpha\)</span> if
<span class="math notranslate nohighlight">\(\hat{\lambda}_j' \le \alpha\)</span>.</p>
<p>It follows that applying Fisher’s combining function to <span class="math notranslate nohighlight">\(\hat{\lambda}' = (\hat{\lambda}_j')_{j=1}^J\)</span> gives a conservative test of the intersection null hypothesis.</p>
</div>
</div>
<div class="section" id="dependent-tests">
<h2>Dependent tests<a class="headerlink" href="#dependent-tests" title="Permalink to this headline">¶</a></h2>
<p>If <span class="math notranslate nohighlight">\(\{ \lambda_j \}_{j=1}^J\)</span> are dependent, the distribution of <span class="math notranslate nohighlight">\(\phi_F(\lambda)\)</span> is no longer chi-square when the null hypotheses are true.
Nonetheless, one can calibrate a test based on Fisher’s combining function (or any other combining function) by simulation.
This is commonly used in multivariate permutation tests involving dependent partial tests
using “lockstep” permutations.</p>
<p>See, e.g., Pesarin, F. and L. Salmaso, 2010. <em>Permutation Tests for Complex Data: Theory, Applications and Software</em>, Wiley, 978-0-470-51641-6.</p>
<p>We shall illustrate how the approach can be used to construct nonparametric multivariate tests from univariate tests to address for the two-sample problem (i.e., is there evidence that two samples come from different populations, or is it plausible
that they are a single population randomly divided into two groups?).
This is equivalent to testing whether treatment has an effect in a controlled, randomized study
in which the subjects who receive treatment are a simple random sample of the study group,
using the Neyman model for causal inference.
(The null hypothesis is that treatment makes no difference whatsoever: each subject’s response would
be the same whether that subject was assigned to treatment or to control, and without regard for the assignment of
other subjects to treatment or control.)</p>
<p>We have <span class="math notranslate nohighlight">\(N\)</span> subjects of whom <span class="math notranslate nohighlight">\(N_t\)</span> are treated and <span class="math notranslate nohighlight">\(N_c\)</span> are controls.
Each subject has a vector of <span class="math notranslate nohighlight">\(J\)</span> measurements.
For each of these <span class="math notranslate nohighlight">\(J\)</span> “dimensions” we have a test statistic <span class="math notranslate nohighlight">\(T_j\)</span> (for instance, the difference between the mean of the treated and the mean of the controls on that dimension–but we could use something else, and we don’t have to use the same test statistic for different dimensions).</p>
<p>Each <span class="math notranslate nohighlight">\(T_j\)</span> takes the responses of the treated and the controls on dimension <span class="math notranslate nohighlight">\(j\)</span> and yields a number.
We will assume that larger values of <span class="math notranslate nohighlight">\(T_j\)</span> are stronger evidence against the null hypothesis that for dimension <span class="math notranslate nohighlight">\(j\)</span> treatment doesn’t make any difference.</p>
<p>Let <span class="math notranslate nohighlight">\(T\)</span> denote the whole <span class="math notranslate nohighlight">\(J\)</span>-vector of test statistics. Let <span class="math notranslate nohighlight">\(t(0)\)</span> denote the observed value of the
<span class="math notranslate nohighlight">\(J\)</span>-vector of test statistics for the original data.</p>
<p>Now consider randomly re-labelling <span class="math notranslate nohighlight">\(N_t\)</span> of the <span class="math notranslate nohighlight">\(N\)</span> subjects as treated and the remaining <span class="math notranslate nohighlight">\(N_c\)</span> as controls
by simple random sampling, so that all subsets of size <span class="math notranslate nohighlight">\(N_t\)</span> are equally likely to be labeled “treated.”
Each re-labelling carries the subject’s entire <span class="math notranslate nohighlight">\(J\)</span>-vector of responses with it:
the dimensions are randomized “in lockstep.”</p>
<p>Let <span class="math notranslate nohighlight">\(t(k)\)</span> denote the observed <span class="math notranslate nohighlight">\(J\)</span>-vector of test statistics for the <span class="math notranslate nohighlight">\(k\)</span>th random allocation (i.e., the <span class="math notranslate nohighlight">\(k\)</span>th random permutation).
We permute the data <span class="math notranslate nohighlight">\(K\)</span> times in all, each yielding a <span class="math notranslate nohighlight">\(J\)</span>-vector <span class="math notranslate nohighlight">\(t(k)\)</span> of observed values of the test statistics. This gives <span class="math notranslate nohighlight">\(K+1\)</span> permutations in all, including the original data (the zeroth permutation), for which the vector is <span class="math notranslate nohighlight">\(t(0)\)</span>. Let <span class="math notranslate nohighlight">\(t_j(k)\)</span> denote the test statistic for dimension <span class="math notranslate nohighlight">\(j\)</span> for the <span class="math notranslate nohighlight">\(k\)</span>th permutation.</p>
<p>We now transform the <span class="math notranslate nohighlight">\(J\)</span> by <span class="math notranslate nohighlight">\((K+1)\)</span> matrix <span class="math notranslate nohighlight">\([t_j(k)]_{j=1}^J{}_{k=0}^K\)</span> to a corresponding matrix of <span class="math notranslate nohighlight">\(P\)</span>-values for the univariate permutation tests.</p>
<p>For <span class="math notranslate nohighlight">\(j=1, \ldots , J\)</span> and <span class="math notranslate nohighlight">\(k = 0, \ldots , K\)</span>, define</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   P_j(k) \equiv \frac{\#\{ \ell \in \{0, \ldots, K \} : t_j(\ell) \ge t_j(k) \}}{K+1}.  
\end{equation*}\]</div>
<p>This is the simulated upper tail probability of the <span class="math notranslate nohighlight">\(k\)</span>th observed value of the <span class="math notranslate nohighlight">\(j\)</span>th test statistic
under the null hypothesis.</p>
<p>Think of the values of <span class="math notranslate nohighlight">\(P_j(k)\)</span> as a matrix.
Each column corresponds to a random permutation of the original data (the 0th column corresponds to the original data); each row corresponds to a dimension of measurement; each entry is a number between <span class="math notranslate nohighlight">\(1/(K+1)\)</span> and 1.</p>
<p>Now apply Fisher’s combining function <span class="math notranslate nohighlight">\(\phi_F\)</span> (or Tippett’s, or Stouffer’s, or anything else) to each column of
<span class="math notranslate nohighlight">\(J\)</span> numbers. That gives <span class="math notranslate nohighlight">\(K+1\)</span> numbers,
<span class="math notranslate nohighlight">\(f(k), k=0, \ldots , K\)</span>, one for each permutation of the data.
The overall “Non-Parametric Combination of tests” (NPC) <span class="math notranslate nohighlight">\(P\)</span>-value is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
 P_{\mbox{NPC}} \equiv \frac{\#\{ k \in \{0, \ldots, K\} : f(k) \ge f(0) \}}{K+1}. 
\end{equation*}\]</div>
<p>This is the simulated lower tail probability of the observed value of the combining function under the randomization.</p>
<p>Ultimately, this whole thing is just a univariate permutation test that uses a complicated test statistic
<span class="math notranslate nohighlight">\(\phi_F\)</span> that assigns one number to each permutation of the multivariate data.</p>
<p>Also see <a class="reference external" href="http://statlab.github.io/permute/">the permute Python package</a>.</p>
</div>
<div class="section" id="stratified-permutation-tests">
<h2>Stratified Permutation Tests<a class="headerlink" href="#stratified-permutation-tests" title="Permalink to this headline">¶</a></h2>
<p>Two examples:</p>
<ul class="simple">
<li><p>Boring, A., K. Ottoboni, and P.B. Stark, 2016. Student Evaluations of Teaching (Mostly) Do Not Measure Teaching Effectiveness, <em>ScienceOpen</em>, doi 10.14293/S2199-1006.1.SOR-EDU.AETBZC.v1</p></li>
<li><p>Hessler, M.,  D.M. Pöpping, H. Hollstein, H. Ohlenburg, P.H. Arnemann, C. Massoth, L.M. Seidel, A. Zarbock &amp; M. Wenk, 2018. Availability of cookies during an academic course session affects evaluation of teaching, <em>Medical Education, 52</em>, 1064–1072. doi 10.1111/medu.13627</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Philip Stark and Fernando Pérez<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>