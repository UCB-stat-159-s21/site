
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Combinations of tests and stratified tests &#8212; Collaborative and Reproducible Data Science</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Collaborative and Reproducible Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../overview.html">
   Statistics 159/259, Spring 2021 Course Summary
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../schedule.html">
   Statistics 159/259: Weekly Plan
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Syllabus
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../syllabus.html">
   Syllabus for Statistics 159/259: Reproducible and Collaborative Statistical Data Science
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Homework Assignments
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw01-background.html">
   1. Statistics 159/259, Homework 1.
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture Notes
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01/index.html">
   Introduction to Git and Github
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   An Idiosyncratic Sample of Applied Statistics
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Notes/combining-tests.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Notes/combining-tests.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#p-values">
   <span class="math notranslate nohighlight">
    \(P\)
   </span>
   -values
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#randomized-tests">
     Randomized tests
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intersection-union-hypotheses">
   Intersection-Union Hypotheses
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combinations-of-experiments-and-stratified-experiments">
   Combinations of experiments and stratified experiments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combining-evidence">
   Combining evidence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#combining-functions">
   Combining functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fisher-s-combining-function">
     Fisher’s combining function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#liptak-s-combining-function">
     Liptak’s combining function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tippet-s-combining-function">
     Tippet’s combining function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#direct-combination-of-test-statistics">
     Direct combination of test statistics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fisher-s-combining-function-for-independent-p-values">
   Fisher’s combining function for independent
   <span class="math notranslate nohighlight">
    \(P\)
   </span>
   -values
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#when-p-values-have-atoms">
   When
   <span class="math notranslate nohighlight">
    \(P\)
   </span>
   -values have atoms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-p-values-by-simulation">
     Estimating
     <span class="math notranslate nohighlight">
      \(P\)
     </span>
     -values by simulation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#accounting-for-simulation-error-in-stratum-wise-p-values">
   Accounting for simulation error in stratum-wise
   <span class="math notranslate nohighlight">
    \(P\)
   </span>
   -values
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-crude-approach-simultaneous-one-sided-upper-confidence-bounds-for-every-lambda-j">
     A crude approach: simultaneous one-sided upper confidence bounds for every
     <span class="math notranslate nohighlight">
      \(\lambda_j\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-sharper-approach-use-a-related-randomized-test">
     A sharper approach: use a related randomized test
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dependent-tests">
   Dependent tests
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stratified-permutation-tests">
   Stratified Permutation Tests
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="combinations-of-tests-and-stratified-tests">
<h1>Combinations of tests and stratified tests<a class="headerlink" href="#combinations-of-tests-and-stratified-tests" title="Permalink to this headline">¶</a></h1>
<div class="section" id="p-values">
<h2><span class="math notranslate nohighlight">\(P\)</span>-values<a class="headerlink" href="#p-values" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Observe data <span class="math notranslate nohighlight">\(X \sim \mathbb{P}\)</span>.</p></li>
<li><p>Null hypothesis <span class="math notranslate nohighlight">\(\mathbb{P} = \mathbb{P}_0\)</span> (or more generally, <span class="math notranslate nohighlight">\(\mathbb{P} \in \mathcal{P}_0\)</span>).</p></li>
<li><p>Nested (monotone) hypothesis tests:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\{A_\alpha : \alpha \in (0, 1] \}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}_0 \{ X \notin A_\alpha \} \le \alpha\)</span> (or more generally, <span class="math notranslate nohighlight">\(\mathbb{P} \{ X \notin A_\alpha \} \le \alpha, \; \forall \mathbb{P} \in \mathcal{P}_0\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(A_\alpha \subset A_\beta\)</span> if <span class="math notranslate nohighlight">\(\beta &lt; \alpha\)</span> (Can always re-define <span class="math notranslate nohighlight">\(A_\alpha \leftarrow \cup_{\beta \ge \alpha } A_\beta\)</span>)</p></li>
</ul>
</li>
<li><p>If we observe <span class="math notranslate nohighlight">\(X = x\)</span>, <span class="math notranslate nohighlight">\(P\)</span>-value is <span class="math notranslate nohighlight">\(\sup \{ \alpha: x \in A_\alpha \}\)</span>.</p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(P = P(X)\)</span> is a <span class="math notranslate nohighlight">\(P\)</span>-value,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  
 \Pr \{ P(X) \le p || \mbox{null hypothesis is true} \} \le p.
\end{equation*}\]</div>
<p>In fact, we could take this to be the <em>definition</em> of a <span class="math notranslate nohighlight">\(P\)</span>-value.</p>
<div class="section" id="randomized-tests">
<h3>Randomized tests<a class="headerlink" href="#randomized-tests" title="Permalink to this headline">¶</a></h3>
<p>Sometimes it is useful for a hypothesis test to depend not only on the data but also on “auxilliary” randomness, typically a <span class="math notranslate nohighlight">\(U[0, 1]\)</span> variable that is independent of the data <span class="math notranslate nohighlight">\(X\)</span>.  Then we still require</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  
 \Pr_{X, U} \{ P(X, U) \le p || \mbox{null hypothesis is true} \} \le p,
\end{equation*}\]</div>
<p>where the probability is with respect to the joint distribution of the data and the auxilliary randomness <span class="math notranslate nohighlight">\(U\)</span>.</p>
</div>
</div>
<div class="section" id="intersection-union-hypotheses">
<h2>Intersection-Union Hypotheses<a class="headerlink" href="#intersection-union-hypotheses" title="Permalink to this headline">¶</a></h2>
<p>In many situations, a null hypothesis of interest is the intersection of simpler hypotheses. For instance, the hypothesis that a university does not discriminate in its graduate admissions might be represented as</p>
<p>(does not discriminate in arts and humanities) <span class="math notranslate nohighlight">\(\cap\)</span> (does not discriminate in sciences) <span class="math notranslate nohighlight">\(\cap\)</span> (does not discriminate in engineering) <span class="math notranslate nohighlight">\(\cap\)</span> (does not discriminate in professional schools).</p>
<p>In this example, the alternative hypothesis is a <em>union</em>, viz.,</p>
<p>(discriminates in arts and humanities) <span class="math notranslate nohighlight">\(\cup\)</span> (discriminates in sciences) <span class="math notranslate nohighlight">\(\cup\)</span> (discriminates in engineering) <span class="math notranslate nohighlight">\(\cup\)</span> (discriminates in professional schools).</p>
<p>Framing a test this way leads to an <em>intersection-union test</em>.
The null hypothesis is the intersection</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   H_0 \equiv \cap_{j=1}^J H_{0j}
\end{equation*}\]</div>
<p>and the alternative is the union</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   H_1 \equiv \cup_{j=1}^J H_{0j}^c.
\end{equation*}\]</div>
<p>There can be good reasons for representating a null hypothesis as such an intersection.
In the example just mentioned, the applicant pool might be quite different across disciplines, making it hard to judge at the aggregate level whether there is discrimination, while testing within each discipline is more straightforward (that is, <em>Simpson’s Paradox</em> can be an issue).</p>
<p>Hypotheses about multivariate distributions can sometimes be expressed as the intersection of hypotheses about each dimension separately. For instance, the hypothesis that an <span class="math notranslate nohighlight">\(J\)</span>-dimensional distribution has zero mean could be represented as</p>
<p>(1st component has zero mean) <span class="math notranslate nohighlight">\(\cap\)</span> (2nd component has zero mean) <span class="math notranslate nohighlight">\(\cap\)</span> <span class="math notranslate nohighlight">\(\cdots\)</span> <span class="math notranslate nohighlight">\(\cap\)</span> (<span class="math notranslate nohighlight">\(J\)</span>th component has zero mean)</p>
<p>The alternative is again a union:</p>
<p>(1st component has nonzero mean) <span class="math notranslate nohighlight">\(\cup\)</span> (2nd component has nonzero mean) <span class="math notranslate nohighlight">\(\cup\)</span> <span class="math notranslate nohighlight">\(\cdots\)</span> <span class="math notranslate nohighlight">\(\cup\)</span> (<span class="math notranslate nohighlight">\(J\)</span>th component has nonzero mean)</p>
</div>
<div class="section" id="combinations-of-experiments-and-stratified-experiments">
<h2>Combinations of experiments and stratified experiments<a class="headerlink" href="#combinations-of-experiments-and-stratified-experiments" title="Permalink to this headline">¶</a></h2>
<p>The same kind of issue arises when combining information from different experiments.
For instance, imagine testing whether a drug is effective. We might have several randomized, controlled trials in different places, or a large experiment involving a number of centers, each of which performs its own randomization (i.e., the randomization is stratified).</p>
<p>How can we combine the information from the separate (independent) experiments to test the null hypothesis that the drug is ineffective?</p>
<p>Again, the overall null hypothesis is “the drug doesn’t help,” which can be written as an intersection of hypotheses</p>
<p>(drug doesn’t help in experiment 1) <span class="math notranslate nohighlight">\(\cap\)</span> (drug doesn’t help in experiment 2) <span class="math notranslate nohighlight">\(\cap\)</span> <span class="math notranslate nohighlight">\(\cdots\)</span>  <span class="math notranslate nohighlight">\(\cap\)</span> (drug doesn’t help in experiment <span class="math notranslate nohighlight">\(J\)</span>),</p>
<p>and the alternative can be written as</p>
<p>(drug helps in experiment 1) <span class="math notranslate nohighlight">\(\cup\)</span> (drug helps in experiment 2) <span class="math notranslate nohighlight">\(\cup\)</span> <span class="math notranslate nohighlight">\(\cdots\)</span>  <span class="math notranslate nohighlight">\(\cup\)</span> (drug helps in experiment <span class="math notranslate nohighlight">\(J\)</span>),</p>
<p>a union.</p>
</div>
<div class="section" id="combining-evidence">
<h2>Combining evidence<a class="headerlink" href="#combining-evidence" title="Permalink to this headline">¶</a></h2>
<p>Suppose we have a test of each “partial” null hypothesis <span class="math notranslate nohighlight">\(H_{0j}\)</span>. Clearly, if the <span class="math notranslate nohighlight">\(P\)</span>-value for one of those tests is sufficiently small, that’s evidence that the overall null <span class="math notranslate nohighlight">\(H_0\)</span> is false.</p>
<p>But suppose none of the individual <span class="math notranslate nohighlight">\(P\)</span>-values is small, but many are “not large.”
Is there a way to combine them to get sronger evidence about <span class="math notranslate nohighlight">\(H_0\)</span>?</p>
</div>
<div class="section" id="combining-functions">
<h2>Combining functions<a class="headerlink" href="#combining-functions" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\lambda\)</span> be a <span class="math notranslate nohighlight">\(J\)</span>-vector of statistics such that the distribution of <span class="math notranslate nohighlight">\(\lambda_j\)</span>
if hypothesis <span class="math notranslate nohighlight">\(H_{0j}\)</span> is true is known.
We assume that smaller values of <span class="math notranslate nohighlight">\(\lambda_j\)</span> are stronger evidence against <span class="math notranslate nohighlight">\(H_{0j}\)</span>.
For instance, <span class="math notranslate nohighlight">\(\lambda_j\)</span> might be the <span class="math notranslate nohighlight">\(P\)</span>-value of <span class="math notranslate nohighlight">\(H_{0j}\)</span> for some test.</p>
<p>Consider a function</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi: [0, 1]^J \rightarrow \Re; \lambda = (\lambda_1, \ldots, \lambda_J) \mapsto \phi(\lambda)
\end{equation*}\]</div>
<p>with the properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> is non-increasing in every argument, i.e., <span class="math notranslate nohighlight">\(\phi( \ldots, \lambda_j, \ldots) \ge \phi(( \ldots, \lambda_j', \ldots)\)</span> if <span class="math notranslate nohighlight">\(\lambda_j \le \lambda_j'\)</span>, <span class="math notranslate nohighlight">\(j = 1, \ldots, J\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> attains its maximum if any of its arguments equals 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi\)</span> attains its minimum if all of its arguments equal 1.</p></li>
<li><p>for all <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>, there exist finite functions <span class="math notranslate nohighlight">\(\phi_-(\alpha)\)</span>, <span class="math notranslate nohighlight">\(\phi_+(\alpha)\)</span> such that if every partial null hypothesis <span class="math notranslate nohighlight">\(\{H_{0j}\}\)</span> is true,</p></li>
</ul>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} \Pr \{\phi_-(\alpha) \le \phi(\lambda) \le \phi_+(\alpha) \} \ge 1-\alpha\end{equation*}\]</div>
<p>and <span class="math notranslate nohighlight">\([\phi_-(\alpha), \phi_+(\alpha)] \subset [\phi_-(\alpha'), \phi_+(\alpha')]\)</span> if <span class="math notranslate nohighlight">\(\alpha \ge \alpha'\)</span>.</p>
<p>Then we can use <span class="math notranslate nohighlight">\(\phi(\lambda)\)</span> as the basis of a test of <span class="math notranslate nohighlight">\(H_0 = \cap_{j=1}^J H_{0j}\)</span>.</p>
<div class="section" id="fisher-s-combining-function">
<h3>Fisher’s combining function<a class="headerlink" href="#fisher-s-combining-function" title="Permalink to this headline">¶</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi_F(\lambda) \equiv -2 \sum_{j=1}^J \ln(\lambda_j).\end{equation*}\]</div>
</div>
<div class="section" id="liptak-s-combining-function">
<h3>Liptak’s combining function<a class="headerlink" href="#liptak-s-combining-function" title="Permalink to this headline">¶</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi_L(\lambda) \equiv \sum_{j=1}^J \Phi^{-1}(1-\lambda_j),\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> is the inverse standard normal CDF.</p>
</div>
<div class="section" id="tippet-s-combining-function">
<h3>Tippet’s combining function<a class="headerlink" href="#tippet-s-combining-function" title="Permalink to this headline">¶</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi_T(\lambda) \equiv \max_{j=1}^J (1-\lambda_j).\end{equation*}\]</div>
</div>
<div class="section" id="direct-combination-of-test-statistics">
<h3>Direct combination of test statistics<a class="headerlink" href="#direct-combination-of-test-statistics" title="Permalink to this headline">¶</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}  \phi_D \equiv \sum_{j=1}^J f_j(\lambda_j), \end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\{ f_j \}\)</span> are suitable decreasing functions. For instance, if <span class="math notranslate nohighlight">\(\lambda_j\)</span> is the <span class="math notranslate nohighlight">\(P\)</span>-value for <span class="math notranslate nohighlight">\(H_{0j}\)</span> corresponding to some test statistic <span class="math notranslate nohighlight">\(T_j\)</span> for which larger values are stronger evidence against <span class="math notranslate nohighlight">\(H_{0j}\)</span>, we could use <span class="math notranslate nohighlight">\(\phi_D = \sum_j T_j\)</span>.</p>
</div>
</div>
<div class="section" id="fisher-s-combining-function-for-independent-p-values">
<h2>Fisher’s combining function for independent <span class="math notranslate nohighlight">\(P\)</span>-values<a class="headerlink" href="#fisher-s-combining-function-for-independent-p-values" title="Permalink to this headline">¶</a></h2>
<p>Suppose <span class="math notranslate nohighlight">\(H_0\)</span> is true, that <span class="math notranslate nohighlight">\(\lambda_j\)</span> is the <span class="math notranslate nohighlight">\(P\)</span>-value of <span class="math notranslate nohighlight">\(H_{0j}\)</span> for some pre-specified test, that the distribution of <span class="math notranslate nohighlight">\(\lambda_j\)</span> is continuous under <span class="math notranslate nohighlight">\(H_{0j}\)</span>, and that <span class="math notranslate nohighlight">\(\{ \lambda_j \}\)</span> are independent if <span class="math notranslate nohighlight">\(H_0\)</span> is true.</p>
<p>Then, if <span class="math notranslate nohighlight">\(H_0\)</span> is true, <span class="math notranslate nohighlight">\(\{ \lambda_j \}\)</span> are IID <span class="math notranslate nohighlight">\(U[0,1]\)</span>.</p>
<p>Under <span class="math notranslate nohighlight">\(H_{0j}\)</span>, the distribution of <span class="math notranslate nohighlight">\(-\ln \lambda_j\)</span> is exponential(1):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \Pr \{ -\ln \lambda_j \le x \} = \Pr \{ \ln \lambda_j \ge -x \} = \Pr \{ \lambda_j \ge e^{-x} \} = 1 - e^{-x}.
\end{equation*}\]</div>
<p>The distribution of 2 times an exponential is <span class="math notranslate nohighlight">\(\chi_2^2\)</span>:
the pdf of a chi-square with <span class="math notranslate nohighlight">\(k\)</span> degrees of freedom is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \frac{1}{2^{k/2}\Gamma(k/2)} x^{k/2-1} e^{-x/2}.
\end{equation*}\]</div>
<p>For <span class="math notranslate nohighlight">\(k=2\)</span>, this simplifies to <span class="math notranslate nohighlight">\(e^{-x/2}/2\)</span>, the exponential density scaled by a factor of 2.</p>
<p>Thus, under <span class="math notranslate nohighlight">\(H_0\)</span>, <span class="math notranslate nohighlight">\(\phi_F(\lambda)\)</span> is the sum of <span class="math notranslate nohighlight">\(J\)</span> independent <span class="math notranslate nohighlight">\(\chi_2^2\)</span> random variables. The distribution of a sum of independent chi-square random variables is a chi-square random variable with degrees of freedom equal to the sum of the degrees of freedom of the variables that were added.</p>
<p>Hence, under <span class="math notranslate nohighlight">\(H_0\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \phi_F(\lambda) \sim \chi_{2J}^2,
\end{equation*}\]</div>
<p>the chi-square distribution with <span class="math notranslate nohighlight">\(2n\)</span> degrees of freedom.</p>
<p>Let <span class="math notranslate nohighlight">\(\chi_{k}^2(\alpha)\)</span> denote the <span class="math notranslate nohighlight">\(1-\alpha\)</span> quantile of the chi-square distribution
with <span class="math notranslate nohighlight">\(k\)</span> degrees of freedom.
If we reject <span class="math notranslate nohighlight">\(H_0\)</span> when</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \phi_F(\lambda) \ge \chi_{2J}^2(\alpha),
\end{equation*}\]</div>
<p>that yields a significance level <span class="math notranslate nohighlight">\(\alpha\)</span> test of <span class="math notranslate nohighlight">\(H_0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Simulate distribution of Fisher&#39;s combining function when all nulls are true</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.polynomial</span> <span class="kn">import</span> <span class="n">polynomial</span> <span class="k">as</span> <span class="n">P</span>

<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span><span class="p">,</span> <span class="n">binom</span>

<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span><span class="p">,</span> <span class="n">display</span><span class="p">,</span> <span class="n">HTML</span>

<span class="k">def</span> <span class="nf">plot_fisher_null</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">reps</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">reps</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">U</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">reps</span><span class="o">/</span><span class="mi">40</span><span class="p">),</span> <span class="mi">5</span><span class="p">),</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;simulation&quot;</span><span class="p">)</span>
    <span class="n">mxv</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mxv</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">chi2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;chi-square pdf, df=&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interact</span><span class="p">(</span><span class="n">plot_fisher_null</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "8eb6a058573046df8b1547ee8d908678", "version_major": 2, "version_minor": 0}
</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function __main__.plot_fisher_null(n=5, reps=10000)&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="when-p-values-have-atoms">
<h2>When <span class="math notranslate nohighlight">\(P\)</span>-values have atoms<a class="headerlink" href="#when-p-values-have-atoms" title="Permalink to this headline">¶</a></h2>
<p>A real random variable <span class="math notranslate nohighlight">\(X\)</span> is first-order stochastically larger than a real random variable <span class="math notranslate nohighlight">\(Y\)</span> if for all <span class="math notranslate nohighlight">\(x \in \Re\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \Pr \{ X \ge x \} \ge \Pr \{ Y \ge x \},
\end{equation*}\]</div>
<p>with strict inequality for some <span class="math notranslate nohighlight">\(x \in \Re\)</span>.</p>
<p>Suppose <span class="math notranslate nohighlight">\(\{\lambda_j \}\)</span> for <span class="math notranslate nohighlight">\(\{ H_{0j}\}\)</span> satisfy</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \Pr \{ \lambda_j \le p  || H_{0j} \} \le p.
\end{equation*}\]</div>
<p>This takes into account the possibility that <span class="math notranslate nohighlight">\(\lambda_j\)</span> does not have a continuous
distribution under <span class="math notranslate nohighlight">\(H_{0j}\)</span>, ensuring that <span class="math notranslate nohighlight">\(\lambda_j\)</span> is still a <em>conservative</em> <span class="math notranslate nohighlight">\(P\)</span>-value.</p>
<p>Since <span class="math notranslate nohighlight">\(\ln\)</span> is monotone, it follows that for all <span class="math notranslate nohighlight">\(x \in \Re\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \Pr \{ -2 \ln \lambda_j \ge x \} \le \Pr \{ -2 \ln U \ge x \}.
\end{equation*}\]</div>
<p>That is, if <span class="math notranslate nohighlight">\(\lambda_j\)</span> does not have a continuous distribution,
the a <span class="math notranslate nohighlight">\(\chi_2^2\)</span> variable is stochastically larger than the distribution of <span class="math notranslate nohighlight">\(-2\ln \lambda_j\)</span>.</p>
<p>It turns out that <span class="math notranslate nohighlight">\(X\)</span> is stochastically larger than <span class="math notranslate nohighlight">\(Y\)</span> if and only if
there is some probability space on which there exist
two random variables, <span class="math notranslate nohighlight">\(\tilde{X}\)</span> and <span class="math notranslate nohighlight">\(\tilde{Y}\)</span> such that <span class="math notranslate nohighlight">\(\tilde{X} \sim X\)</span>,
<span class="math notranslate nohighlight">\(\tilde{Y} \sim Y\)</span>, and <span class="math notranslate nohighlight">\(\Pr \{\tilde{X} \ge \tilde{Y} \} = 1\)</span>.
(See, e.g., Grimmett and Stirzaker,<em>Probability and Random Processes</em>, 3rd edition,
Theorem 4.12.3.)</p>
<p>Let <span class="math notranslate nohighlight">\(\{X_j\}_{j=1}^n\)</span> be IID <span class="math notranslate nohighlight">\(\chi_2^2\)</span> random variables,
and let <span class="math notranslate nohighlight">\(Y_j \equiv - 2 \ln \lambda_j\)</span>, <span class="math notranslate nohighlight">\(j=1, \ldots, J\)</span>.</p>
<p>Then there is some probability space
for which we can define <span class="math notranslate nohighlight">\(\{\tilde{Y_j}\}\)</span> and <span class="math notranslate nohighlight">\(\{\tilde{X_j}\}\)</span> such that</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\tilde{Y_j})\)</span> has the same joint distribution as <span class="math notranslate nohighlight">\((Y_j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((\tilde{X_j})\)</span> has the same joint distribution as <span class="math notranslate nohighlight">\((X_j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\tilde{X_j} \ge \tilde{Y_j}\)</span> for all <span class="math notranslate nohighlight">\(j\)</span> with probability one.</p></li>
</ul>
<p>Then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sum_j \tilde{Y_j}\)</span> has the same distribution as <span class="math notranslate nohighlight">\(\sum_j Y_j = -2 \sum_j \ln \lambda_j\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_j \tilde{X_j}\)</span> has the same distribution as <span class="math notranslate nohighlight">\(\sum_j X_j\)</span> (namely, chi-square with <span class="math notranslate nohighlight">\(2J\)</span> degrees of freedom),</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_j \tilde X_j  \ge \sum_j \tilde{Y_j}\)</span>.</p></li>
</ul>
<p>That is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \Pr \left \{-2 \sum_j \ln \lambda_j \ge \chi_{2J}^2(\alpha) \right \} \le \alpha.
\end{equation*}\]</div>
<p>Thus, we still get a conservative hypothesis test if one or more of the <span class="math notranslate nohighlight">\(p\)</span>-values for the
partial tests have atoms under their respective null hypotheses <span class="math notranslate nohighlight">\(\{H_{0j}\}\)</span>.</p>
<div class="section" id="estimating-p-values-by-simulation">
<h3>Estimating <span class="math notranslate nohighlight">\(P\)</span>-values by simulation<a class="headerlink" href="#estimating-p-values-by-simulation" title="Permalink to this headline">¶</a></h3>
<p>Suppose that if the null hypothesis is true, the probability distribution of the
data is invariant under some group <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, for instance, the reflection group or the symmetric (i.e., permutation) group.</p>
<p>For any pre-specified test statistic <span class="math notranslate nohighlight">\(T\)</span>, we can estimate a <span class="math notranslate nohighlight">\(P\)</span>-value by generating uniformly distributed random elements of the orbit of the data under the action of the group (see <a class="reference internal" href="math-foundations.html"><span class="doc std std-doc">Mathematical Fundations</span></a> if these notions are unfamiliar).</p>
<p>Suppose we generate <span class="math notranslate nohighlight">\(n\)</span> random elements of the orbit.
Let <span class="math notranslate nohighlight">\(x_0\)</span> denote the original data; let <span class="math notranslate nohighlight">\(\{\pi_k\}_{k=1}^K\)</span> denote IID random elements of
<span class="math notranslate nohighlight">\(\mathcal{G}\)</span> and <span class="math notranslate nohighlight">\(x_k = \pi_k(x_0)\)</span>, <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span> denote <span class="math notranslate nohighlight">\(K\)</span> random elements of the
orbit of <span class="math notranslate nohighlight">\(x_0\)</span> under <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>.</p>
<p>An unbiased estimate of the <span class="math notranslate nohighlight">\(P\)</span>-value (assuming that the random elements are generated uniformly at random–see <a class="reference internal" href="permute-sample.html"><span class="doc std std-doc">Algorithms for Pseudo-Random Sampling</span></a> for a caveats), is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \hat{P} = \frac{\#\{ k&gt;0: T(\pi_k(x_0)) \ge T(x_0)\}}{K}.
\end{equation*}\]</div>
<p>Once <span class="math notranslate nohighlight">\(x_0\)</span> is known, the events <span class="math notranslate nohighlight">\(\{T(\pi_j(x_0)) \ge T(x_0)\}\)</span> are IID with probability
<span class="math notranslate nohighlight">\(P\)</span> of occurring, and <span class="math notranslate nohighlight">\(\hat{P}\)</span> is an unbiased estimate of <span class="math notranslate nohighlight">\(P\)</span>.</p>
<p>Another estimate of <span class="math notranslate nohighlight">\(P\)</span>, arguably preferable (as discussed below), is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \hat{P}' = \frac{\#\{ k \ge 0: T(\pi_k(x_0)) \ge T(x_0)\}}{K+1},
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_0\)</span> is the identity permutation.</p>
<p>The reasoning behind this choice is that, if the null hypothesis is true, the original data are one of the equally likely elements of the orbit of the data–exactly as likely as the elements generated from it.
Thus there are really <span class="math notranslate nohighlight">\(K+1\)</span> values that are equally likely if the null is true,
rather than <span class="math notranslate nohighlight">\(K\)</span>: nature provided one more random permutation, the original data.
The estimate <span class="math notranslate nohighlight">\(\hat{P}'\)</span> is never smaller than <span class="math notranslate nohighlight">\(1/(K+1)\)</span>.
Some practitioners like this because it never estimates the <span class="math notranslate nohighlight">\(P\)</span>-value to be zero.
There are other reasons for preferring it, discussed below.</p>
<p>The estimate <span class="math notranslate nohighlight">\(\hat{P}'\)</span> of <span class="math notranslate nohighlight">\(P\)</span> is generally biased, however, since <span class="math notranslate nohighlight">\(\hat{P}\)</span> is unbiased and</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \hat{P}' = \frac{K\hat{P} + 1}{K+1} = \frac{K}{K+1} \hat{P} + \frac{1}{K+1},
\end{equation*}\]</div>
<p>so</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   \mathbb{E} \hat{P}' = \frac{K}{K+1} P + \frac{1}{K+1} =
   P  + (1-P) \frac{1}{K+1} &gt; P.
\end{equation*}\]</div>
</div>
</div>
<div class="section" id="accounting-for-simulation-error-in-stratum-wise-p-values">
<h2>Accounting for simulation error in stratum-wise <span class="math notranslate nohighlight">\(P\)</span>-values<a class="headerlink" href="#accounting-for-simulation-error-in-stratum-wise-p-values" title="Permalink to this headline">¶</a></h2>
<p>Suppose that the <span class="math notranslate nohighlight">\(P\)</span>-value <span class="math notranslate nohighlight">\(\lambda_j\)</span> for <span class="math notranslate nohighlight">\(H_{0j}\)</span> is estimated by <span class="math notranslate nohighlight">\(b_j\)</span> simulations instead of being known exactly.
How can we take the uncertainty of the simulation estimate into account?</p>
<p>Here, we will pretend that the simulation itself is perfect: that the PRNG generates true IID <span class="math notranslate nohighlight">\(U[0,1]\)</span> variables, that pseudo-random integers on <span class="math notranslate nohighlight">\(\{0, 1, \ldots, N\}\)</span> really are equally likely, and that pseudo-random samples or permutations really are equally likely, etc.</p>
<p>The error we are accounting for is not the imperfection of the PRNG or other algorithms, just the uncertainty due to approximating a theoretical probability <span class="math notranslate nohighlight">\(\lambda_j\)</span> by an estimate via (perfect) simulation.</p>
<div class="section" id="a-crude-approach-simultaneous-one-sided-upper-confidence-bounds-for-every-lambda-j">
<h3>A crude approach: simultaneous one-sided upper confidence bounds for every <span class="math notranslate nohighlight">\(\lambda_j\)</span><a class="headerlink" href="#a-crude-approach-simultaneous-one-sided-upper-confidence-bounds-for-every-lambda-j" title="Permalink to this headline">¶</a></h3>
<p>Suppose we find, for each <span class="math notranslate nohighlight">\(j\)</span>, an upper confidence bound for <span class="math notranslate nohighlight">\(\lambda_j\)</span> (the “true” <span class="math notranslate nohighlight">\(P\)</span>-value in stratum <span class="math notranslate nohighlight">\(j\)</span>),
for instance, by inverting binomial tests based on <span class="math notranslate nohighlight">\(\# \{k &gt; 0: T(\pi_k(x_0)) \ge T(x_0) \}\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\phi\)</span> is monotonic in every coordinate, the upper confidence confidence bounds
for <span class="math notranslate nohighlight">\(\{ \lambda_j \}\)</span> imply a lower confidence bound for <span class="math notranslate nohighlight">\(\phi(\lambda)\)</span>, which translates to an upper confidence bound for the combined <span class="math notranslate nohighlight">\(P\)</span>-value.</p>
<p>What is the confidence level of the bound on the combined <span class="math notranslate nohighlight">\(P\)</span>-value?
If the <span class="math notranslate nohighlight">\(P\)</span>-value estimates are independent, the joint coverage probability of a set of <span class="math notranslate nohighlight">\(n\)</span> independent confidence bounds with confidence level <span class="math notranslate nohighlight">\(\alpha\)</span> is <span class="math notranslate nohighlight">\(1-(1-\alpha)^n\)</span>, as we shall show.</p>
<p>Let <span class="math notranslate nohighlight">\(A_j\)</span> denote the event that the upper confidence bound for <span class="math notranslate nohighlight">\(\lambda_j\)</span> is greater than or equal to <span class="math notranslate nohighlight">\(\lambda_j\)</span>, and suppose <span class="math notranslate nohighlight">\(\Pr \{A_j\} = 1-\alpha_j\)</span>.</p>
<p>Regardless of the dependence among the events <span class="math notranslate nohighlight">\(\{A_j \}\)</span>, the chance that all of the confidence bounds cover their corresponding parameters can be bounded using Bonferroni’s inequality:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Pr \{ \cap_j A_j \} = 1 - \Pr \{ \cup_j A_j^c \} \ge 1 - \sum_j \Pr \{A_j^c \} 
   = 1 - \sum_j (1- \Pr \{A_j \}) = 1 - \sum_j \alpha_j.
\end{equation*}\]</div>
<p>If <span class="math notranslate nohighlight">\(\{A_j\}\)</span> are independent,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Pr \{ \cap_j A_j \} = \prod_j \Pr \{ A_j \} = \prod_j (1-\alpha_j).
\end{equation*}\]</div>
<p>Both of those expressions tend to get small quickly as <span class="math notranslate nohighlight">\(n\)</span> gets large;
bounding <span class="math notranslate nohighlight">\(\phi(\lambda)\)</span> by bounding the components of <span class="math notranslate nohighlight">\(\lambda\)</span> is inefficient.</p>
<p>Let’s look for a different approach.</p>
</div>
<div class="section" id="a-sharper-approach-use-a-related-randomized-test">
<h3>A sharper approach: use a related randomized test<a class="headerlink" href="#a-sharper-approach-use-a-related-randomized-test" title="Permalink to this headline">¶</a></h3>
<p>This section presents a different approach, based on <span class="math notranslate nohighlight">\(\hat{P}'\)</span> (the biased estimate of <span class="math notranslate nohighlight">\(P\)</span>) rather than
<span class="math notranslate nohighlight">\(\hat{P}\)</span>.
It yields a surprisingly simple and elegant conservative test.</p>
<p>The key is to change the test itself: instead of treating <span class="math notranslate nohighlight">\(\hat{\lambda}_j\)</span> or <span class="math notranslate nohighlight">\(\hat{\lambda}_j'\)</span> as an estimate of
<span class="math notranslate nohighlight">\(\lambda_j\)</span>–the <span class="math notranslate nohighlight">\(P\)</span>-value for <span class="math notranslate nohighlight">\(H_{0j}\)</span> for the original test–we define a <em>new</em> test based on</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
  \hat{\lambda}_j' \equiv \frac{\#\{k \ge 0: T(\pi_k(x_0) \ge T(x_0)\}}{K_j+1},
\end{equation*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_0\)</span> is the identity permutation and <span class="math notranslate nohighlight">\(\{ \pi_k \}_{k=1}^{K_j}\)</span> are elements of <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>
selected at random uniformly.</p>
<p>While <span class="math notranslate nohighlight">\(\hat{\lambda}_j'\)</span> is a biased estimate of <span class="math notranslate nohighlight">\(\lambda_j\)</span>, we shall see that <strong>it is itself a valid conditional <span class="math notranslate nohighlight">\(P\)</span>-value</strong>; that is,  <span class="math notranslate nohighlight">\(\Pr \{ \hat{\lambda}_j' \le p \} \le p\)</span>, given that the data are in the orbit of <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>Note that this (conditional) probability involved has two sources of randomness:</p>
<ol class="simple">
<li><p>The randomness in the original data, <span class="math notranslate nohighlight">\(x_0\)</span> (although we condition on the event that the data fall in the orbit of <span class="math notranslate nohighlight">\(x_0\)</span>)</p></li>
<li><p>The randomness in generating the random transformations <span class="math notranslate nohighlight">\(\{ \pi_k \}_{k=1}^{K_j} \subset \mathcal{G}\)</span></p></li>
</ol>
<p>The resulting hypothesis test is a <em>randomized test</em>: it uses auxilliary randomness
in addition to the randomness in the data.
If the experiment were repeated and the data turned out to be <span class="math notranslate nohighlight">\(x_0\)</span> again,
the test will in general give a different <span class="math notranslate nohighlight">\(P\)</span> value: <span class="math notranslate nohighlight">\(\hat{\lambda}_j'\)</span> is random even if <span class="math notranslate nohighlight">\(x_0\)</span> is known.
The decision to reject the null hypothesis (or not) is random even after the data have been observed.</p>
<p>Randomized tests have a number of desirable theoretical properties (related to continuity and convexity),
but they are rarely used explicitly in practice.
Tests involving simulated <span class="math notranslate nohighlight">\(P\)</span>-values are an example where randomized tests are used implicitly rather than explicitly–generally without recognizing that the resulting test is randomized.</p>
<p>This section shows that the randomization involved in simulating <span class="math notranslate nohighlight">\(P\)</span>-values can be taken into account explicitly to get a conservative test.</p>
<p>By construction, <span class="math notranslate nohighlight">\(\{\pi_k(x_0)\}_{k=1}^{K_j}\)</span> are IID uniformly distributed on the orbit of <span class="math notranslate nohighlight">\(x_0\)</span>.
(We are ignoring imperfections in the PRNG and other algorithms.)
Thus, <span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span> are IID random variables.
The event <span class="math notranslate nohighlight">\(\hat{\lambda}_j' \le p\)</span> is the event that <span class="math notranslate nohighlight">\(T(x_0)= T(\pi_0(x_0))\)</span> is
larger than all but (at most) <span class="math notranslate nohighlight">\((K_j+1)p\)</span> of the values <span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span>.
Under the null, <span class="math notranslate nohighlight">\(T(x_0)\)</span> is equally likely to be any of them.</p>
<p>Let <span class="math notranslate nohighlight">\(p' = \lfloor (K_j+1)p \rfloor /(K_j+1)\)</span>. Then <span class="math notranslate nohighlight">\(p' \le p\)</span> and <span class="math notranslate nohighlight">\((K_j+1)p'\)</span> is an integer.
Sort the values <span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span> from largest to smallest, breaking ties arbitrarily.
Consider the <span class="math notranslate nohighlight">\((K_j+1)p'\)</span>th element of the list.
If it is strictly greater than the <span class="math notranslate nohighlight">\((K_j+1)p'+1\)</span>st element of the list, then there are <span class="math notranslate nohighlight">\((K_j+1)p'\)</span> permutations
<span class="math notranslate nohighlight">\(\pi_k\)</span> for which <span class="math notranslate nohighlight">\(T(\pi_k(x_0))\)</span> is strictly greater than all but <span class="math notranslate nohighlight">\((K_j+1)p\)</span> of the values
<span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span>.
If the <span class="math notranslate nohighlight">\((K_j+1)p'\)</span>th element of the list is equal to the <span class="math notranslate nohighlight">\((K_j+1)p'+1\)</span> element, then there are
<em>fewer</em> than <span class="math notranslate nohighlight">\((K_j+1)p'\)</span> such permutations.
Either way, the chance that a randomly selected element of the multiset <span class="math notranslate nohighlight">\(\{T(\pi_k(x_0))\}_{k=0}^{K_j}\)</span>
is strictly greater than all but at most <span class="math notranslate nohighlight">\((K_j+1)p'\)</span> of the elements is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
\Pr \{ \hat{\lambda}_j' \le p \} = \Pr \{ \hat{\lambda}_j' \le p' \} \le \frac{(K_j+1)p'}{K_j+1} = p' \le p.
\end{equation*}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\Pr \{\hat{\lambda}_j' \le p \} \le p\)</span>.
That is, <strong><span class="math notranslate nohighlight">\(\hat{\lambda}_j\)</span> is <em>itself</em> a conservative <span class="math notranslate nohighlight">\(P\)</span>-value</strong> for a randomized test, separate from the fact that it is a (biased) estimate of <span class="math notranslate nohighlight">\(\lambda_j\)</span>, the <span class="math notranslate nohighlight">\(P\)</span>-value for a related non-randomized test.</p>
<p>The test is defined implicitly: reject <span class="math notranslate nohighlight">\(H_{0j}\)</span> at significance level <span class="math notranslate nohighlight">\(\alpha\)</span> if
<span class="math notranslate nohighlight">\(\hat{\lambda}_j' \le \alpha\)</span>.</p>
<p>It follows that applying Fisher’s combining function to <span class="math notranslate nohighlight">\(\hat{\lambda}' = (\hat{\lambda}_j')_{j=1}^J\)</span> gives a conservative test of the intersection null hypothesis.</p>
</div>
</div>
<div class="section" id="dependent-tests">
<h2>Dependent tests<a class="headerlink" href="#dependent-tests" title="Permalink to this headline">¶</a></h2>
<p>If <span class="math notranslate nohighlight">\(\{ \lambda_j \}_{j=1}^J\)</span> are dependent, the distribution of <span class="math notranslate nohighlight">\(\phi_F(\lambda)\)</span> is no longer chi-square when the null hypotheses are true.
Nonetheless, one can calibrate a test based on Fisher’s combining function (or any other combining function) by simulation.
This is commonly used in multivariate permutation tests involving dependent partial tests
using “lockstep” permutations.</p>
<p>See, e.g., Pesarin, F. and L. Salmaso, 2010. <em>Permutation Tests for Complex Data: Theory, Applications and Software</em>, Wiley, 978-0-470-51641-6.</p>
<p>We shall illustrate how the approach can be used to construct nonparametric multivariate tests from univariate tests to address for the two-sample problem (i.e., is there evidence that two samples come from different populations, or is it plausible
that they are a single population randomly divided into two groups?).
This is equivalent to testing whether treatment has an effect in a controlled, randomized study
in which the subjects who receive treatment are a simple random sample of the study group,
using the Neyman model for causal inference.
(The null hypothesis is that treatment makes no difference whatsoever: each subject’s response would
be the same whether that subject was assigned to treatment or to control, and without regard for the assignment of
other subjects to treatment or control.)</p>
<p>We have <span class="math notranslate nohighlight">\(N\)</span> subjects of whom <span class="math notranslate nohighlight">\(N_t\)</span> are treated and <span class="math notranslate nohighlight">\(N_c\)</span> are controls.
Each subject has a vector of <span class="math notranslate nohighlight">\(J\)</span> measurements.
For each of these <span class="math notranslate nohighlight">\(J\)</span> “dimensions” we have a test statistic <span class="math notranslate nohighlight">\(T_j\)</span> (for instance, the difference between the mean of the treated and the mean of the controls on that dimension–but we could use something else, and we don’t have to use the same test statistic for different dimensions).</p>
<p>Each <span class="math notranslate nohighlight">\(T_j\)</span> takes the responses of the treated and the controls on dimension <span class="math notranslate nohighlight">\(j\)</span> and yields a number.
We will assume that larger values of <span class="math notranslate nohighlight">\(T_j\)</span> are stronger evidence against the null hypothesis that for dimension <span class="math notranslate nohighlight">\(j\)</span> treatment doesn’t make any difference.</p>
<p>Let <span class="math notranslate nohighlight">\(T\)</span> denote the whole <span class="math notranslate nohighlight">\(J\)</span>-vector of test statistics. Let <span class="math notranslate nohighlight">\(t(0)\)</span> denote the observed value of the
<span class="math notranslate nohighlight">\(J\)</span>-vector of test statistics for the original data.</p>
<p>Now consider randomly re-labelling <span class="math notranslate nohighlight">\(N_t\)</span> of the <span class="math notranslate nohighlight">\(N\)</span> subjects as treated and the remaining <span class="math notranslate nohighlight">\(N_c\)</span> as controls
by simple random sampling, so that all subsets of size <span class="math notranslate nohighlight">\(N_t\)</span> are equally likely to be labeled “treated.”
Each re-labelling carries the subject’s entire <span class="math notranslate nohighlight">\(J\)</span>-vector of responses with it:
the dimensions are randomized “in lockstep.”</p>
<p>Let <span class="math notranslate nohighlight">\(t(k)\)</span> denote the observed <span class="math notranslate nohighlight">\(J\)</span>-vector of test statistics for the <span class="math notranslate nohighlight">\(k\)</span>th random allocation (i.e., the <span class="math notranslate nohighlight">\(k\)</span>th random permutation).
We permute the data <span class="math notranslate nohighlight">\(K\)</span> times in all, each yielding a <span class="math notranslate nohighlight">\(J\)</span>-vector <span class="math notranslate nohighlight">\(t(k)\)</span> of observed values of the test statistics. This gives <span class="math notranslate nohighlight">\(K+1\)</span> permutations in all, including the original data (the zeroth permutation), for which the vector is <span class="math notranslate nohighlight">\(t(0)\)</span>. Let <span class="math notranslate nohighlight">\(t_j(k)\)</span> denote the test statistic for dimension <span class="math notranslate nohighlight">\(j\)</span> for the <span class="math notranslate nohighlight">\(k\)</span>th permutation.</p>
<p>We now transform the <span class="math notranslate nohighlight">\(J\)</span> by <span class="math notranslate nohighlight">\((K+1)\)</span> matrix <span class="math notranslate nohighlight">\([t_j(k)]_{j=1}^J{}_{k=0}^K\)</span> to a corresponding matrix of <span class="math notranslate nohighlight">\(P\)</span>-values for the univariate permutation tests.</p>
<p>For <span class="math notranslate nohighlight">\(j=1, \ldots , J\)</span> and <span class="math notranslate nohighlight">\(k = 0, \ldots , K\)</span>, define</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
   P_j(k) \equiv \frac{\#\{ \ell \in \{0, \ldots, K \} : t_j(\ell) \ge t_j(k) \}}{K+1}.  
\end{equation*}\]</div>
<p>This is the simulated upper tail probability of the <span class="math notranslate nohighlight">\(k\)</span>th observed value of the <span class="math notranslate nohighlight">\(j\)</span>th test statistic
under the null hypothesis.</p>
<p>Think of the values of <span class="math notranslate nohighlight">\(P_j(k)\)</span> as a matrix.
Each column corresponds to a random permutation of the original data (the 0th column corresponds to the original data); each row corresponds to a dimension of measurement; each entry is a number between <span class="math notranslate nohighlight">\(1/(K+1)\)</span> and 1.</p>
<p>Now apply Fisher’s combining function <span class="math notranslate nohighlight">\(\phi_F\)</span> (or Tippett’s, or Stouffer’s, or anything else) to each column of
<span class="math notranslate nohighlight">\(J\)</span> numbers. That gives <span class="math notranslate nohighlight">\(K+1\)</span> numbers,
<span class="math notranslate nohighlight">\(f(k), k=0, \ldots , K\)</span>, one for each permutation of the data.
The overall “Non-Parametric Combination of tests” (NPC) <span class="math notranslate nohighlight">\(P\)</span>-value is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*} 
 P_{\mbox{NPC}} \equiv \frac{\#\{ k \in \{0, \ldots, K\} : f(k) \ge f(0) \}}{K+1}. 
\end{equation*}\]</div>
<p>This is the simulated lower tail probability of the observed value of the combining function under the randomization.</p>
<p>Ultimately, this whole thing is just a univariate permutation test that uses a complicated test statistic
<span class="math notranslate nohighlight">\(\phi_F\)</span>, which assigns one number to each permutation of the multivariate data.</p>
<p>Also see <a class="reference external" href="http://statlab.github.io/permute/">the permute Python package</a>.</p>
</div>
<div class="section" id="stratified-permutation-tests">
<h2>Stratified Permutation Tests<a class="headerlink" href="#stratified-permutation-tests" title="Permalink to this headline">¶</a></h2>
<p>Two examples:</p>
<ul class="simple">
<li><p>Boring, A., K. Ottoboni, and P.B. Stark, 2016. Student Evaluations of Teaching (Mostly) Do Not Measure Teaching Effectiveness, <em>ScienceOpen</em>, doi 10.14293/S2199-1006.1.SOR-EDU.AETBZC.v1</p></li>
<li><p>Hessler, M.,  D.M. Pöpping, H. Hollstein, H. Ohlenburg, P.H. Arnemann, C. Massoth, L.M. Seidel, A. Zarbock &amp; M. Wenk, 2018. Availability of cookies during an academic course session affects evaluation of teaching, <em>Medical Education, 52</em>, 1064–1072. doi 10.1111/medu.13627</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Philip Stark and Fernando Pérez<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>