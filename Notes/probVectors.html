
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Probability, Random Vectors, and the Multivariate Normal Distribution &#8212; Collaborative and Reproducible Data Science</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Collaborative and Reproducible Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../overview.html">
   Statistics 159/259, Spring 2021 Course Summary
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../schedule.html">
   Statistics 159/259: Weekly Plan
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Syllabus
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../syllabus.html">
   Syllabus for Statistics 159/259: Reproducible and Collaborative Statistical Data Science
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Homework Assignments
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Hw/hw01-background.html">
   1. Statistics 159/259, Homework 1.
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Lecture Notes
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01/index.html">
   Introduction to Git and Github
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   An Idiosyncratic Sample of Applied Statistics
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Notes/probVectors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Notes/probVectors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Probability, Random Vectors, and the Multivariate Normal Distribution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability">
     Probability
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-variables">
     Random variables
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exercises">
       Exercises
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jointly-distributed-random-variables">
     Jointly Distributed Random Variables
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectation">
     Expectation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-expectation">
       Calculating Expectation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance-standard-error-and-covariance">
     Variance,  Standard Error, and Covariance
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-vectors">
     Random Vectors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#right">
   \right )
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-multivariate-normal-distribution">
     The Multivariate Normal Distribution
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example">
       Example
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-central-limit-theorem">
     The Central Limit Theorem
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-inequalities-and-identities">
     Probability Inequalities and Identities
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-tail-integral-formula-for-expectation">
       The tail-integral formula for expectation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#jensen-s-inequality">
       Jensen’s Inequality
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#markov-s-chebychev-s-and-related-inequalities">
       Markov’s, Chebychev’s, and related inequalities
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-chernoff-bound">
       The Chernoff Bound
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hoeffding-s-inequality">
       Hoeffding’s Inequality
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hoeffding-s-other-inequality">
       Hoeffding’s Other Inequality
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bernstein-s-inequality">
       Bernstein’s Inequality
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="probability-random-vectors-and-the-multivariate-normal-distribution">
<h1>Probability, Random Vectors, and the Multivariate Normal Distribution<a class="headerlink" href="#probability-random-vectors-and-the-multivariate-normal-distribution" title="Permalink to this headline">¶</a></h1>
<div class="section" id="probability">
<h2>Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h2>
<p>This course assumes that you have already had an introduction to probability, random variables, expectation, and so on, so the treatment is very cursory.
The treatment is quite informal, omitting important technical concepts such as sigma-algebras,
measurability, and the like.</p>
<p>Modern probability theory starts with Kolmogorov’s Axioms; here is an informal startement of the axioms.
For more (but still a very informal treatment), see these chapters of SticiGui:
<a class="reference external" href="http://www.stat.berkeley.edu/~stark/SticiGui/Text/probabilityPhilosophy.htm">Probability: Philosophy and Mathematical Background</a>,
<a class="reference external" href="http://www.stat.berkeley.edu/~stark/SticiGui/Text/sets.htm">Set theory</a>,
and
<a class="reference external" href="http://www.stat.berkeley.edu/~stark/SticiGui/Text/probabilityAxioms.htm">Probability: Axioms and Fundaments</a>.</p>
<p>Let <span class="math notranslate nohighlight">\(S\)</span> denote the <em>outcome space</em>, the set of all possible outcomes of a random experiment,
and let <span class="math notranslate nohighlight">\(\{A_i\}_{i=1}^\infty\)</span> be subsets of <span class="math notranslate nohighlight">\(S\)</span>.
(Note that here <span class="math notranslate nohighlight">\(A\)</span> denotes a subset, not a matrix.)
Then any probability function <span class="math notranslate nohighlight">\({\mathbb P}\)</span> must satisfy these axioms:</p>
<ol class="simple">
<li><p>For every <span class="math notranslate nohighlight">\(A \subset S\)</span>, <span class="math notranslate nohighlight">\({\mathbb P}(A) \ge 0\)</span> (probabilities are nonnegative)</p></li>
<li><p><span class="math notranslate nohighlight">\({\mathbb P}(S) = 1\)</span> (the chance that <em>something</em> happens is 100%)</p></li>
<li><p>If <span class="math notranslate nohighlight">\(A_i \cap A_j = \emptyset\)</span> for <span class="math notranslate nohighlight">\(i \ne j\)</span>, then <span class="math notranslate nohighlight">\({\mathbb P} \cup_{i=1}^\infty A_i = \sum_{i=1}^\infty {\mathbb P}(A_i)\)</span>
(If a countable collection of events is <em>pairwise disjoint</em>, then the chance that any of the
events occurs is the sum of the chances that they occur individually.)</p></li>
</ol>
<p>These axioms have many useful consequences, among them that <span class="math notranslate nohighlight">\({\mathbb P}(\emptyset) = 0\)</span>, <span class="math notranslate nohighlight">\({\mathbb P}(A^c) = 1 - {\mathbb P}(A)\)</span>,
and <span class="math notranslate nohighlight">\({\mathbb P}(A \cup B) = {\mathbb P}(A) + {\mathbb P}(B) - {\mathbb P}(AB)\)</span>.</p>
<hr />
### Definitions
<p>Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be subsets of outcome space <span class="math notranslate nohighlight">\(S\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(AB = \emptyset\)</span>, then <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are <em>mutually exclusive</em>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\({\mathbb P}(AB) = {\mathbb P}(A){\mathbb P}(B)\)</span>, then <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are <em>independent</em>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\({\mathbb P}(B) &gt; 0\)</span>, then the <em>conditional probability of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span></em> is
<span class="math notranslate nohighlight">\({\mathbb P}(A | B) \equiv {\mathbb P}(AB)/{\mathbb P}(B)\)</span>.</p></li>
</ul>
<hr />
<p>Independence is an extremely specific relationship.
At one extreme, <span class="math notranslate nohighlight">\(AB = \emptyset\)</span>; then <span class="math notranslate nohighlight">\({\mathbb P}(AB) = 0 \le {\mathbb P}(A){\mathbb P}(B)\)</span>.
At another extreme, either <span class="math notranslate nohighlight">\(A\)</span> is a subset of <span class="math notranslate nohighlight">\(B\)</span> or vice versa; then <span class="math notranslate nohighlight">\({\mathbb P}(AB) = \min({\mathbb P}(A),{\mathbb P}(B)) \ge {\mathbb P}(A){\mathbb P}(B)\)</span>.
Independence lies at a precise point in between.</p>
</div>
<div class="section" id="random-variables">
<h2>Random variables<a class="headerlink" href="#random-variables" title="Permalink to this headline">¶</a></h2>
<p>Briefly, a <em>real-valued random variable</em> <span class="math notranslate nohighlight">\(X\)</span> can be characterized by its probability distribution, which specifies (for a suitable collection of subsets of the real line <span class="math notranslate nohighlight">\(\Re\)</span> that comprises a sigma-algebra), the chance that the value of <span class="math notranslate nohighlight">\(X\)</span> will be in each such subset.
There are technical requirements regarding  <em>measurability</em>, which generally we will ignore.
Perhaps the most natural mathematical setting for probability theory involves <em>Lebesgue integration</em>;
we will largely ignore the difference between a <em>Riemann integral</em> and a <em>Lebesgue integral</em>.</p>
<p>Let <span class="math notranslate nohighlight">\(P_X\)</span> denote the probability distribution of the random variable <span class="math notranslate nohighlight">\(X\)</span>.
Then if <span class="math notranslate nohighlight">\(A \subset \Re\)</span>, <span class="math notranslate nohighlight">\(P_X(A) = {\mathbb P} \{ X \in A \}\)</span>.
We write <span class="math notranslate nohighlight">\(X \sim P_X\)</span>,
pronounced “<span class="math notranslate nohighlight">\(X\)</span> is distributed as <span class="math notranslate nohighlight">\(P_X\)</span>” or “<span class="math notranslate nohighlight">\(X\)</span> has distribution <span class="math notranslate nohighlight">\(P_X\)</span>.”</p>
<p>If two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have the same distribution, we write <span class="math notranslate nohighlight">\(X \sim Y\)</span> and we say that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>
are <em>identically distributed</em>.</p>
<p>Real-valued random variables can be <em>continuous</em>, <em>discrete</em>, or <em>mixed (general)</em>.</p>
<p>Continuous random variables have <em>probability density functions</em> with respect to Lebesgue measure.
If <span class="math notranslate nohighlight">\(X\)</span> is a continuous random variables, there is some nonnegative function <span class="math notranslate nohighlight">\(f(x)\)</span>,
the probability density of <span class="math notranslate nohighlight">\(X\)</span>, such that
for any (suitable) set <span class="math notranslate nohighlight">\(A \subset \Re\)</span>,
$<span class="math notranslate nohighlight">\(
  {\mathbb P} \{ X \in A \} = \int_A f(x) dx.
\)</span><span class="math notranslate nohighlight">\(
Since \)</span>{\mathbb P} { X \in \Re } = 1<span class="math notranslate nohighlight">\(, it follows that \)</span>\int_{-\infty}^\infty f(x) dx = 1$.</p>
<p><em>Example.</em>
Let <span class="math notranslate nohighlight">\(f(x) = \lambda e^{-\lambda x}\)</span> for <span class="math notranslate nohighlight">\(x \ge 0\)</span>, and <span class="math notranslate nohighlight">\(f(x) = 0\)</span> otherwise.
Clearly <span class="math notranslate nohighlight">\(f(x) \ge 0\)</span>.
$<span class="math notranslate nohighlight">\(
  \int_{-\infty}^\infty f(x) dx = \int_0^\infty \lambda e^{-\lambda x} dx
  = - e^{-\lambda x}|_0^\infty = - 0 + 1 = 1.
\)</span><span class="math notranslate nohighlight">\(
Hence, \)</span>\lambda e^{-\lambda x}$ can be the probability density of a continuous random variable.
A random variable with this density is said to be <em>exponentially distributed</em>.
Exponentially distributed random variables are used to model radioactive decay and the failure
of items that do not “fatigue.” For instance, the lifetime of a semiconductor after an initial
“burn-in” period is often modeled as an exponentially distributed random variable.
It is also a common model for the occurrence of earthquakes (although it does not fit the data well).</p>
<p><em>Example.</em>
Let <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> be real numbers with <span class="math notranslate nohighlight">\(a &lt; b\)</span>, and let <span class="math notranslate nohighlight">\(f(x) = \frac{1}{b-a}\)</span>, <span class="math notranslate nohighlight">\(x \in [a, b]\)</span> and
<span class="math notranslate nohighlight">\(f(x)=0\)</span>, otherwise.
Then <span class="math notranslate nohighlight">\(f(x) \ge 0\)</span> and <span class="math notranslate nohighlight">\(\int_{-\infty}^\infty f(x) dx = \int_a^b \frac{1}{b-a} = 1\)</span>,
so <span class="math notranslate nohighlight">\(f(x)\)</span> can be the probability density function of a continuous random variable.
A random variable with this density is sad to be <em>uniformly distributed on the interval <span class="math notranslate nohighlight">\([a, b]\)</span></em>.</p>
<p>Discrete random variables assign all their probability to some <em>countable</em> set of points <span class="math notranslate nohighlight">\(\{x_i\}_{i=1}^n\)</span>,
where <span class="math notranslate nohighlight">\(n\)</span> might be infinite.
Discrete random variables have <em>probability mass functions</em>.
If <span class="math notranslate nohighlight">\(X\)</span> is a discrete random variable, there is a nonnegative function <span class="math notranslate nohighlight">\(p\)</span>, the probability mass function
of <span class="math notranslate nohighlight">\(X\)</span>, such that
for any set <span class="math notranslate nohighlight">\(A \subset \Re\)</span>,
$<span class="math notranslate nohighlight">\(
  {\mathbb P} \{X \in A \} = \sum_{i: x_i \in A} p(x_i).
\)</span><span class="math notranslate nohighlight">\(
The value \)</span>p(x_i) = {\mathbb P} {X = x_i}<span class="math notranslate nohighlight">\(, and \)</span>\sum_{i=1}^\infty p(x_i) = 1$.</p>
<p><em>Example.</em>
Let <span class="math notranslate nohighlight">\(x_i = i-1\)</span> for <span class="math notranslate nohighlight">\(i=1, 2, \ldots\)</span>, and let <span class="math notranslate nohighlight">\(p(x_i) = e^{-\lambda} \lambda^{x_i}/x_i!\)</span>.
Then <span class="math notranslate nohighlight">\(p(x_i) &gt; 0\)</span> and
$<span class="math notranslate nohighlight">\( 
\sum_{i=1}^\infty p(x_i) = e^{-\lambda} \sum_{j=0}^\infty \lambda^j/j! = e^{-\lambda} e^{\lambda} = 1.
\)</span><span class="math notranslate nohighlight">\(
Hence, \)</span>p(x)<span class="math notranslate nohighlight">\( is the probability mass function of a discrete random variable.
A random variable with this probability mass function is said to be _Poisson distributed (with parameter
\)</span>\lambda$)_.
Poisson-distributed random variables are often used to model rare events.</p>
<p><em>Example.</em>
Let <span class="math notranslate nohighlight">\(x_i = i\)</span> for <span class="math notranslate nohighlight">\(i=1, \ldots, n\)</span>, and let <span class="math notranslate nohighlight">\(p(x_i) = 1/n\)</span> and <span class="math notranslate nohighlight">\(p(x) = 0\)</span>, otherwise.
Then <span class="math notranslate nohighlight">\(p(x) \ge 0\)</span> and <span class="math notranslate nohighlight">\(\sum_{x_i} p(x_i) = 1\)</span>.
Hence, <span class="math notranslate nohighlight">\(p(x)\)</span> can be the probability mass function of a discrete random variable.
A random variable with this probability mass function is said to be <em>uniformly distributed on <span class="math notranslate nohighlight">\(1, \ldots, n\)</span></em>.</p>
<p><em>Example.</em>
Let <span class="math notranslate nohighlight">\(x_i = i-1\)</span> for <span class="math notranslate nohighlight">\(i=1, \ldots, n+1\)</span>, and let <span class="math notranslate nohighlight">\(p(x_i) = {n \choose x_i} p^{x_i} (1-p)^{n-x_i}\)</span>, and
<span class="math notranslate nohighlight">\(p(x) = 0\)</span> otherwise.
Then <span class="math notranslate nohighlight">\(p(x) \ge 0\)</span> and
$<span class="math notranslate nohighlight">\(
\sum_{x_i} p(x_i) = \sum_{j=0}^n {n \choose j} p^j (1-p)^{n-j} = 1,
\)</span><span class="math notranslate nohighlight">\(
by the binomial theorem.
Hence \)</span>p(x)<span class="math notranslate nohighlight">\( is the probability mass function of a discrete random variable.
A random variable with this probability mass function is said to be _binomially distributed
with parameters \)</span>n<span class="math notranslate nohighlight">\( and \)</span>p<span class="math notranslate nohighlight">\(_.
The number of successes in \)</span>n<span class="math notranslate nohighlight">\( independent trials that each have the same probability \)</span>p<span class="math notranslate nohighlight">\( of success
has a binomial distribution with parameters \)</span>n<span class="math notranslate nohighlight">\( and \)</span>p<span class="math notranslate nohighlight">\(
For instance, the number of times a fair die lands with 3 spots showing in 10 independent rolls has
a binomial distribution with parameters \)</span>n=10<span class="math notranslate nohighlight">\( and \)</span>p = 1/6$.</p>
<p>For general random variables, the chance that <span class="math notranslate nohighlight">\(X\)</span> is in some subset of <span class="math notranslate nohighlight">\(\Re\)</span> cannot be written as
a sum or as a Riemann integral; it is more naturally represented as a Lebesgue integral (with respect to
a measure other than Lebesgue measure).
For example, imagine a random variable <span class="math notranslate nohighlight">\(X\)</span> that has probability <span class="math notranslate nohighlight">\(\alpha\)</span> of being equal to zero;
and if <span class="math notranslate nohighlight">\(X\)</span> is not zero, it has a uniform distribution on the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>.
Such a random variable is neither continuous nor discrete.</p>
<p>Most of the random variables in this class are either discrete or continuous.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a random variable such that, for some constant <span class="math notranslate nohighlight">\(x_1 \in \Re\)</span>, <span class="math notranslate nohighlight">\({\mathbb P}(X = x_1) = 1\)</span>, <span class="math notranslate nohighlight">\(X\)</span>
is called a <em>constant random variable</em>.</p>
<div class="section" id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Show analytically that <span class="math notranslate nohighlight">\(\sum_{x_i} p(x_i) = \sum_{j=0}^n {n \choose j} p^j (1-p)^{n-j} = 1\)</span>.</p></li>
<li><p>Write an R script that verifies that equation numerically for <span class="math notranslate nohighlight">\(n=10\)</span>: for 1000 values of <span class="math notranslate nohighlight">\(p\)</span>
equispaced on the interval <span class="math notranslate nohighlight">\((0, 1)\)</span>, find the maximum absolute value of the difference between the sum and</p></li>
</ul>
<ol class="simple">
<li></li>
</ol>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\( \in (0, 1]\)</span>; let <span class="math notranslate nohighlight">\(x_i = 1, 2, \ldots\)</span>; and define <span class="math notranslate nohighlight">\(p(x_i) = (1-p)^{x_i-1}p\)</span>, and <span class="math notranslate nohighlight">\(p(x) = 0\)</span> otherwise. Show analytically that <span class="math notranslate nohighlight">\(p(x)\)</span> is the probability mass function of a discrete random variable.
(A random variable with this probability mass function is said to be <em>geometrically distributed
with parameter <span class="math notranslate nohighlight">\(p\)</span></em>.)</p></li>
<li><p>Starting from Kolmogorov’s three axioms, show that <span class="math notranslate nohighlight">\({\mathbb P}(A \cup B) = {\mathbb P}(A) + {\mathbb P}(B) - {\mathbb P}(AB)\)</span>.</p></li>
<li><p>Starting from Kolmogorov’s three axioms, show that
<span class="math notranslate nohighlight">\({\mathbb P}(A \cup B \cup C) = {\mathbb P}(A) + {\mathbb P}(B) + {\mathbb P}(C) - {\mathbb P}(AB) - {\mathbb P}(AC) - {\mathbb P}(BC) + {\mathbb P}(ABC)\)</span>.</p></li>
<li><p>Starting from Kolmogorov’s three axioms, show that <span class="math notranslate nohighlight">\({\mathbb P}(AB) \le \min({\mathbb P}(A),{\mathbb P}(B))\)</span>.</p></li>
</ul>
</div>
</div>
<div class="section" id="jointly-distributed-random-variables">
<h2>Jointly Distributed Random Variables<a class="headerlink" href="#jointly-distributed-random-variables" title="Permalink to this headline">¶</a></h2>
<p>Often we work with more than one random variable at a time.
Indeed, much of this course concerns <em>random vectors</em>, the components of which are individual
real-valued random variables.</p>
<p>The <em>joint probability distribution</em> of a collection of random variables <span class="math notranslate nohighlight">\(\{X_i\}_{i=1}^n\)</span> gives the probability that
the variables simultaneously fall in subsets of their possible values.
That is, for every (suitable) subset <span class="math notranslate nohighlight">\( A \in \Re^n\)</span>, the joint probability distribution of <span class="math notranslate nohighlight">\(\{X_i\}_{i=1}^n\)</span>
gives <span class="math notranslate nohighlight">\({\mathbb P} \{ (X_1, \ldots, X_n) \in A \}\)</span>.</p>
<p>An <em>event determined by the random variable <span class="math notranslate nohighlight">\(X\)</span></em> is an event of the form <span class="math notranslate nohighlight">\(X \in A\)</span>, where <span class="math notranslate nohighlight">\(A \subset \Re\)</span>.</p>
<p>An <em>event determined by the random variables <span class="math notranslate nohighlight">\(\{X_j\}_{j \in J}\)</span></em> is an event of the form
<span class="math notranslate nohighlight">\((X_j)_{j \in J} \in A\)</span>, where <span class="math notranslate nohighlight">\(A \subset \Re^{\#J}\)</span>.</p>
<p>Two random variables <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are <em>independent</em> if every event determined by <span class="math notranslate nohighlight">\(X_1\)</span> is independent
of every event determined by <span class="math notranslate nohighlight">\(X_2\)</span>.
If two random variables are not independent, they are <em>dependent</em>.</p>
<p>A collection of random variables <span class="math notranslate nohighlight">\(\{X_i\}_{i=1}^n\)</span> is <em>independent</em> if every event determined by every subset
of those variables is independent of every event determined by any disjoint subset of those variables.
If a collection of random variables is not independent, it is <em>dependent</em>.</p>
<p>Loosely speaking, a collection of random variables is independent if learning the values of some of them
tells you nothing about the values of the rest of them.
If learning the values of some of them tells you anything about the values of the rest of them,
the collection is dependent.</p>
<p>For instance, imagine tossing a fair coin twice and rolling a fair die.
Let <span class="math notranslate nohighlight">\(X_1\)</span> be the number of times the coin lands heads, and <span class="math notranslate nohighlight">\(X_2\)</span> be the number of spots that show on the die.
Then <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are independent: learning how many times the coin lands heads tells you nothing about what
the die did.</p>
<p>On the other hand, let <span class="math notranslate nohighlight">\(X_1\)</span> be the number of times the coin lands heads, and let <span class="math notranslate nohighlight">\(X_2\)</span> be the sum of the
number of heads and the number of spots that show on the die.
Then <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are dependent. For instance, if you know the coin landed heads twice, you know that the sum
of the number of heads and the number of spots must be at least 3.</p>
</div>
<div class="section" id="expectation">
<h2>Expectation<a class="headerlink" href="#expectation" title="Permalink to this headline">¶</a></h2>
<p>See <a class="reference external" href="http://www.stat.berkeley.edu/~stark/SticiGui/Text/expectation.htm">SticiGui: The Long Run and the Expected Value</a> for an elementary introduction to expectation.</p>
<p>The <em>expectation</em> or <em>expected value</em> of a random variable <span class="math notranslate nohighlight">\(X\)</span>, denoted <span class="math notranslate nohighlight">\({\mathbb E}X\)</span>, is a probability-weighted average of its possible values.
From a frequentist perspective, it is the long-run limit (in probabiity) of the average of its values in repeated experiments.
The expected value of a real-valued random variable (when it exists) is a fixed number, not a random value.
The expected value depends on the probability distribution of <span class="math notranslate nohighlight">\(X\)</span> but not on any realized value of <span class="math notranslate nohighlight">\(X\)</span>.
If two random variables have the same probability distribution, they have the same expected value.</p>
<hr />
### Properties of Expectation
<ul class="simple">
<li><p>For any real <span class="math notranslate nohighlight">\(\alpha \in \Re\)</span>, if <span class="math notranslate nohighlight">\({\mathbb P} \{X = \alpha\} = 1\)</span>, then <span class="math notranslate nohighlight">\({\mathbb E}X = \alpha\)</span>: the expected
value of a constant random variable is that constant.</p></li>
<li><p>For any real <span class="math notranslate nohighlight">\(\alpha \in \Re\)</span>, <span class="math notranslate nohighlight">\({\mathbb E}(\alpha X) = \alpha {\mathbb E}X\)</span>: scalar homogeneity.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are random variables, <span class="math notranslate nohighlight">\({\mathbb E}(X+Y) = {\mathbb E}X + {\mathbb E}Y\)</span>: additivity.</p></li>
</ul>
<hr />
<div class="section" id="calculating-expectation">
<h3>Calculating Expectation<a class="headerlink" href="#calculating-expectation" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a continuous real-valued random variable with density <span class="math notranslate nohighlight">\(f(x)\)</span>, then the expected value of <span class="math notranslate nohighlight">\(X\)</span> is
$<span class="math notranslate nohighlight">\(
   {\mathbb E}X = \int_{-\infty}^\infty x f(x) dx,
\)</span>$
provided the integral exists.</p>
<p><em>Example.</em>
Suppose <span class="math notranslate nohighlight">\(X\)</span> has density <span class="math notranslate nohighlight">\(f(x) = \frac{1}{b-a}\)</span> for <span class="math notranslate nohighlight">\(a \le x \le b\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.
Then <span class="math notranslate nohighlight">\( {\mathbb E}X = \int_{-\infty}^\infty x f(x) dx = \frac{1}{b-a} \int_a^b x dx = \frac{b^2-a^2}{2(b-a)} =
\frac{a+b}{2}\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a discrete real-valued random variable with probability function <span class="math notranslate nohighlight">\(p\)</span>, then the expected value of <span class="math notranslate nohighlight">\(X\)</span> is
$<span class="math notranslate nohighlight">\(
   {\mathbb E}X = \sum_{i=1}^\infty x_i p(x_i),
\)</span><span class="math notranslate nohighlight">\(
where \)</span>{x_i} = { x \in \Re: p(x) &gt; 0}$,
provided the sum exists.</p>
<p><em>Example.</em>
Suppose <span class="math notranslate nohighlight">\(X\)</span> has a Poisson distribution with parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.
Then <span class="math notranslate nohighlight">\({\mathbb E}X = e^{-\lambda} \sum_{j=0}^\infty j \lambda^j/j! = \lambda\)</span>.</p>
</div>
</div>
<div class="section" id="variance-standard-error-and-covariance">
<h2>Variance,  Standard Error, and Covariance<a class="headerlink" href="#variance-standard-error-and-covariance" title="Permalink to this headline">¶</a></h2>
<p>See <a class="reference external" href="http://www.stat.berkeley.edu/~stark/SticiGui/Text/standardError.htm">SticiGui: Standard Error</a> for an elementary introduction to variance and standard error.</p>
<p>The <em>variance</em> of a random variable <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\mbox{Var }X = {\mathbb E}(X - {\mathbb E}X)^2\)</span>.</p>
<p>Algebraically, the following identity holds:
$<span class="math notranslate nohighlight">\(
\mbox{Var } X = {\mathbb E}(X - {\mathbb E}X)^2 = {\mathbb E}X^2 - 2({\mathbb E}X)^2 + ({\mathbb E}X)^2 =
{\mathbb E}X^2 - ({\mathbb E}X)^2.
\)</span><span class="math notranslate nohighlight">\(
However, this is generally not a good way to calculate \)</span>\mbox{Var} X$ numerically, because of roundoff:
it sacrifices precision unnecessarily.</p>
<p>The <em>standard error</em> of a random variable <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\mbox{SE }X = \sqrt{\mbox{Var } X}\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(\{X_i\}_{i=1}^n\)</span> are independent, then <span class="math notranslate nohighlight">\(\mbox{Var} \sum_{i=1}^n X_i = \sum_{i=1}^n \mbox{Var }X_i\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have a joint distribution, then <span class="math notranslate nohighlight">\(\mbox{cov} (X,Y) = {\mathbb E} (X - {\mathbb E}X)(Y - {\mathbb E}Y)\)</span>.
It follows from this definition (and the commutativity of multiplication)
that <span class="math notranslate nohighlight">\(\mbox{cov}(X,Y) = \mbox{cov}(Y,X)\)</span>.
Also,
$<span class="math notranslate nohighlight">\(
\mbox{var }(X+Y) = \mbox{var }X + \mbox{var }Y + 2\mbox{cov}(X,Y).
\)</span>$</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, <span class="math notranslate nohighlight">\(\mbox{cov }(X,Y) = 0\)</span>.
However, the converse is not necessarily true: <span class="math notranslate nohighlight">\(\mbox{cov}(X,Y) = 0\)</span> does not in general imply that
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.</p>
</div>
<div class="section" id="random-vectors">
<h2>Random Vectors<a class="headerlink" href="#random-vectors" title="Permalink to this headline">¶</a></h2>
<p>Suppose <span class="math notranslate nohighlight">\(\{X_i\}_{i=1}^n\)</span> are jointly distributed random variables, and let
$$
X =</p>
<div class="amsmath math notranslate nohighlight" id="equation-8fb16f78-a428-4a04-b1f8-2bffc4ec66c6">
<span class="eqno">()<a class="headerlink" href="#equation-8fb16f78-a428-4a04-b1f8-2bffc4ec66c6" title="Permalink to this equation">¶</a></span>\[\begin{pmatrix}
X_1 \\
\vdots \\
X_n
\end{pmatrix}\]</div>
<p>.
$<span class="math notranslate nohighlight">\(
Then \)</span>X<span class="math notranslate nohighlight">\( is a random vector, a \)</span>n<span class="math notranslate nohighlight">\( by \)</span>1$ vector of real-valued random variables.</p>
<p>The expected value of <span class="math notranslate nohighlight">\(X\)</span> is
$$
{\mathbb E} X =</p>
<div class="amsmath math notranslate nohighlight" id="equation-7ceee2fc-eba1-4f0b-9510-f3118608c7ff">
<span class="eqno">()<a class="headerlink" href="#equation-7ceee2fc-eba1-4f0b-9510-f3118608c7ff" title="Permalink to this equation">¶</a></span>\[\begin{pmatrix}
{\mathbb E} X_1 \\
\vdots \\
{\mathbb E} X_n
\end{pmatrix}\]</div>
<p>.
$$</p>
<p>The <em>covariance matrix</em> of <span class="math notranslate nohighlight">\(X\)</span> is
$$
\mbox{cov } X =
{\mathbb E}
\left (</p>
<div class="amsmath math notranslate nohighlight" id="equation-f05a4986-558f-4b91-8bf9-300b723cae1d">
<span class="eqno">()<a class="headerlink" href="#equation-f05a4986-558f-4b91-8bf9-300b723cae1d" title="Permalink to this equation">¶</a></span>\[\begin{pmatrix}
X_1 - {\mathbb E} X_1 \\
\vdots \\
X_n - {\mathbb E} X_n
\end{pmatrix}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-c0c65d3b-c259-41d0-ab71-2db8ff7bbe35">
<span class="eqno">()<a class="headerlink" href="#equation-c0c65d3b-c259-41d0-ab71-2db8ff7bbe35" title="Permalink to this equation">¶</a></span>\[\begin{pmatrix}
X_1 - {\mathbb E} X_1 &amp; \cdots &amp; X_n - {\mathbb E} X_n
\end{pmatrix}\]</div>
</div>
</div>
<div class="section" id="right">
<h1>\right )<a class="headerlink" href="#right" title="Permalink to this headline">¶</a></h1>
<p>{\mathbb E}</p>
<div class="amsmath math notranslate nohighlight" id="equation-8e84e096-979d-43cb-8993-55ffe6809491">
<span class="eqno">()<a class="headerlink" href="#equation-8e84e096-979d-43cb-8993-55ffe6809491" title="Permalink to this equation">¶</a></span>\[\begin{pmatrix}
(X_1 - {\mathbb E} X_1)^2 &amp; (X_1 - {\mathbb E} X_1)(X_2 - {\mathbb E} X_2) &amp; \cdots &amp; (X_1 - {\mathbb E} X_1)(X_n - {\mathbb E} X_n) \\
(X_1 - {\mathbb E} X_1)(X_2 - {\mathbb E} X_2) &amp; (X_2 - {\mathbb E} X_2)^2 &amp; \cdots &amp; (X_2 - {\mathbb E} X_2)(X_n - {\mathbb E} X_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
(X_1 - {\mathbb E} X_1)(X_n - {\mathbb E} X_n) &amp; (X_2 - {\mathbb E} X_2)(X_n - {\mathbb E} X_n) &amp; \cdots &amp; (X_n - {\mathbb E} X_n)^2
\end{pmatrix}\]</div>
<div class="math notranslate nohighlight">
\[\]</div>
<p>=</p>
<div class="amsmath math notranslate nohighlight" id="equation-77782f4a-1c06-4bbd-a773-3c1674e1226c">
<span class="eqno">()<a class="headerlink" href="#equation-77782f4a-1c06-4bbd-a773-3c1674e1226c" title="Permalink to this equation">¶</a></span>\[\begin{pmatrix}
{\mathbb E}(X_1 - {\mathbb E} X_1)^2 &amp; {\mathbb E}((X_1 - {\mathbb E} X_1)(X_2 - {\mathbb E} X_2)) &amp; \cdots &amp; 
{\mathbb E}(X_1 - {\mathbb E} X_1)(X_n - {\mathbb E} X_n)) \\
{\mathbb E}((X_1 - {\mathbb E} X_1)(X_2 - {\mathbb E} X_2)) &amp; {\mathbb E}(X_2 - {\mathbb E} X_2)^2 &amp; \cdots &amp; 
{\mathbb E}((X_2 - {\mathbb E} X_2)(X_n - {\mathbb E} X_n)) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
{\mathbb E}((X_1 - {\mathbb E} X_1)(X_n - {\mathbb E} X_n)) &amp; {\mathbb E}(X_2 - {\mathbb E} X_2)(X_n - {\mathbb E} X_n)) &amp; \cdots &amp; {\mathbb E}(X_n - {\mathbb E} X_n)^2
\end{pmatrix}\]</div>
<p>.
$$</p>
<p>Covariance matrices are always <em>positive semidefinite</em>.</p>
<div class="section" id="the-multivariate-normal-distribution">
<h2>The Multivariate Normal Distribution<a class="headerlink" href="#the-multivariate-normal-distribution" title="Permalink to this headline">¶</a></h2>
<p>The notation <span class="math notranslate nohighlight">\(X \sim {\mathcal N}(\mu, \sigma^2)\)</span> means that <span class="math notranslate nohighlight">\(X\)</span> has a normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span>
and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.
This distribution is continuous, with probability density function
$<span class="math notranslate nohighlight">\(
\frac{1}{\sqrt{2\pi} \sigma} e^{\frac{-(x-\mu)^2}{2\sigma^2}}.
\)</span>$</p>
<p>If <span class="math notranslate nohighlight">\(X \sim {\mathcal N}(\mu, \sigma^2)\)</span>, then <span class="math notranslate nohighlight">\(\frac{X-\mu}{\sigma} \sim {\mathcal N}(0, 1)\)</span>,
the <em>standard normal distribution</em>.
The probability density function of the standard normal distribution is
$<span class="math notranslate nohighlight">\(
\phi(x) = \frac{1}{\sqrt{2\pi}} e^ {-x^2/2}.
\)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">## Plot the normal density</span>
<span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="n">from</span> <span class="o">=</span> <span class="m">-5</span><span class="p">,</span> <span class="n">to</span> <span class="o">=</span> <span class="m">5</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="m">0.01</span><span class="p">);</span>
<span class="n">y</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="kc">pi</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">-1</span><span class="o">/</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">^</span><span class="m">2</span><span class="o">/</span><span class="m">2</span><span class="p">);</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&quot;n&quot;</span><span class="p">)</span> <span class="o">+</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">h</span> <span class="o">=</span> <span class="m">0</span><span class="p">)</span> <span class="o">+</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span>
<span class="nf">title</span><span class="p">(</span><span class="n">main</span> <span class="o">=</span> <span class="s">&quot;The Standard Normal Curve&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><ol class=list-inline>
</ol>
</div><img alt="../_images/probVectors_9_1.png" src="../_images/probVectors_9_1.png" />
</div>
</div>
<p>A collection of random variables <span class="math notranslate nohighlight">\(\{ X_1, X_2, \ldots, X_n\} = \{X_j\}_{j=1}^n\)</span> is <em>jointly normal</em>
if all linear combinations of those variables have normal distributions.
That is, the collection is jointly normal if for all <span class="math notranslate nohighlight">\(\alpha \in \Re^n\)</span>, <span class="math notranslate nohighlight">\(\sum_{j=1}^n \alpha_j X_j\)</span>
has a normal distribution.</p>
<p>If <span class="math notranslate nohighlight">\(\{X_j \}_{j=1}^n\)</span> are independent, normally distributed random variables, they are jointly normal.</p>
<p>If for some <span class="math notranslate nohighlight">\(\mu \in \Re^n\)</span> and positive-definite matrix <span class="math notranslate nohighlight">\(G\)</span>, the joint density of <span class="math notranslate nohighlight">\(\{X_j \}_{j=1}^n\)</span> is
$<span class="math notranslate nohighlight">\( 
\left ( \frac{1}{\sqrt{2 \pi}}\right )^n \frac{1}{\sqrt{\left | G \right |}} 
\exp \left \{ - \frac{1}{2} (x - \mu)'G^{-1}(x-\mu) \right \},
\)</span><span class="math notranslate nohighlight">\(
then \)</span>{X_j }<em>{j=1}^n<span class="math notranslate nohighlight">\( are jointly normal, and the covariance matrix of \)</span>{X_j}</em>{j=1}^n<span class="math notranslate nohighlight">\( is \)</span>G$.</p>
<div class="section" id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<p>[TO DO!]</p>
</div>
</div>
<div class="section" id="the-central-limit-theorem">
<h2>The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Permalink to this headline">¶</a></h2>
<p>For an elementary discussion, see <a class="reference external" href="http://www.stat.berkeley.edu/~stark/SticiGui/Text/clt.htm">SticiGui: The Normal Curve, The Central Limit Theorem, and Markov’s and Chebychev’s Inequalities for Random Variables</a>.</p>
<p>Suppose <span class="math notranslate nohighlight">\(\{X_j \}_{j=1}^\infty\)</span> are independent and identically distributed (iid), have finite expected value <span class="math notranslate nohighlight">\({\mathbb E}X_j = \mu\)</span>, and have finite variance <span class="math notranslate nohighlight">\(\mbox{var }X_j = \sigma^2\)</span>.</p>
<p>Define the sum <span class="math notranslate nohighlight">\(S_n \equiv \sum_{j=1}^n X_j\)</span>.
Then
$<span class="math notranslate nohighlight">\(
{\mathbb E}S_n = {\mathbb E} \sum_{j=1}^n X_j = \sum_{j=1}^n {\mathbb E} X_j = \sum_{j=1}^n \mu = n\mu,
\)</span><span class="math notranslate nohighlight">\(
and
\)</span><span class="math notranslate nohighlight">\(
\mbox{var }S_n = \mbox{var } \sum_{j=1}^n X_j = n\sigma^2.
\)</span><span class="math notranslate nohighlight">\(
(The last step follows from the independence of \)</span>{X_j}$: the variance of the sum is the sum of the variances.)</p>
<p>Define <span class="math notranslate nohighlight">\(Z_n \equiv \frac{S_n - n\mu}{\sqrt{n}\sigma}\)</span>.
Then for every <span class="math notranslate nohighlight">\(a, b \in \Re\)</span> with <span class="math notranslate nohighlight">\(a \le b\)</span>,
$<span class="math notranslate nohighlight">\(
\lim_{n \rightarrow \infty} {\mathbb P} \{ a \le Z_n \le b \} = \frac{1}{\sqrt{2\pi}} \int_a^b e^{-x^2/2} dx.
\)</span>$
This is a basic form of the Central Limit Theorem.</p>
</div>
<div class="section" id="probability-inequalities-and-identities">
<h2>Probability Inequalities and Identities<a class="headerlink" href="#probability-inequalities-and-identities" title="Permalink to this headline">¶</a></h2>
<p>This follows Lugosi (2006) rather closely; it’s also a repeat of the results in the chapter on
<span class="xref myst">Mathematical Preliminaries</span>, which we largely omitted.</p>
<div class="section" id="the-tail-integral-formula-for-expectation">
<h3>The tail-integral formula for expectation<a class="headerlink" href="#the-tail-integral-formula-for-expectation" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a nonnegative real-valued random variable,
$<span class="math notranslate nohighlight">\(
{\mathbb E} X = \int_0^\infty {\mathbb P} \{X \ge x\} dx
\)</span>$</p>
</div>
<div class="section" id="jensen-s-inequality">
<h3>Jensen’s Inequality<a class="headerlink" href="#jensen-s-inequality" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(\phi\)</span> is a convex function from <span class="math notranslate nohighlight">\({\mathcal X}\)</span> to <span class="math notranslate nohighlight">\(\Re\)</span>, then <span class="math notranslate nohighlight">\(\phi({\mathbb E} X) \le {\mathbb E} \phi(X)\)</span>.</p>
</div>
<div class="section" id="markov-s-chebychev-s-and-related-inequalities">
<h3>Markov’s, Chebychev’s, and related inequalities<a class="headerlink" href="#markov-s-chebychev-s-and-related-inequalities" title="Permalink to this headline">¶</a></h3>
<p>From the tail-integral formula,
$<span class="math notranslate nohighlight">\(
{\mathbb E} X \ge \int_0^t {\mathbb P} \{X \ge x\} dx \ge t {\mathbb P} \{X \ge t \},
\)</span><span class="math notranslate nohighlight">\(
which yields _Markov's Inequality_:
\)</span><span class="math notranslate nohighlight">\(
{\mathbb P} \{ X \ge t \} \le \frac{{\mathbb E} X}{t}.
\)</span>$</p>
<p>Moreover, for any strictly monotonic function <span class="math notranslate nohighlight">\(f\)</span> and nonnegative <span class="math notranslate nohighlight">\(X\)</span>,
we have the <em>Generalized Markov Inequality</em>:
$<span class="math notranslate nohighlight">\(
{\mathbb P} \{ X \ge t \} = {\mathbb P} \{ f(X) \ge f(t) \} \le \frac{{\mathbb E} f(X)}{f(t)}.
\)</span><span class="math notranslate nohighlight">\(
In particular, for any real-valued \)</span>X<span class="math notranslate nohighlight">\( and real \)</span>q &gt; 0<span class="math notranslate nohighlight">\(,
\)</span>| X - {\mathbb E} X |<span class="math notranslate nohighlight">\( is a nonnegative
random variable and \)</span>f(x) = x^q<span class="math notranslate nohighlight">\( is strictly monotonic, so
\)</span><span class="math notranslate nohighlight">\(
{\mathbb P} \{| X - {\mathbb E} X | \ge t \} = {\mathbb P} \{ | X - {\mathbb E} X |^q \ge t^q \} \le
\frac{{\mathbb E}  | X - {\mathbb E} X |^q}{t^q}.
\)</span><span class="math notranslate nohighlight">\(
_Chebychev's inequality_ is a special case:
\)</span><span class="math notranslate nohighlight">\(
{\mathbb P} \{ | X - {\mathbb E} X |^2 \ge t^2 \} \le \frac{{\mathbb E}  | X - {\mathbb E} X |^2}{t^2}
= \frac{{\mbox{Var}} X}{t^2}.
\)</span>$</p>
</div>
<div class="section" id="the-chernoff-bound">
<h3>The Chernoff Bound<a class="headerlink" href="#the-chernoff-bound" title="Permalink to this headline">¶</a></h3>
<p>Apply the Generalized Markov Inequality with <span class="math notranslate nohighlight">\(f(x) = e^{sx}\)</span> for positive <span class="math notranslate nohighlight">\(s\)</span>
to obtain the <em>Chernoff Bound</em>:
$<span class="math notranslate nohighlight">\(
{\mathbb P} \{ X \ge t \} = {\mathbb P} \{ e^{sX} \ge e^{st} \} \le \frac{{\mathbb E} e^{sX}}{e^{st}}
\)</span><span class="math notranslate nohighlight">\(
for all \)</span>s<span class="math notranslate nohighlight">\(.
For particular \)</span>X<span class="math notranslate nohighlight">\(, one can optimize over \)</span>s$.</p>
</div>
<div class="section" id="hoeffding-s-inequality">
<h3>Hoeffding’s Inequality<a class="headerlink" href="#hoeffding-s-inequality" title="Permalink to this headline">¶</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\{ X_j \}_{j=1}^n\)</span> be independent, and define
<span class="math notranslate nohighlight">\(S_n \equiv \sum_{j=1}^n X_j\)</span>.
Applying the Chernoff Bound yields
$<span class="math notranslate nohighlight">\(
{\mathbb P} \{ S_n - {\mathbb E} S_n \ge t \} \le e^{-st} {\mathbb E} e^{sS_n} =
e^{-st} \prod_{j=1}^n e^{s(X_j - E X_j)}.
\)</span><span class="math notranslate nohighlight">\(
Hoeffding bounds the moment generating function for a bounded random variable
\)</span>X<span class="math notranslate nohighlight">\(:
If \)</span>a \le X \le b<span class="math notranslate nohighlight">\( with probability 1, then
\)</span><span class="math notranslate nohighlight">\(
{\mathbb E} e^{sX}  \le e^{s^2 (b-a)^2/8},
\)</span>$
from which follows <em>Hoeffding’s tail bound</em>.</p>
<p>If <span class="math notranslate nohighlight">\(\{X_j\}_{j=1}^n\)</span> are independent and <span class="math notranslate nohighlight">\({\mathbb P} \{a_j \le X_j \le b_j\} = 1\)</span>,
then
$<span class="math notranslate nohighlight">\( 
{\mathbb P} \{ S_n - {\mathbb E} S_n \ge t \} \le e^{-2t^2/\sum_{j=1}^n (b_j - a_j)^2}
\)</span><span class="math notranslate nohighlight">\(
and
\)</span><span class="math notranslate nohighlight">\( 
{\mathbb P} \{ S_n - {\mathbb E} S_n \le -t \} \le e^{-2t^2/\sum_{j=1}^n (b_j - a_j)^2}
\)</span>$</p>
</div>
<div class="section" id="hoeffding-s-other-inequality">
<h3>Hoeffding’s Other Inequality<a class="headerlink" href="#hoeffding-s-other-inequality" title="Permalink to this headline">¶</a></h3>
<p>Suppose <span class="math notranslate nohighlight">\(f\)</span> is a convex, real function and <span class="math notranslate nohighlight">\({\mathcal X}\)</span> is a finite set.
Let <span class="math notranslate nohighlight">\(\{X_j \}_{j=1}^n\)</span> be a simple random sample from <span class="math notranslate nohighlight">\({\mathcal X}\)</span> and
let <span class="math notranslate nohighlight">\(\{Y_j \}_{j=1}^n\)</span> be an iid uniform random sample (with replacement) from <span class="math notranslate nohighlight">\({\mathcal X}\)</span>.
Then
$<span class="math notranslate nohighlight">\(  
{\mathbb E} f \left ( \sum_{j=1}^n X_j \right ) \le {\mathbb E} f \left ( \sum_{j=1}^n Y_j \right ).
\)</span>$</p>
</div>
<div class="section" id="bernstein-s-inequality">
<h3>Bernstein’s Inequality<a class="headerlink" href="#bernstein-s-inequality" title="Permalink to this headline">¶</a></h3>
<p>Suppose <span class="math notranslate nohighlight">\(\{X_j \}_{j=1}^n\)</span> are independent with <span class="math notranslate nohighlight">\({\mathbb E} X_j = 0\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>,
<span class="math notranslate nohighlight">\({\mathbb P} \{ | X_j | \le c\} = 1\)</span>,
<span class="math notranslate nohighlight">\(\sigma_j^2 = {\mathbb E} X_j^2\)</span> and <span class="math notranslate nohighlight">\(\sigma = \frac{1}{n} \sum_{j=1}^n \sigma_j^2\)</span>.
Then for any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>,
$<span class="math notranslate nohighlight">\(
{\mathbb P} \{ S_n/n &gt; \epsilon \} \le e^{-n \epsilon^2 / 2(\sigma^2 + c\epsilon/3)}.
\)</span>$</p>
<p>Next chapter: <a class="reference internal" href="inference.html"><span class="doc std std-doc">Inference and Statistical Models</span></a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./Notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Philip Stark and Fernando Pérez<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>